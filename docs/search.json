[
  {
    "objectID": "documentacion.html",
    "href": "documentacion.html",
    "title": "Documentación",
    "section": "",
    "text": "El proyecto está diseñado para ser desplegado en entornos Linux o Windows con Python 3.12.9. Requiere acceso a Ollama (para la ejecución de modelos open-source como gemma3:12b), así como conectividad a una instancia de MongoDB y bases de datos MySQL.\nLa aplicación backend se expone a través de FastAPI en el puerto 8000. Es crucial asegurar que este puerto esté abierto y accesible en el entorno de despliegue.\nArchivo .env con variables cargadas\nTodas las credenciales y configuraciones sensibles se gestionan mediante un archivo de variables de entorno (.env), garantizando la seguridad y facilidad de configuración.\n\n\n\n\n\nPython: Versión 3.12.9\nPip: Última versión\nUV: Última versión (gestor de paquetes y entornos)\nOllama: Instalado y en ejecución en el servidor para el hosting de modelos open-source.\nMongoDB: Acceso remoto configurado para las colecciones de historial, productos, ofertas y fichas técnicas.\nMySQL: Acceso remoto configurado para la extracción de datos de productos y precios.\n\n\n\n\n\nlangchain: Framework principal para la construcción de cadenas RAG y la orquestación del flujo del chatbot.\ntiktoken: Utilizado para el conteo preciso de tokens en las consutlas y respuestas, fundamental para la estimación de costos.\nollama: Herramienta para servir modelos de lenguaje open-source localmente, como gemma3:12b, permitiendo flexibilidad en la elección del LLM.\npymongo: Driver Python para la interacción con MongoDB, utilizado para el almacenamiento y recuperación de sesiones de usuario, historial de mensajes, fichas técnicas, y datos de productos/ofertas.\nmysql-connector-python: Conector para MySQL, empleado para la extracción de datos de producto, sus detalles y precios desde la base de datos relacional.\nfaiss-cpu: Biblioteca para la búsqueda eficiente de similitudes, crucial para la creación y consulta de la base de datos vectorial donde se almacenan los embeddings de productos.\ngunicorn: Servidor WSGI utilizado para desplegar la aplicación FastAPI en producción, gestionando la concurrencia y el rendimiento.\npodman: Herramienta de virtualización y contenedores sin daemon, utilizada para ejecutar la aplicación dentro de entornos aislados (containers) de manera similar a Docker, pero con mayor seguridad y compatibilidad con sistemas Linux. Facilita el despliegue reproducible de la aplicación y sus servicios asociados (como la base de datos o el servidor vectorial).\nOtras dependencias: Todas las demás librerías requeridas se detallan en el archivo pyproject.toml. La instalación de este archivo se detalla más adelante.\n\n\n\n\n\n\ngit clone https://github.com/anmerino-pnd/proyectoCT\ncd proyectoCT\n\n\n\nSe recomienda usar uv por su eficiencia.\npip install uv # En caso de no estar instalado\nuv venv\nsource .venv/bin/activate  # Para Linux/macOS\n# o `.venv\\Scripts\\activate` para Windows\nuv pip install -e .\n\n\n\nVerifica que el servicio de Ollama esté instalado y activo, y que el modelo gemma3:12b esté disponible.\ncurl -fsSL https://ollama.com/install.sh | sh # Para instalar Ollama\nollama serve\nollama list # Para verificar que el modelo gemma3:12b esté descargado y listo\nollama pull gemma3:12b # Correr esta línea en caso que el modelo no aparezca\nConfigurar el servicio de Redis el cual se encarga del cache de la información.\nmkdir -p ~/proyectoCT/datos/redis_data\n\nchmod 700 ~/proyectoCT/datos/redis_data\n\n# Crear un volumen para persistir los datos en cache\npodman run -d \\\n  --name redis-semantic \\\n  -p 6380:6379 \\\n  -v redis-data:/data \\\n  --restart unless-stopped \\\n  redis:latest redis-server --appendonly yes --save \"\"\n\n#Nota: Se usa puerto **6380** en lugar de 6379 porque el puerto estándar ya está ocupado por el servicio Redis del sistema.\n\n# Verificar que esté corriendo\npodman ps -a\npodman exec -it redis-semantic redis-cli ping # Debe responder PONG\npodman logs redis-semantic\npython3 -c \"import redis; r = redis.Redis(host='localhost', port=6380); print(r.ping())\"\nNOTA: En el caso que aparezca este error de Redis en los logs de las conversaciones:\nRedis update failed: Command # 1 (HSET cebdab3b4c033ee7ada24b16b3fc09f0 0 {\"lc\": 1, \"type\":\n\"constructor\", \"id\": [\"langchain\",...) of pipeline caused error: MISCONF Redis is configured to \nsave RDB snapshots, but it's currently unable to persist to disk. Commands that may modify the \ndata set are disabled, because this instance is configured to report errors during writes if RDB \nsnapshotting fails (stop-writes-on-bgsave-error option). Please check the Redis logs for details \nabout the RDB error.\nSeguir estos pasos:\n# 1. Detener y eliminar el contenedor actual\npodman stop redis-semantic\npodman rm redis-semantic\n\n# 2. Crear con volumen nombrado (Podman maneja permisos automáticamente)\npodman run -d \\\n  --name redis-semantic \\\n  -p 6380:6379 \\\n  -v redis-data:/data \\\n  --restart unless-stopped \\\n  redis:latest redis-server --appendonly yes --save \"\"\n\n# 3. Verificar que esté corriendo\npodman ps\n\n# 4. Probar que funcione\npodman exec -it redis-semantic redis-cli ping\npodman exec -it redis-semantic redis-cli SET test \"hello\"\npodman exec -it redis-semantic redis-cli GET test\n\n# Verificar que se solucionó\n# Ver logs (no debe haber errores de permisos)\npodman logs redis-semantic\n\n# Probar escritura\npodman exec -it redis-semantic redis-cli\n# Dentro de redis-cli:\nSET mykey \"test value\"\nGET mykey\nBGSAVE  # Forzar guardado en disco\nexit\n\n# Ver que no haya errores\npodman logs redis-semantic | tail -20\n\npodman exec -it redis-semantic redis-cli CONFIG GET save    \n# Debería arrojar esto :\n# 1) \"save\"\n# 2) \"\"\nConfigurar la instancia de Mongo local que almacena las fichas técnicas de los productos.\n# 1. Crear y levantar el contenedor MongoDB\npodman run -d \\\n  --name mongo-semantic \\\n  -p 27017:27017 \\\n  -v mongo-data:/data/db \\\n  mongo:latest\n\n# 2. Verificar que el contenedor esté corriendo\npodman ps -a\n\n# 3. Copiar el archivo JSON de las fichas técnicas al contenedor\npodman cp ./datos/CT_API_Publica.tbl_mongo_collection_specifications.json mongo-semantic:/tmp/specs.json\n\n# 4. Importar el JSON (esto crea automáticamente la BD y la colección)\npodman exec -it mongo-semantic mongoimport \\\n  --db CT_API_Publica \\\n  --collection tbl_mongo_collection_specifications \\\n  --file /tmp/specs.json \\\n  --jsonArray\n\n# 5. Conectar a MongoDB para verificar\npodman exec -it mongo-semantic mongosh\n\n# 6. Dentro de mongosh, verificar los datos:\nuse CT_API_Publica          # Cambiar a la base de datos correcta\nshow collections            # Ver las colecciones (debe aparecer tbl_mongo_collection_specifications)\ndb.tbl_mongo_collection_specifications.countDocuments()  # Contar documentos\ndb.tbl_mongo_collection_specifications.findOne()         # Ver un documento de ejemplo\nexit                        # Salir de mongosh\n\n\n\nAntes de levantar el backend, asegurarse de que el archivo .env en la raíz del proyecto contenga las siguientes variables con sus valores correctos.\n# Conexión a la base de datos SQL\nip=\nport=\nuser=\npwd=\ndb=\n\n# Clave de la API de OpenAI para correr sus modelos\nOPENAI_API_KEY=\n\n# Configuración para el servicio de fichas técnicas\nsucursales_url = \"\"  # Url de la sección de sucursales\nreload_vectors_post = \"https://localhost:8000/internal/reload_vectorstores\"\nurl= ''           # Url del servicio de fichas tecnicas\nToken-api=''\nToken-ct=''\nContent-Type=''\nCookie=''\n\nsucursales_url= \"\"\n\ndominio=\"\"\nboundary=''\n\n# Conexión a MongoDB\nMONGO_URI = \"mongodb://\" # En la URI debe estar incrustrado el nombre de la DB\nMONGO_DB = \"\"\nMONGO_COLLECTION_SESSIONS = \"tbl_sessions\"\nMONGO_COLLECTION_MESSAGE_BACKUP = \"tbl_message_backup\"\nMONGO_COLLECTION_PRODUCTS = \"tbl_productos\"\nMONGO_COLLECTION_SALES = \"tbl_ofertas\"         \nMONGO_COLLECTION_SPECIFICATIONS = \"tbl_mongo_collection_specifications\"\nMONGO_COLLECTION_PEDIDOS=\"tbl_pedidos\"\n\nPODMAN_REDIS_URL=redis://localhost:6379\n\n\n\nEste comando inicia la API, especificando el número de workers, el binding de IP y puerto, y la configuración de SSL/TLS para HTTPS.\nnohup gunicorn ct.main:app   --workers 4   --bind 0.0.0.0:8000   --certfile=static/ssl/cert.pem   --keyfile=static/ssl/key.pem   -k uvicorn.workers.UvicornWorker --timeout 120 --access-logfile -   --error-logfile - &\nEl uso de nogup y & asegura que el proceso continúe ejecutándose en segundo plano incluso si la sesión SSH se cierra.\n\n\n\nSi el certificado SSL autofirmado ha expirado o necesitas uno nuevo:\nopenssl req -x509 -newkey rsa:2048 -nodes -keyout ssl/key.pem -out ssl/cert.pem -days 365\nAsegurarse de que los archivos cert.pem y key.pem estén en la ruta ssl dentro de tu proyecto.\n\n\n\nAl correr la API con nohup, este genera un archivo nohup.out, con el cual podemos ver los logs del sistema, para eso solo hay que ubicarse en donde está dicho archivo y correr lo siguiente:\ntail -f nohup.out\nLos logs también se pueden analizar para el reporte automatizado\nnohup streamlit run run_report.py --server.fileWatcherType none --server.port 3000 &\n\n\n\n\n\nCargar archivos del widget: Los archivos del frontend (principalmente sdk.js y cualquier recurso gráfico como chat.png) deben ser cargados en el servidor donde reside el frontend de la página.\nIncrustar el widget en el HTML: Ejemplo de cómo se puede añadir el widget en la página web donde se desea que aparezca el chatbot.\n\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"es\"&gt;\n&lt;head&gt;\n  &lt;meta charset=\"UTF-8\" /&gt;\n  &lt;title&gt;Prueba del Widget&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n  &lt;script \n    src=\"sdk.js\" \n    data-user-id=\"test\" \n    data-user-key=\"2\" \n    data-api-base=\"https://ctdev.ctonline.mx/chatbot\" \n    data-chat-icon-url=\"chat.png\" \n    type=\"text/javascript\"&gt;\n  &lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\nNotas importantes para el data-api-base:\n\nSi la API corre en HTTP y el frontend en HTTPS, se enfrentarán problemas de “contenido mixto”. La solución propuesta fue usar un archivo PHP (backend del sitio web) como intermediario, el cual es crucial aquí. La API debe apuntar a este PHP y el PHP a su frontend donde se encuentra el widget.\nLa data-api-base es el dominio donde es accesible la API mediante PHP.\n\n\n\n\nPara asegurar que el chatbot tenga acceso a la información más reciente de productos, promociones y fichas técnicas, es necesario ejecutar periódicamente el pipeline ETL. Este proceso extrae, transforma y carga los datos, actualizando la base de datos vectorial utilizada por el sistema RAG.\nPara ejecutar el pipeline ETL, sigue estos pasos:\n\nAcceder al entorno virtual: Asegurarse de estar en el directorio raíz del proyecto (proyectoCT) y activa el entorno virtual donde se instalaron las dependencias del backend.\n\nsource .venv/bin/activate # Para Linux/macOS\n# o `.venv/Scripts/activate` para Windows\n\nEjecutar el pipeline ETL: Una vez activado el entorno, puedes ejecutar una de las funciones dentro del script pipeline.py dependiendo la necesidad.\n\nEn caso de cargar la base general de productos, correr este comando. Recomendación, correr cada 2 o 3 meses, ya que la información técnica cambia con poca frecuencia.\npython3 -c \"from ct.ETL.pipeline import load_products; load_products()\"\nConsejo: si ya se tiene una base de datos vectorial de productos, agregar productos nuevos con el siguiente comando. Esto evita tener que extraer, transformar y cargar todos los productos, simplemente va agregando los faltantes.\npython3 -c \"from ct.ETL.pipeline import update_products; update_products()\"\nEn caso de cargar únicamente los productos en promoción, correr este comando. Eficiente para cada mes que hay productos nuevos en promoción.\npython3 -c \"from ct.ETL.pipeline import load_sales; load_sales()\"\nUna vez que ya se tienen las dos bases vectoriales, es necesario combinarlos y cargarlos.\npython3 -c \"from ct.ETL.pipeline import load_sales_products; load_sales_products()\"\nEn caso de querer actualizar ambas al mismo tiempo, correr este comando. Esto elimina productos antiguos que sean innecesarios almacenar.\npython3 -c \"from ct.ETL.pipeline import update_all; update_all()\"\n\nCrontab del ETL (opcional): Se recomienda automatizar la ejecución de este pipeline (por ejemplo, cada hora entre las 8:30 a 18:30) para mantener actualizada la base de conocimientos del chatbot.\n\nEn sistemas Linux, esto se puede realizar fácilmente mediante un cron job y el archivo reload_all.sh que:\n\nEjecuta src/ct/ETL/update_vector_stores.py usando el python del virtualenv del proyecto.\nSi detecta que el vector store fue regenerado, envía SIGHUP al proceso master de Gunicorn para forzar la recarga de todos los workers.\nRegistra salida en logs/reload_cron_wrapper.log.\n\n# Dar permisos de ejecución al archivo bash\nchmod +x ~/proyectoCT/reload_all.sh\n\n# Probar manualmente \n~/proyectoCT/reload_all.sh\n\n# Revisar el log\ntail -n 200 ~/proyectoCT/logs/reload_cron_wrapper.log\n\n# Si no hubo fallas. Agregar la tarea al cron\ncrontab -e\n\n# Añade la siguiente línea\n30 8-18 * * 1-6 ~/proyectoCT/reload_all.sh &gt;&gt; ~/proyectoCT/logs/reload_cron_wrapper.log 2&gt;&1\n\n# Verificar que se hizo correctamente con\ncrontab -l\n\n# Cuando ya se haya ejecutado el flujo del cron, se puede revisar con\ncat ~/proyectoCT/logs/reload_cron_wrapper.log\nComandos útiles de cron\n\n\n\nAcción\nComando\n\n\n\n\nVer tareas activas\ncrontab -l\n\n\nBorrar todas las tareas\ncrontab -r\n\n\nPausar una tarea\nEditar y comentar la línea con #\n\n\n\nCómo salir del editor\n\nEn nano -&gt; Ctrl + O, Enter, luego Ctrl + X\nEn vi o vim -&gt; I, editar, Esc, luego :wq para guardar o :q! para salir sin guardar\n\n\n\n\nEstas configuraciones son necesarios para que no haya problemas dentro del sistema de conversaciones y el sistema de reportes automatizados.\npython -m spacy download es_core_news_lg\npython -m spacy download es_core_news_sm\n\n\n\n\nProblemas de caché: Es común que los navegadores almacenen versiones antiguas de archivos JS/CSS. Si la interfaz del widget no funciona correctamente después de una actualización, instruye a los usuarios a limpiar la caché de su navegador o a realizar un “hard refresh” (Ctrl+F5). Implementar una estrategia de versioning para los archivos del widget (ej.js?v=1.2.3) puede mitigar esto a futuro.\nRotación de IP para fichas técnicas: El sistema está diseñado para manejar el bloqueo de IP del servicio de fichas técnicas. Se recomienda monitorear los logs de la extracción (extraction.py) para identificar errores 403, lo que indicaría la necesidad de actualizar la IP en el servicio externo."
  },
  {
    "objectID": "documentacion.html#manual-de-instalación-y-despliegue.",
    "href": "documentacion.html#manual-de-instalación-y-despliegue.",
    "title": "Documentación",
    "section": "",
    "text": "El proyecto está diseñado para ser desplegado en entornos Linux o Windows con Python 3.12.9. Requiere acceso a Ollama (para la ejecución de modelos open-source como gemma3:12b), así como conectividad a una instancia de MongoDB y bases de datos MySQL.\nLa aplicación backend se expone a través de FastAPI en el puerto 8000. Es crucial asegurar que este puerto esté abierto y accesible en el entorno de despliegue.\nArchivo .env con variables cargadas\nTodas las credenciales y configuraciones sensibles se gestionan mediante un archivo de variables de entorno (.env), garantizando la seguridad y facilidad de configuración.\n\n\n\n\n\nPython: Versión 3.12.9\nPip: Última versión\nUV: Última versión (gestor de paquetes y entornos)\nOllama: Instalado y en ejecución en el servidor para el hosting de modelos open-source.\nMongoDB: Acceso remoto configurado para las colecciones de historial, productos, ofertas y fichas técnicas.\nMySQL: Acceso remoto configurado para la extracción de datos de productos y precios.\n\n\n\n\n\nlangchain: Framework principal para la construcción de cadenas RAG y la orquestación del flujo del chatbot.\ntiktoken: Utilizado para el conteo preciso de tokens en las consutlas y respuestas, fundamental para la estimación de costos.\nollama: Herramienta para servir modelos de lenguaje open-source localmente, como gemma3:12b, permitiendo flexibilidad en la elección del LLM.\npymongo: Driver Python para la interacción con MongoDB, utilizado para el almacenamiento y recuperación de sesiones de usuario, historial de mensajes, fichas técnicas, y datos de productos/ofertas.\nmysql-connector-python: Conector para MySQL, empleado para la extracción de datos de producto, sus detalles y precios desde la base de datos relacional.\nfaiss-cpu: Biblioteca para la búsqueda eficiente de similitudes, crucial para la creación y consulta de la base de datos vectorial donde se almacenan los embeddings de productos.\ngunicorn: Servidor WSGI utilizado para desplegar la aplicación FastAPI en producción, gestionando la concurrencia y el rendimiento.\npodman: Herramienta de virtualización y contenedores sin daemon, utilizada para ejecutar la aplicación dentro de entornos aislados (containers) de manera similar a Docker, pero con mayor seguridad y compatibilidad con sistemas Linux. Facilita el despliegue reproducible de la aplicación y sus servicios asociados (como la base de datos o el servidor vectorial).\nOtras dependencias: Todas las demás librerías requeridas se detallan en el archivo pyproject.toml. La instalación de este archivo se detalla más adelante.\n\n\n\n\n\n\ngit clone https://github.com/anmerino-pnd/proyectoCT\ncd proyectoCT\n\n\n\nSe recomienda usar uv por su eficiencia.\npip install uv # En caso de no estar instalado\nuv venv\nsource .venv/bin/activate  # Para Linux/macOS\n# o `.venv\\Scripts\\activate` para Windows\nuv pip install -e .\n\n\n\nVerifica que el servicio de Ollama esté instalado y activo, y que el modelo gemma3:12b esté disponible.\ncurl -fsSL https://ollama.com/install.sh | sh # Para instalar Ollama\nollama serve\nollama list # Para verificar que el modelo gemma3:12b esté descargado y listo\nollama pull gemma3:12b # Correr esta línea en caso que el modelo no aparezca\nConfigurar el servicio de Redis el cual se encarga del cache de la información.\nmkdir -p ~/proyectoCT/datos/redis_data\n\nchmod 700 ~/proyectoCT/datos/redis_data\n\n# Crear un volumen para persistir los datos en cache\npodman run -d \\\n  --name redis-semantic \\\n  -p 6380:6379 \\\n  -v redis-data:/data \\\n  --restart unless-stopped \\\n  redis:latest redis-server --appendonly yes --save \"\"\n\n#Nota: Se usa puerto **6380** en lugar de 6379 porque el puerto estándar ya está ocupado por el servicio Redis del sistema.\n\n# Verificar que esté corriendo\npodman ps -a\npodman exec -it redis-semantic redis-cli ping # Debe responder PONG\npodman logs redis-semantic\npython3 -c \"import redis; r = redis.Redis(host='localhost', port=6380); print(r.ping())\"\nNOTA: En el caso que aparezca este error de Redis en los logs de las conversaciones:\nRedis update failed: Command # 1 (HSET cebdab3b4c033ee7ada24b16b3fc09f0 0 {\"lc\": 1, \"type\":\n\"constructor\", \"id\": [\"langchain\",...) of pipeline caused error: MISCONF Redis is configured to \nsave RDB snapshots, but it's currently unable to persist to disk. Commands that may modify the \ndata set are disabled, because this instance is configured to report errors during writes if RDB \nsnapshotting fails (stop-writes-on-bgsave-error option). Please check the Redis logs for details \nabout the RDB error.\nSeguir estos pasos:\n# 1. Detener y eliminar el contenedor actual\npodman stop redis-semantic\npodman rm redis-semantic\n\n# 2. Crear con volumen nombrado (Podman maneja permisos automáticamente)\npodman run -d \\\n  --name redis-semantic \\\n  -p 6380:6379 \\\n  -v redis-data:/data \\\n  --restart unless-stopped \\\n  redis:latest redis-server --appendonly yes --save \"\"\n\n# 3. Verificar que esté corriendo\npodman ps\n\n# 4. Probar que funcione\npodman exec -it redis-semantic redis-cli ping\npodman exec -it redis-semantic redis-cli SET test \"hello\"\npodman exec -it redis-semantic redis-cli GET test\n\n# Verificar que se solucionó\n# Ver logs (no debe haber errores de permisos)\npodman logs redis-semantic\n\n# Probar escritura\npodman exec -it redis-semantic redis-cli\n# Dentro de redis-cli:\nSET mykey \"test value\"\nGET mykey\nBGSAVE  # Forzar guardado en disco\nexit\n\n# Ver que no haya errores\npodman logs redis-semantic | tail -20\n\npodman exec -it redis-semantic redis-cli CONFIG GET save    \n# Debería arrojar esto :\n# 1) \"save\"\n# 2) \"\"\nConfigurar la instancia de Mongo local que almacena las fichas técnicas de los productos.\n# 1. Crear y levantar el contenedor MongoDB\npodman run -d \\\n  --name mongo-semantic \\\n  -p 27017:27017 \\\n  -v mongo-data:/data/db \\\n  mongo:latest\n\n# 2. Verificar que el contenedor esté corriendo\npodman ps -a\n\n# 3. Copiar el archivo JSON de las fichas técnicas al contenedor\npodman cp ./datos/CT_API_Publica.tbl_mongo_collection_specifications.json mongo-semantic:/tmp/specs.json\n\n# 4. Importar el JSON (esto crea automáticamente la BD y la colección)\npodman exec -it mongo-semantic mongoimport \\\n  --db CT_API_Publica \\\n  --collection tbl_mongo_collection_specifications \\\n  --file /tmp/specs.json \\\n  --jsonArray\n\n# 5. Conectar a MongoDB para verificar\npodman exec -it mongo-semantic mongosh\n\n# 6. Dentro de mongosh, verificar los datos:\nuse CT_API_Publica          # Cambiar a la base de datos correcta\nshow collections            # Ver las colecciones (debe aparecer tbl_mongo_collection_specifications)\ndb.tbl_mongo_collection_specifications.countDocuments()  # Contar documentos\ndb.tbl_mongo_collection_specifications.findOne()         # Ver un documento de ejemplo\nexit                        # Salir de mongosh\n\n\n\nAntes de levantar el backend, asegurarse de que el archivo .env en la raíz del proyecto contenga las siguientes variables con sus valores correctos.\n# Conexión a la base de datos SQL\nip=\nport=\nuser=\npwd=\ndb=\n\n# Clave de la API de OpenAI para correr sus modelos\nOPENAI_API_KEY=\n\n# Configuración para el servicio de fichas técnicas\nsucursales_url = \"\"  # Url de la sección de sucursales\nreload_vectors_post = \"https://localhost:8000/internal/reload_vectorstores\"\nurl= ''           # Url del servicio de fichas tecnicas\nToken-api=''\nToken-ct=''\nContent-Type=''\nCookie=''\n\nsucursales_url= \"\"\n\ndominio=\"\"\nboundary=''\n\n# Conexión a MongoDB\nMONGO_URI = \"mongodb://\" # En la URI debe estar incrustrado el nombre de la DB\nMONGO_DB = \"\"\nMONGO_COLLECTION_SESSIONS = \"tbl_sessions\"\nMONGO_COLLECTION_MESSAGE_BACKUP = \"tbl_message_backup\"\nMONGO_COLLECTION_PRODUCTS = \"tbl_productos\"\nMONGO_COLLECTION_SALES = \"tbl_ofertas\"         \nMONGO_COLLECTION_SPECIFICATIONS = \"tbl_mongo_collection_specifications\"\nMONGO_COLLECTION_PEDIDOS=\"tbl_pedidos\"\n\nPODMAN_REDIS_URL=redis://localhost:6379\n\n\n\nEste comando inicia la API, especificando el número de workers, el binding de IP y puerto, y la configuración de SSL/TLS para HTTPS.\nnohup gunicorn ct.main:app   --workers 4   --bind 0.0.0.0:8000   --certfile=static/ssl/cert.pem   --keyfile=static/ssl/key.pem   -k uvicorn.workers.UvicornWorker --timeout 120 --access-logfile -   --error-logfile - &\nEl uso de nogup y & asegura que el proceso continúe ejecutándose en segundo plano incluso si la sesión SSH se cierra.\n\n\n\nSi el certificado SSL autofirmado ha expirado o necesitas uno nuevo:\nopenssl req -x509 -newkey rsa:2048 -nodes -keyout ssl/key.pem -out ssl/cert.pem -days 365\nAsegurarse de que los archivos cert.pem y key.pem estén en la ruta ssl dentro de tu proyecto.\n\n\n\nAl correr la API con nohup, este genera un archivo nohup.out, con el cual podemos ver los logs del sistema, para eso solo hay que ubicarse en donde está dicho archivo y correr lo siguiente:\ntail -f nohup.out\nLos logs también se pueden analizar para el reporte automatizado\nnohup streamlit run run_report.py --server.fileWatcherType none --server.port 3000 &\n\n\n\n\n\nCargar archivos del widget: Los archivos del frontend (principalmente sdk.js y cualquier recurso gráfico como chat.png) deben ser cargados en el servidor donde reside el frontend de la página.\nIncrustar el widget en el HTML: Ejemplo de cómo se puede añadir el widget en la página web donde se desea que aparezca el chatbot.\n\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"es\"&gt;\n&lt;head&gt;\n  &lt;meta charset=\"UTF-8\" /&gt;\n  &lt;title&gt;Prueba del Widget&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n  &lt;script \n    src=\"sdk.js\" \n    data-user-id=\"test\" \n    data-user-key=\"2\" \n    data-api-base=\"https://ctdev.ctonline.mx/chatbot\" \n    data-chat-icon-url=\"chat.png\" \n    type=\"text/javascript\"&gt;\n  &lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\nNotas importantes para el data-api-base:\n\nSi la API corre en HTTP y el frontend en HTTPS, se enfrentarán problemas de “contenido mixto”. La solución propuesta fue usar un archivo PHP (backend del sitio web) como intermediario, el cual es crucial aquí. La API debe apuntar a este PHP y el PHP a su frontend donde se encuentra el widget.\nLa data-api-base es el dominio donde es accesible la API mediante PHP.\n\n\n\n\nPara asegurar que el chatbot tenga acceso a la información más reciente de productos, promociones y fichas técnicas, es necesario ejecutar periódicamente el pipeline ETL. Este proceso extrae, transforma y carga los datos, actualizando la base de datos vectorial utilizada por el sistema RAG.\nPara ejecutar el pipeline ETL, sigue estos pasos:\n\nAcceder al entorno virtual: Asegurarse de estar en el directorio raíz del proyecto (proyectoCT) y activa el entorno virtual donde se instalaron las dependencias del backend.\n\nsource .venv/bin/activate # Para Linux/macOS\n# o `.venv/Scripts/activate` para Windows\n\nEjecutar el pipeline ETL: Una vez activado el entorno, puedes ejecutar una de las funciones dentro del script pipeline.py dependiendo la necesidad.\n\nEn caso de cargar la base general de productos, correr este comando. Recomendación, correr cada 2 o 3 meses, ya que la información técnica cambia con poca frecuencia.\npython3 -c \"from ct.ETL.pipeline import load_products; load_products()\"\nConsejo: si ya se tiene una base de datos vectorial de productos, agregar productos nuevos con el siguiente comando. Esto evita tener que extraer, transformar y cargar todos los productos, simplemente va agregando los faltantes.\npython3 -c \"from ct.ETL.pipeline import update_products; update_products()\"\nEn caso de cargar únicamente los productos en promoción, correr este comando. Eficiente para cada mes que hay productos nuevos en promoción.\npython3 -c \"from ct.ETL.pipeline import load_sales; load_sales()\"\nUna vez que ya se tienen las dos bases vectoriales, es necesario combinarlos y cargarlos.\npython3 -c \"from ct.ETL.pipeline import load_sales_products; load_sales_products()\"\nEn caso de querer actualizar ambas al mismo tiempo, correr este comando. Esto elimina productos antiguos que sean innecesarios almacenar.\npython3 -c \"from ct.ETL.pipeline import update_all; update_all()\"\n\nCrontab del ETL (opcional): Se recomienda automatizar la ejecución de este pipeline (por ejemplo, cada hora entre las 8:30 a 18:30) para mantener actualizada la base de conocimientos del chatbot.\n\nEn sistemas Linux, esto se puede realizar fácilmente mediante un cron job y el archivo reload_all.sh que:\n\nEjecuta src/ct/ETL/update_vector_stores.py usando el python del virtualenv del proyecto.\nSi detecta que el vector store fue regenerado, envía SIGHUP al proceso master de Gunicorn para forzar la recarga de todos los workers.\nRegistra salida en logs/reload_cron_wrapper.log.\n\n# Dar permisos de ejecución al archivo bash\nchmod +x ~/proyectoCT/reload_all.sh\n\n# Probar manualmente \n~/proyectoCT/reload_all.sh\n\n# Revisar el log\ntail -n 200 ~/proyectoCT/logs/reload_cron_wrapper.log\n\n# Si no hubo fallas. Agregar la tarea al cron\ncrontab -e\n\n# Añade la siguiente línea\n30 8-18 * * 1-6 ~/proyectoCT/reload_all.sh &gt;&gt; ~/proyectoCT/logs/reload_cron_wrapper.log 2&gt;&1\n\n# Verificar que se hizo correctamente con\ncrontab -l\n\n# Cuando ya se haya ejecutado el flujo del cron, se puede revisar con\ncat ~/proyectoCT/logs/reload_cron_wrapper.log\nComandos útiles de cron\n\n\n\nAcción\nComando\n\n\n\n\nVer tareas activas\ncrontab -l\n\n\nBorrar todas las tareas\ncrontab -r\n\n\nPausar una tarea\nEditar y comentar la línea con #\n\n\n\nCómo salir del editor\n\nEn nano -&gt; Ctrl + O, Enter, luego Ctrl + X\nEn vi o vim -&gt; I, editar, Esc, luego :wq para guardar o :q! para salir sin guardar\n\n\n\n\nEstas configuraciones son necesarios para que no haya problemas dentro del sistema de conversaciones y el sistema de reportes automatizados.\npython -m spacy download es_core_news_lg\npython -m spacy download es_core_news_sm\n\n\n\n\nProblemas de caché: Es común que los navegadores almacenen versiones antiguas de archivos JS/CSS. Si la interfaz del widget no funciona correctamente después de una actualización, instruye a los usuarios a limpiar la caché de su navegador o a realizar un “hard refresh” (Ctrl+F5). Implementar una estrategia de versioning para los archivos del widget (ej.js?v=1.2.3) puede mitigar esto a futuro.\nRotación de IP para fichas técnicas: El sistema está diseñado para manejar el bloqueo de IP del servicio de fichas técnicas. Se recomienda monitorear los logs de la extracción (extraction.py) para identificar errores 403, lo que indicaría la necesidad de actualizar la IP en el servicio externo."
  },
  {
    "objectID": "documentacion.html#documentación-técnica-del-código",
    "href": "documentacion.html#documentación-técnica-del-código",
    "title": "Documentación",
    "section": "2 Documentación técnica del código",
    "text": "2 Documentación técnica del código\n\n2.1 Estructura de carpetas y módulos\nEl proyecto sigue una estructura modular para facilitar la gestión y el mantenimiento. A continuación, se detalla el propósito de los módulos principales y algunas de sus funciones clave:\nct/langchain/vectorstore.py\nEste módulo implementa la lógica para la creación, carga y consulta de la base de datos vectorial FAISS. Gestiona el retriever que busca los documentos más relevantes.\n\nClases y funciones clave:\n\nclass LangchainVectorStore:\n\n__init__(self, embedder, index_path: str = None): Inicializa el vector store.\n\nPropósito: Inicializa el vector store de Langchain, configurando el embedder y la ruta del índice. Si el índice ya existe, lo carga; de lo contrario, se preparará para crear uno nuevo.\nParámetros:\n\nembedder: Una instancia de la clase embedder utilizada para generar los embeddings (ej., OpenAIEmbeddings(openai_api_key=openai_api_key)).\nindex_path (str): Ruta al directorio donde se guardará o cargará en el índice FAISS.\n\nComportarmiento:\n\nAlmacena el embedder y el index_path.\nSi index_path existe y es un directorio válido, llama a _load_index() para cargar el índice preexistente.\nInicializa self.vectorstore as None hasta que se cargue o cree.\n\n\n_load_index(self):\n\nPropósito: Carga un índice FAISS existente desde disco en self.vectorstore.\nParámetros: Ninguno (usa self.index_path).\nComportamiento:\n\nUtiliza FAISS.load_local() para cargar el índice desde la folder_path especificada en self.index_path, usando el embedder configurado.\nallow_dangerous_deserialization=True se usa para permitir la carga de índices serializados.\nEs una función interna que no debe ser llamada directamente desde fuera de la clase.\n\n\n\n\n\nct/langchain/tool_agent.py\nEste archivo contiene la lógica principal del agente conversacional, incluyendo la interacción con herramientas externas, gestión del historial de conversación, uso de MongoDB, y conexión con el modelo GPT-4.1 a través de LangChain y OpenAI.\n\nClases y funciones clave:\n\nclass ToolAgent:\n\n__init__:\n\nPropósito: Inicializa el agente, configurando el modelo LLM, conectando a MongoDB para la persistencia de sesiones e historial, definiendo el prompt principal del sistema y registrando las herramientas disponibles.\nComportamiento:\n\nEstablece self.model a “gpt-4.1”.\nInicializa las conexiones a las colecciones de MongoDB (sessions, message_backup).\nDefine self.prompt como un ChatPromptTemplate que guía el comportamiento del agente, incluyendo instrucciones de formato de respuesta y el manejo del historial (chat_history).\nDefine self.tools como una lista de objetos Tool y StructuredTool, que el agente puede invocar. Estas herramientas incluyen search_information_tool, inventory_tool y sales_rules_tool, cada una con su descripción y esquema de argumentos (args_schema) cuando aplica.\nself.executor se inicializa a None y se construye bajo demanda.\n\n\nclear_session_history(self, session_id: str) -&gt; bool:\n\nPropósito: Limpia el historial de mensajes (last_messages) para una sesión de usuario específica en la base de datos de MongoDB. Limpia el historial de mensajes para una sesión en particular.\nParámetros:\n\nsession_id (str): El identificador único de la sesión cuyo historial se desea borrar.\n\nRetorna: bool: True si la operación fue exitosa, False en caso de error.\nComportamiento: Actualiza el documento de la sesión en MongoDB, estableciendo last_messages como una lista vacía. Maneja excepciones de PyMongo y otras.\n\nensure_session(self, session: str) -&gt; dict:\n\nPropósito: Garantiza que exista una entrada para la sesion_id en la colección sessions de MongoDB. Si no existe, la crea; si existe, actualiza la marca de tiempo de la última actividad.\nParámetros:\n\nsession_id (str): El identificador de la sesión.\n\nRetorna: dict: El documento de la sesión actualizado o recién creado.\nComportamiento: Utiliza update_one con $setOnInsert y $set para manejar la lógica de upsert y actualización de actividad.\n\nbuild_executor(self):\n\nPropósito: Construye el AgentExecutor de LangChain, que es el componente principal que orquesta la interacción entre el LLM, las herramientas y el prompt.\nParámetros: Ninguno (usa atributos de la clase).\nComportamiento:\n\nCrea un ChatOpenAI LLM con el modelo y la configuración de streaming.\nCrea un agente de funciones de OpenAI (create_openai_functions_agent) vinculando el LLM, las herramientas y el prompt.\nInicializa self.executor como una instancia de AgentExecutor, configurándolo para ser verbose=False y con un max_iterations para controlar la profundidad de la ejecución del agente.\n\n\nrun:\n\nasync def run(\n session_id: str, \n question: str, \n listaPrecio: str = None\n ) -&gt; AsyncGenerator[str, None]: \n\nPropósito: Ejecuta una consulta del usuario a través del agente, gestiona el historial de chat, recopila métricas y transmite la respuesta en tiempo real.\nParámetros:\n\nsession_id (str): ID de la sesión del usuario.\nquestion (str): La pregunta del usuario.\nlistaPrecio (str): El nivel de lista de precios asociado al usuario, usado en el prompt del LLM.\n\nRetorna: AsyncGenerator[str, None]: Un generador asíncrono que cede fragmentos (chunks) de la respuesta a medida que se generan.\nComportamiento:\n\nRecupera el historial completo de la sesión (get_session_history).\nTrunca el historial (trim_messages) para ajustarse a la ventana de contexto del LLM, priorizando los mensajes más recientes.\nInicializa un TokenCostProcess y CostCalcAsyncHandler para el seguimiento de tokens y costos.\nSi el executor no está construido, llama a build_executor().\nDefine los inputs para el executor, incluyendo la query, chat_history, listaPrecio y session_id.\nUtiliza self.executor.astream() para obtener la respuesta en streaming.\nAcumula los fragmentos de la respuesta completa.\nEn el bloque finally, calcula la duración y los metadatos de la interacción.\nPersiste los mensajes del usuario y del asistente en las colecciones sessions y message_backup de MongoDB.\n\nget_session_history(self, session_id: str) -&gt; list[BaseMessage]:\n\nPropósito: Recupera el historial de mensajes de una sesión específica desde MongoDB y lo convierte a objetos BaseMessage de LangChain.\nParámetros:\n\nsession_id (str): El ID del usuario cuyo historial se desea recuperar.\n\nRetorna: list[baseMessage]: Una lista de objetos HumanMessage y AIMessage que representan el historial de conversación.\nComportamiento: Consulta la colección sessions en MongoDB para el session_id dado y mapea los mensajes almacenados a los tipos de mensaje de LangChain.\n\nadd_message\n\ndef add_message(\n session_id: str, \n message_type: str, \n content: str, \n metadata: dict = None): \n\nPropósito: Añade un nuevo mensaje (de usuario o asistente) al historial de last_messages de una sesión en MongoDB, manteniendo un tamaño fijo para optimizar el rendimiento.\nParámetros:\n\nsession_id (str): ID de la sesión.\nmessage_type (str): Tipo de mensaje, puede ser “human” o “assistant”.\ncontent (str): Contenido textual del mensaje.\n\nComportamiento:\n\nCrea un diccionario short_msg con el tipo, contenido y timestamp.\nUtiliza $push con $each, $sort y $slice para añadir el nuevo mensaje y truncar la lista last_messages a los últimos 24 mensajes (configurable).\n\nadd_message_backup\n\ndef add_message_backup(\n session_id: str, \n question: str, \n full_answer: str, \n metadata: dict = None): \n\nPropósito: Guarda un respaldo completo de cada interacción (pregunta del usuario y respuesta completa del asistente) junto con métricas detalladas en la colección message_backup de MongoDB para análisis posterior.\nParámetros:\n\nsession_id (str): ID de la sesión.\nquestion (str): La pregunta original del usuario.\nfull_answer (str): La respuesta completa generada por el asistente.\nmetadata (dict): Diccionario con metadatos adicionales (tokens, costo, duración, modelo utilizado).\n\nComportamiento: Inserta un nuevo documento en message_backup con toda la información relevante para análisis posterior.\nadd_irrelevant_message\n\ndef add_irrelevant_message(\n self,\n session_id: str, \n question: str, \n full_answer: str, \n metadata: dict = None): \n\nPropósito: Guarda un mensaje etiquetado como “irrelevante” en la colección message_backup. Esto es útil para el monitoreo y posible re-entrenamiento del clasificador.\nParámetros:\n\nsession_id (str): ID de la sesión.\nquestion (str): La pregunta del usuario clasificada como irrelevante.\nfull_answer (str): La respuesta generada por el moderador para consultas irrelevantes.\n\nComportamiento: Inserta un nuevo documento en message_backup con el campo label establecido en False.\nmake_metadata\n\ndef make_metadata(\n self,\n token_cost_process: TokenCostProcess,\n duration: float = None) -&gt; dict : \n\nPropósito: Genera un diccionario con metadatos de la interacción, incluyendo información sobre el costo, los tokens utilizados y el tiempo de procesamiento.\nParámetros:\n\ntoken_cost_process (TokenCostProcess): Objeto que contiene información de los tokens.\nduration (float): Duración de la ejecución en segundos.\n\nRetorna: dict: Diccionario con metadatos.\n\n\n\nct/tools/search_information.py\nEste módulo define la herramienta search_information_tool, que permite al chatbot realizar búsquedas semánticas en las bases de datos vectoriales de productos y promociones para encontrar elementos relevantes.\n\nClases y funciones clave:\n\nvectorstore:\n\nPropósito: Una instancia global de LangchainVectorStore que carga el vector store combinado de productos y promociones desde SALES_PRODUCTS_VECTOR_PATH.\n\nretriever_productos:\n\nPropósito: Un retriever configurado para buscar similitudes en el vector store, filtrando específicamente por documentos de la colección “productos”.\nConfiguración: search_type='similarity', k=2 (devuelve los 2 resultados más similares), score_threshold=0.95 (filtra resultados con baja similitud), filter={\"collection\": \"productos\"}.\n\nretriever_promociones:\n\nPropósito: Un retriever configurado de manera similar, pero filtrando por documentos de la colección “promociones”.\nConfiguración: search_type='similarity', k=2 (devuelve los 2 resultados más similares), score_threshold=0.95 (filtra resultados con baja similitud), filter={\"collection\": \"promociones\"}.\n\nparse_page_content (content):\n\nPropósito: Una función auxiliar interna que parsea el page_content de un documento de LangChain (que es una cadena de texto concatenada) de nuevo a un diccionario de clave-valor.\nParámetros:\n\ncontent (str): La cadena de texto del page_content del documento.\n\nRetorna: dict: Un diccionario con las características del producto/promoción.\nComportamiento: Utiliza expresiones regulares para dividir la cadena por . y luego por : para extraer las claves y valores.\n\nsearch_information_tool(query) -&gt; dict:\n\nPropósito: Busca productos y promociones relevantes en las bases de datos vectoriales utilizando la búsqueda semántica.\nParámetros:\n\nquery (str): La consulta de búsqueda del usuario.\n\nRetorna: dict: Un diccionario que contiene dos listas: \"Promociones\" y \"Productos\", donde cada lista contiene diccionarios de los resultados encontrados.\nComportamiento:\n\nInvoca retriever_promociones.invoke(query) y retriever_productos.invoke(query) para obtener los documentos más relevantes de cada colección.\nUtiliza parse_page_content() para transformar el page_content de cada documento recuperado en un formato de diccionario estructurado.\n\n\n\n\nct/tools/inventory.py\nEste módulo define la herramienta inventory_tool, que permite al chatbot consultar la disponibilidad, precio y moneda de un producto específico en la base de datos MySQL.\n\nClases y funciones clave:\n\nclass InventoryInput(BaseModel):\n\nPropósito: Define el esquema de entrada (parámetros) para la herramienta inventory_tool utilizando Pydantic, asegurando la validación de los datos.\nAtributos:\n\nclave (str): La clave única del producto a consultar.\nlistaPrecio (int): El ID de la lista de precios a considerar para la consulta.\n\n\ninventory_tool(clave: str, listaPrecio: int) -&gt; str:\n\nPropósito: Define el esquema de entrada (parámetros) para la herramienta inventory_tool utilizando Pydantic, asegurando la validación de los datos.\nAtributos:\n\nclave (str): La clave del producto.\nlistaPrecio (int): El ID de la lista de precios.\n\nRetorna: str: Una cadena de texto formateada con la información del producto (claave, precio original, moneda, existencias, si está en promoción) o un mensaje de promoción no encontrada.\nComportamiento:\n\nConstruye una consulta SQL que une las tablas productos, existencias, precio y promociones.\nSe conecta a MySQL, ejecuta la consulta con los parámetros proporcionados.\nFormatea el resultado para indicar la moneda (MXN/USD) y el estado de promoción (si el producto en cuestión está o no en promoción).\nIncluye manejo de errores para problemas de conexión a la base de datos o errores inesperados.\n\n\n\n\nct/tools/sales_rules_tool.py\nEste módulo define la herramienta sales_rules_tool, que permite al chatbot aplicar reglas de promoción y calcular el precio final de un producto, considerando la lista de precios y la sucursal del usuario.\n\nClases y funciones clave:\n\nSUCURSALES:\n\nPropósito: Un diccionario global cargado desde un archivo JSON (ID_SUCURSAL) que mapea nemónicos de sucursal a sus IDs.\n\nclass SalesInput(BaseModel):\n\nPropósito: Define el esquema de entrada (parámetros) para la herramienta sales_rule_tool utilizando Pydantic.\nAtributos:\n\nclave (str): La clave única del producto en promoción.\nlistaPrecio (int): El ID de la lista de precios a considerar.\nsession_id (str): El ID de la sesión del usuario, utilizado para inferir la sucursal.\n\n\nobtener_id_sucursal(session_id: str) -&gt; str:\n\nPropósito: Extrae el ID de la sucursal a partir del session_id del usuario, utilizando patrones predefinidos (e.g., “XXCTIN” o nemónicos como “HMO”).\nParámetros:\n\nsession_id (str): El ID de la sesión del usuario.\n\nRetorna: str: El ID de la sucursal como una cadena.\nComportamiento: Utiliza expresiones regulares para extraer el nemónico o ID de la sucursal del session_id y lo busca en el diccionario SUCURSALES. Lanza ValueError si no puede extraer o encontrar la sucursal.\n\nquery_sales():\n\nPropósito: Retorna la consulta SQL para obtener los detalles de la promoción más relevante para un producto, lista de precios y sucursal específicos.\nParámetros: Ninguno.\nRetorna: str: La cadena de la consulta SQL.\nComportamiento: La consulta filtra por promociones activas, producto, lista de precios y sucursal, ordenando por fecha de inicio para obtener la promoción más reciente.\n\nsales_rules_tool(clave: str, listaPrecio: int, session_id: str) -&gt; str:\n\nPropósito: Aplica las reglas de promoción para un producto dado, calculando el precio final y generando un mensaje descriptivo para el usuario.\nParámetros:\n\nclave (str): La clave del producto.\nlistaPrecio (int): El ID de la lista de precios.\nsession_id (str): El ID de la sesión del usuario.\n\nRetorna: str: Una cadena de texto formateada que describe la promoción aplicada (precio, final, descuento, condiciones, vigencia) o un mensaje si el producto no está en promoción.\nComportamiento:\n\nObtiene el id_sucursal usando obtener_id_sucursal().\nSe conecta a MySQL y ejecuta query_sales() para obtener los detalles de la promoción.\nEvalúa diferentes tipos de promociones (precio de oferta, descuento porcentual, “en compra de X recibe Y”).\nCalcula el precio_final y construye un mensaje descriptivo.\nManeja casos donde la promoción no está vigente o el producto no se encuentra en promoción.\nIncluye manejo de errores para problemas de base de datos o errores inesperados.\n\n\n\n\nct/tools/status.py\nEste módulo define la herramienta status_tool, que permite al chatbot obtener el estado actual de un pedido a través de su número de factura.\n\nClases y funciones clave:\n\nclass StatusInput:\n\nPropósito: Modelo de datos Pydantic para validar y describir el argumento de entrada factura requerido por la herramienta.\nComportamiento: Asegura que el número de factura sea una cadena de texto.\n\nstatus_tool(factura: str) -&gt; str:\n\nPropósito: Consulta una base de datos de MongoDB para encontrar el estado de un pedido específico usando su número de factura.\nParámetros: factura (str): El número de factura del pedido.\nRetorna: str: Una descripción textual del estado del pedido (ej. “Pedido entregado al domicilio”, “Pedido en generación”).\nComportamiento:\n\nConecta a la base de datos de MongoDB. Realiza una consulta para encontrar el documentos del pedido por su folio (factura).\nRealiza una consulta para encontrar el documento del pedido por su folio (factura).\nSi el pedido es encontrado, navega al último estado registrado en el historial de estados (estatus).\nUtiliza una declaración match para mapear los estados de la base de datos (ej. ‘Pendiente’, ‘Enviado’, ‘Entregado’) a descripciones amigables para el usuario.\nManeja casos especiales como el estado de ‘Transito’ para formatear la fecha y hora de manera legible.\nDevuelve un mensaje apropiado si el pedido no es encontrado.\n\n\n\n\nct/moderation/query_moderator.py\nEste módulo se encarga de clasificar las consultas del usuario (relevante, irrelevante, inapropiado) y de gestionar el comportamiento inapropiado, incluyendo la aplicación de sanciones temporales.\n\nClases y funciones claves:\n\nclass QueryModerator:\n\n__init__(model: str = \"gemma3:4b\", assistant : ToolAgent = None):\n\nPropósito: Inicializa el moderador de consultas, configurando el modelo LLM para clasificación y una referencia al ToolAgent para interactuar con la base de datos de sesiones.\nParámetros:\n\nmodel (str): Nombre del modelo de Ollama a utilizar para la clasificación de consultas (por defecto “gemma3:4b”).\nassistant (ToolAgent): Instancia del ToolAgent para acceder a la gestión de sesiones en MongoDB.\n\n\nclassify_query(query: str) -&gt; str\n\nPropósito: Clasifica la consulta del usuario como ‘relevante’, ‘irrelevante’ o ‘inapropiado’ utilizando un modelo de lenguaje.\nParámetros:\n\nquery (str): La consulta de texto del usuario.\n\nRetorna: Un str con una de las clasificaciones relevante, irrelevante, o inapropiado.\nComportamiento:\n\nUtiliza ollama.generate() con un system_prompt predefinido (_classification_prompt) para guiar la clasificación del modelo.\nConfigura opciones del modelo como temperature = 0 para un comportamiento determinista.\n\n\n_classification_prompt(self) -&gt; str:\n\nPropósito: Retorna el system prompt utilizado por el modelo de clasificación para categorizar las consultas del usuario.\nParámetros: Ninguno.\nRetorna: str: La cadena de texto del system prompt.\nComportamiento: Define las reglas y ejemplos para que el LLM clasifique las consultas en las tres categorías.\n\npolite_answer(self) -&gt; str:\n\nPropósito: Devuelve una respuesta predefinida y amigable cuando la consulta del usuario es clasificada como ‘irrelevante’.\nParámetros: Ninguno.\nRetorna: str: Una cadena de texto con la respuesta cortés.\nComportamiento: No utiliza un modelo de lenguaje para garantizar rapidez y confiabilidad en la respuesta.\n\nban_answer(self) -&gt; str:\n\nPropósito: Devuelve una respuesta predefinida que advierte al usuario sobre el uso de lenguaje inapropiado y las posibles consecuencias.\nParámetros: Ninguno.\nRetorna: str: Una cadena de texto con el mensaje de advertencia.\nComportamiento: No utiliza un modelo de lenguaje para garantizar rapidez y control de tono.\n\nevaluate_inappropriate_behavior(self, session: dict, query: str):\n\nPropósito: Evalúa el comportamiento inapropiado del usuario, incrementa el contador de intentos y determina la duración de una posible sanción (baneo progresivo).\nParámetros:\n\nsession (dict): El documento de la sesión del usuario, que contiene el historial de intentos inapropiados y el estado de baneo.\nquery (str): La consulta inapropiada actual del usuario.\nRetorna: tuple[str, int, Optional[datetime]]: Una tupla que contiene:\n\nmsg (str): El mensaje de sanción a mostrar al usuario.\ntries (int): El número actualizado de intentos inapropiados.\nbanned_until (Optional[datetime]): La fecha y hora hasta la cual el usuario estará baneado (o None si es solo una advertencia).\n\nComportamiento:\n\nImplementa una lógica de escalamiuento progresivo de sanciones (advertencia, 1 min, 3 min, 10 min, 1 hora, 1 día, 7 días) basada en el número de intentos.\nReinicia el contador de intentos si ha pasado suficiente tiempo desde el último incidente.\n\n\n\ncheck_if_banned(self, session: dict) -&gt; Optional[str]:\n\nPropósito: Verifica si el usuario asociado a una sesión está actualmente baneado. Si el baneo ha expirado, limpia el estado de baneo en la base de datos. Verifica si el usuario está actualmente baneado.\nParámetros:\n\nsession (dict): El documento de la sesión del usuario.\n\nRetorna: Optional[str]: Un mensaje de baneo si el usuario está actualmente restringido, o None si no está baneado o si el baneo ha expirado.\nComportamiento: Compara la fecha y hora actual con la fecha banned_until de la sesión. Si el baneo ha expirado, actualiza la sesión en MongoDB para eliminar el campo banned_until.\n\nupdate_inappropriate_session(self, session, tries, banned_until):\n\nPropósito: Actualiza los campos relacionados con el comportamiento inapropiado (inappropiate_tries, last_inappropiate, banned_until) en el documento de la sesión del usuario en MongoDB.\nParámetros:\n\nsession_id (str): ID de la sesión.\ntries (int): El número de intentos inapropiados.\nbanned_until (Optional[datetime]): La fecha y hora hasta la que el usuario está baneado (o None).\n\nComportamiento: Realiza una operación de update_one en la colección sessions para establecer los campos especificados.\n\n\n\n\nct/langchain/moderated_tool_agent.py\nEste módulo orquesta el flujo completo de una consulta de usuario, incluyendo la moderación de contenido y la delegación de la consulta al agente principal de herramientas (ToolAgent) si es relevante.\n\nClases y funciones claves:\n\nclass ModeratedToolAgent:\n\n__init__(self):\n\nPropósito: Inicializa el ModeratedToolAgent, creando instancias de ToolAgent y QueryModerator y vinculándolos para coordinar el flujo de la conversación.\nComportamiento:\n\nCrea self.tool_agent para manejar la lógica principal del chatbot y la interración con herramientas.\nCrea self.moderator para la clasificación y gestión de comportamiento, pasándole self.tool_agent para que el moderador pueda actualizar el estado de la sesión.\n\n\nrun\n\nasync def run(\n query: str, \n session_id: str = None, \n listaPrecio: str = None\n ) -&gt; AsyncGenerator[str, None]: \n\nPropósito: Ejecuta el flujo completo de una consulta del usuario, desde la verificación de baneo y la clasificación, hasta la generación de la respuesta (relevante, irrelevante, inapropiado) y su streaming.\nParámetros:\n\nquery (str): La consulta de texto del usuario.\nsession_id (str): ID de la sesión del usuario.\nlistaPrecio (str): El nivel de lista de precios asociado al usuario.\n\nRetorna: AsyncGenerator[str, None]: Un generador asíncrono que cede fragmentos (chunks) de la respuesta final del chatbot.\nComportamiento:\n\nAsegura la existencia de la sesión (tool_agent.ensure_session).\nVerifica si el usuario está baneado (moderator.check_if_banned). Si lo está, cede el mensaje de baneo y termina.\nClasifica la query (moderator.classify_query).\nBasado en la label de clasificación:\n\nSi es relevante, delega la ejecución al tool_agent.run() para obtener una respuesta detallada con herramientas.\nSi es irrelevante, genera una respuesta cortés (moderator.polite_answer()) y la registra.\nSi es inapropiado, evalúa el comportamiento (moderator.evaluate_inappropriate_behavior()), actualiza la sesión (moderator.update_inappropriate_session()) y cede el mensaje de sanción.\nSi la clasificación no es reconocida, cede un mensaje de error genérico.\n\n\n\n\n\nct/chat.py\nEste módulo define los endpoints de la API FastAPI para la interacción del chat y la gestión del historial de sesiones, sirviendo como la interfaz principal entre el frontend y la lógica del chatbot.\n\nFunciones clave:\n\nassistant = ModeratedToolAgent():\n\nPropósito: Inicializa una instancia global de ModeratedToolAgent que será utilizada por todos los endpoints del chat.\nComportamiento: Se crea una única instancia para mantener el estado y las conexiones a la base de datos.\n\nget_chat_history(user_id: str) -&gt; list[dict[str, str]]:\n\nPropósito: Devuelve el historial de chat de un usuario específico en un formato JSON amigable para el frontend. Recupera el historial de chat de un usuario específico.\nParámetros:\n\nuser_id (str): El ID del usuario cuyo historial se desea recuperar.\n\nRetorna: List[Dict[str, str]]: Una lista de diccionarios, donde cada diccionario representa un mensaje con las claves role (user o bot) y content (el texto del mensaje). Retorna una lista vacía si no hay historial.\nComportamiento: Llama a assistant.tool_agent.get_session_history() para obtener el historial de LangChain y lo transforma al formato JSON deseado.\n\nasync_chat_generator(request: QueryRequest) -&gt; AsyncGenerator[str, None]:\n\nPropósito: Un generador asíncrono que envuelve la función run de la clase ModeratedToolAgent para permitir el streaming de respuestas a los clientes de la API.\nParámetros:\n\nrequest (QueryRequest): Un objeto Pydantic que contiene la consulta del usuario (user_query), el ID del usuario (user_id), y la lista de precios (listaPrecio).\n\nRetorna: AsyncGenerator[str, None]: Cede los fragmentos de respuesta directamente desde el assistant.run().\n\nasync_chat_endpoint(request: QueryRequest) -&gt; StreamingResponse:\n\nPropósito: El endpoint HTTP POST principal para recibir nuevas consultas de chat de los usuarios y devolver respuestas en streaming (Server-Sent Events).\nParámetros\n\nrequest (QueryRequest): Objeto de solicitud Pydantic con la consulta del usuario, ID de usuario y lista de precios.\n\nRetorna: StreamingResponse: Una respuesta HTTP que permite al cliente recibir los fragmentos de la respuesta en tiempo real a medida que se generan.\nComportamiento: Envuelve el async_chat_generator en una StreamingResponse con media_type=\"text/event-stream\".\n\ndelete_chat_history_endpoint(user_id: str) -&gt; str:\n\nPropósito: Endpoint HTTP DELETE para eliminar el historial de chat de un usuario específico.\nParámetros:\n\nuser_id (str): El ID del usuario cuyo historial se va a eliminar.\n\nRetorna: str: Una cadena “success” si la eliminación fue exitosa.\nComportamiento: Llama a assistant.tool_agent.clear_session_history() para borrar el historial. Maneja excepciones y levanta una HTTPException en caso de error interno.\n\n\n\nct/main.py\nEs el archivo principal de la aplicación FastAPI. Configura la aplicación, habilita CORS y registra los endpoints definidos en ct.chat.\n\nFunciones clave:\n\napp = FastAPI: Inicializa la aplicación FastAPI.\napp.add_middleware(CORSMiddleware, ...): Configura el middleware de CORS para permitir solicitudes desde cualquier origen, métodos y cabeceras, lo cual es crucial para la integración del widget en diferentes dominios.\n@app.get(\"/history/{user_id}\"): Decorador que mapea la ruta GET /history/{user_id} a la función handle_history.\nhandle_history(user_id: str): Llama a get_chat_history de ct.chat para obtener y retornar el historial.\n@app.post(\"/chat\"): Decorador que mapea la ruta POST /chat a la función handle_chat.\nhandle_chat(request: QueryRequest): Llama a async_chat_endpoint de ct.chat para manejar la solicitud de chat y el streaming de la respuesta.\n@app.delete(\"/history/{user_id}\"): Decorador que mapea la ruta DELETE /history/{user_id} a la función handle_delete_history.\nhandle_delete_history(user_id: str): Llama a delete_chat_history_endpoint de ct.chat para borrar el historial de un usuario.\nif __name__ == \"__main__\":: Bloque de ejecución principal para correr la aplicación con Uvicorn en desarrollo. En producción con Gunicorn.\n\n\nct/ETL/extraction.py:\nMódulo de la capa de Extracción. Responsable de conectarse a la base de datos MySQL para extraer información de productos y promociones, y de interactuar con el servicio externo para obtener fichas técnicas.\n\nClases y funciones clave:\n\nclass Extraction:\n\n__init__(): Inicializa la clase con los parámetros de conexión a MySQL y configura un cloudscraper para la extracción de fichas técnicas.\n\nParámetros: Ninguno explícito, lee de ct.clients.\nComportamiento: Establece scraper con headers personalizados para los tokens de la API y cookies.\n\nids_query() -&gt; str: Retorna la consulta SQL para obtener IDs de productos válidos (con existencias y precios).\nget_valid_ids() -&gt; list: Ejecuta la consulta ids_query y retorna una lista de IDs de productos válidos.\n\nRetorna: list: Lista de IDs de productos.\nComportamiento: Se conecta a MySQL, ejecuta la consulta y maneja errores de conexión.\n\nproduct_query(id) -&gt; str: Retorna la consulta SQL para obtener detalles de un producto específico por ID. Incluye detalles de precios por lista, categoría, marca, etc.\nget_products() -&gt; pd.DataFrame: Extrae la información de todos los productos válidos desde MySQL.\n\nRetorna: pd.DataFrame: Un DataFrame de Pandas con la información de los productos.\nComportamientos: Itera sobre los IDs válidos, ejecuta product_query para cada uno y consolida los resultados en un DataFrame.\n\ncurrent_sales_query() -&gt; str: Retorna la consulta SQL para obtener las promociones vigentes.\nget_current_sales() -&gt; pd.DataFrame: Extrae las promociones vigentes desde MySQL.\n\nRetorna: pd.DataFrame: Un DataFrame de Pandas con la información de las promociones.\n\nget_specifications_cloudscraper:\n\nget_specifications_cloudscraper(\n    claves: List[str],\n    max_retries: int = 3,\n    sleep_seconds: float = 0.15\n) -&gt; Dict[str, dict]\nIntenta obtener las fichas técnicas para una lista de claves de productos desde un servicio externo, utilizando cloudscraper para manejar posibles protecciones como Cloudflare.\n\nParámetros:\n\nclaves (List[str]): Lista de claves de productos.\nmax_retries (int): Número máximo de reintentos por cada clave.\nsleep_seconds (float): Tiempo inicial de espera entre reintentos (con backoff exponencial).\n\nRetorna: Dict[str, dict]: Un diccionario donde la clave es la claveProducto y el valor es la ficha técnica en formato JSON.\nComportamiento: Realiza solicitudes POST al url del servicio de fichas técnicas. Implementa lógica de reintentos con backoff controlado y maneja diversos errores HTTP (ej., 403 Forbidden) y errores de JSON/red.\n\n\n\nct/ETL/transform.py\nMódulo de la capa de Transformación. Se encarga de limpiar, unificar y normalizar los datos extraídos, y de persistir las fichas técnicas en MongoDB.\n\nClases y funciones claves:\n\nclass Transform:\n\n__init__(): Inicializa la clase Transform, creando una instancia de Extraction y configurando la conexión a la colección de especificaciones de producto.\n\nParámetros:\n\nspecifications (dict): El diccionario que contiene los datos de la ficha técnica de un producto.\n\nRetorna: Un diccionario estructurado con fichaTecnica (pares nombre-valor de características) y resumen (descripciones cortas y largas).\nComportamiento: Navega a través de la estructura anidada de ProductFeature y SummaryDescription para extraer la información.\n\ntransform_specifications(specs: dict) -&gt; dict: Transforma múltiples especificaciones brutas (obtenidas del servicio externo) en un formato limpio.\n\nParámetros:\n\nspecs (dict): Diccionario de fichas técnicas brutas, donde la clave es la claveProducto.\n\nRetorna: Diccionario con fichas técnicas transformadas y limpias, listas para ser usadas o guardadas.\n\ntransform_products() -&gt; pd.DataFrame: Transforma los datos brutos de productos obtenidos de MySQL en un DataFrame limpio y estandarizado.\n\nRetorna: Un DataFrame con columnas relevantes, detalles concatenados y detalles_precio parseado de JSON.\n\nclean_products() -&gt; dict: Limpia los datos de productos y los enriquece con las fichas técnicas. Primero busca en MongoDB y si no encuentra, las extrae y las guarda.\n\nRetorna: Un diccionario de productos limpios y enriquecidos, listos para la carga final.\nComportamiento: Identifica las claves de productos para las que faltan fichas técnicas en MongoDB, las extrae usando self.data.get_specifications, las transforma y las guarda en la colección specifications. Finalmente, combina las fichas técnicas existentes y nuevas con los datos de productos.\n\ntransform_sales() -&gt; pd.DataFrame: Transforma los datos brutos de promociones (ventas) en un DataFrame limpio.\n\nRetorna: Un DataFrame con información de promociones, fechas formateadas y descuentos con símbolo de porcentaje.\n\nclean_sales() -&gt; dict: Limpia los datos de promociones y los enriquece con fichas técnicas de manera similar a clean_products().\n\nRetorna: Un diccionario de promociones limpias y enriquecidas con fichas técnicas.\n\n\n\n\nct/ETL/load.py\nMódulo de la capa de Carga. Maneja la inserción de los datos transformados en las colecciones de MongoDB y la construcción de la base de datos vectorial.\n\nClases y funciones clave:\n\nclass Load:\n\n__init__(): Inicializa la clase Load, creando una instancia de Transform y configurando las conexiones a las colecciones de MongoDB (products, sales, specifications) y el embedder de OpenAI.\nbuild_content(product: dict, product_features: list) -&gt; str: Contruye el contenido textual de un documento a partir de un diccionario de producto y una lista de características.\n\nParámetros:\n\nproduct (dict): Un diccionario con los datos de un producto.\nproduct_features (list): Lista de claves de características a incluir en el contenido.\n\nRetorna: Una cadena de texto concatenada que resume el producto, adecuada para la vectorización.\n\nmongo_products(): Carga las promociones limpias (procesadas por Transform) en la colección products de MongoDB, realizando upserts.\nmongo_sales(): Carga las promociones limpias (procesadas por Transform) en la colección sales de MongoDB, realizando upserts.\nload_products() -&gt; List[Document]: Carga los productos desde la colección products de MongoDB y los convierte en objetos langchain.schema.Document.\n\nRetorna: Una instancia del índice FAISS.\n\nproducts_vs(): Crea o actualiza el vector store para productos.\n\nComportamiento: Carga los productos desde MongoDB, los vectoriza en lotes (batch_size = 250) para optimizar el uso de memoria, y guarda el índice FAISS localmente en PRODUCTS_VECTOR_PATH.\n\nsales_products_vs(): Crea o actualiza el vector store para ventas/ofertas.\n\nComportamiento: Carga las ofertas desde MongoDB. Luego, carga el vector store de productos existente desde PRODUCTS_VECTOR_PATH y añade las ofertas a este índice (también en lotes), guardando el índice combinado en SALES_PRODUCTS_VECTOR_PATH.\n\n\n\n\nct/ETL/pipeline.py\nEste módulo actúa como el orquestador principal del proceso ETL (Extracción, Transformación, Carga). Centraliza la ejecución de las etapas de obtención, limpieza, enriquecimiento y carga de datos, asegurando que los vector stores de productos y promociones estén siempre actualizados.\n\nClases y funciones clave:\n\nrun_etl_pipeline():\n\nPropósito: Función principal que coordina la ejecución secuencial de todas las fases del pipeline ETL.\nComportamiento:\n\nInicializa una instancia de la clase Load.\nInvoca load.products_vs() para crear o actualizar el vector store de productos con los datos preparados.\nInvoca load.products_vs() para crear o actualizar el vector store de productos con los datos preparados.\nInvoca load.load_sales() para extraer, transformar y preparar los datos de promociones.\nInvoca load.sales_products_vs() para crear o actualizar el vector store de promociones, integrándolos con el vector store de productos existente,\n\nUso: Diseñada para ser el punto de entrada para la actualización programada o manual de la base de conocimientos del chatbot.\n\n\n\n\n\n2.2 Modelos LLM utilizados\nEl sistema utiliza una combinación estratégica de modelos de lenguaje para optimizar la funcionalidad y los costos:\n\nClasificación de consultas y respuestas iniciales (Ollama - gemma3:4b):\n\nFunción: Este modelo open-source, cargado localmente a través de Ollama, es el primer punto de contacto. Su función principal es clasificar las consultas de los usuarios como relevantes (productos que se ofrecen en la empresa), irrelevantes (cualquier producto o tema fuera del ámbito de negocio), o inapropiado (lenguaje ofensivo).\nVentaja: Permite una gestión eficiente de consultas no relacionadas con el negocio sin incurrir en costos de API’s comerciales, y es ideal para respuestas de “baneo” o corteses.\n\nGeneración de respuestas relevantes (OpenAI - gpt-41):\n\nFunción: Este modelo avanzado de OpenAI es el encargado de generar las respuestas detalladas y contextualizadas para las consultas clasificadas como relevantes. Trabaja en conjunto con la cadena RAG para integrar la información recuperada de la base de datos vectorial.\nVentaja: Ofrece alta calidad y precisión en las respuestas, especialmente en el manejo de precios, promociones complejas y detalles de productos, lo que fue validad en pruebas comparativas.\n\n\n\n\n2.3 Puntos de entrada y funciones clave\nEstos son los principales puntos de inicio para interactuar con las funcionalidades del chatbot. Es crucial que aquí se documenten las funciones y clases directamente expuestas o que inician un flujo principal.\n\nModeratedToolAgent.run:\n\nModeratedToolAgent.run(\n  query: str, \n  session_id: str = None, \n  listaPrecio: str = None\n  ) -&gt; AsyncGenerator[str, None]:\n\nEs la función principal que orquesta el glujocompleto de una consulta de usuario.\nParámetros:\n\nquery (str): La pregunta o entrada del usuario.\nsession_id (str): Un identificador único para la sesión del usuario, utilizado para mantener el historial correspondiente.\nlistaPrecio (str): El nivel o clave de precio específico del cliente, que se pasa al LLM para asegurar la precisión de los precios en las consultas dinámicas (ej. precios y promociones).\n\nComportamiento:\n\nVerifica si el usuario está actualmente baneado (usando QueryModerator.check_if_banned). Si es así, retorna un mensaje de baneo.\nClasifica la query (usando QueryModerator.classify_query) en una de las categorías: relevante, irrelevante, o inapropiado.\nBasado en la clasificación:\n\n\nSi es relevante, delega la ejecución al ToolAgent.run() para obtener una respuesta detallada con el uso de herramientas y la base de datos vectorial.\nSi es irrelevante, retorna una respuesta predefinida y cortés (QueryModerator.polite_answer()) y registra la interacción.\nSi es inapropiado, evalúa el comportamiento (QueryModerator.evaluate_inappropriate_behavior()), actualiza el estado de baneo en la sesión del usuario (QueryModerator.update_inappropriate_session()) y retorna el mensaje de sanción apropiado.\n\n\nCede fragmentos de la respuesta en tiempo real (streaming).\nRegistra la interacción completa, incluyendo la pregunta, respuesta y metadatos relevantes para análisis futuro.\n\nToolAgent.run:\n\nToolAgent().run(\n session_id: str, \n question: str, \n listaPrecio: str\n ) -&gt; AsyncGenerator[str, None]: \n\nPropósito: Responde a consultas relevantes de productos, utilizando el agente de LangChain con acceso a herramientas y memoria de conversación. Es la función que el ModeratedToolAgent invoca cuando una consulta es clasificada como relevante.\nParámetros:\n\nsession_id (str): ID de la sesión para recuperar/actualizar el historial.\nquestion (str): La pregunta original del usuario.\nlistaPrecio (str): El nivel de precios del cliente.\n\nComportamiento:\n\nAsegura la existencia de la sesión en MongoDB (ensure_session).\nRecupera el historial de la sesión (get_session_history) y lo trunca para ajustarse a la ventana de contexto del LLM (trim_messages).\nInicializa el AgentExecutor si no ha sido construido.\nEjecuta la query a través del AgentExecutor de LangChain, que orquesta el uso del LLM y las herramientas (search_information_tool, inventory_tool, sales_rules_tool) según la necesidad de la consulta.\nTransmite la respuesta en fragmentos (astream()).\nRegistra la interacción completa (pregunta, respuesta, métricas como tokens y costo) en las colecciones sessions y message_backup de MongoDB.\n\nload.py::products_vs() y load.py::sales_products_vs():\n\nPropósito: Funciones clave para la creación y actualización incremental de las bases de datos vectoriales (FAISS) de productos y ofertas, respectivamente. Orquestan el proceso de carga de documentos desde MongoDB y su vectorización.\nComportamiento:\n\nproducts_vs(): Carga todos los productos de la colección products de MongoDB, los convierte a langchain.schema.Document y los vectoriza en lotes (batch_size=250) utilizando OpenAIEmbeddings. Finalmente, guarda el índice FAISS resultante en PRODUCTS_VECTOR_PATH.\nsales_products_vs(): Carga las ofertas de la colección sales de MongoDB. Luego, carga el índice de productos existente (desde PRODUCTS_VECTOR_PATH) y añade las ofertas a este mismo índice, también en lotes (batch_size=200). Finalmente, guarda el índice combinado (productos + ofertas) en SALES_PRODUCTS_VECTOR_PATH. Esta estrategia asegura que las ofertas se integren sin necesidad de re-vectorizar todo el catálogo de productos, optimizando tiempo y recursos."
  },
  {
    "objectID": "documentacion.html#guía-de-entrenamiento-y-mejora",
    "href": "documentacion.html#guía-de-entrenamiento-y-mejora",
    "title": "Documentación",
    "section": "3 Guía de entrenamiento y mejora",
    "text": "3 Guía de entrenamiento y mejora\nFlujo de Datos (ETL)\nLa información que alimenta la base de conocimientos del chatbot sigue un proceso ETL (Extracción, Transformación, Carga) estructurado que garantiza que el modelo tenga acceso a datos actualizados, limpios y ricos en contexto semántico.\nPara una descripción detallada de cada etapa del proceso ETL (Extracción de MySQL y servicio externo de fichas técnicas, Transformación para limpieza, unificación y estructuración, y Carga en MongoDB y la base de datos vectorial FAISS), por favor, consulta el documento “Preparación de los datos”.\n\n3.1 Generación de la base de datos vectorial\nLa base de datos vectorial FAISS es el corazón del sistema RAG, almacenando las representaciones vectoriales de la información de productos y promociones para búsquedas de similitud eficientes. Su generación y actualización son parte integral del proceso ETL, el cual asegura que el modelo tenga acceso a una base de conocimiento robusta y actualizada.\nLos detalles sobre el proceso de creación del índice vectorial, el uso de embeddings, la estrategia de procesamiento en lotes y la inclusión incremental de ofertas se encuentran descritos exhaustivamente en el documento “Preparación de los datos”.\n\n\n3.2 Recomendaciones para futura mejora\n\nEmbeddings locales (Ollama):\n\nPara reducir los costos asociados con los embeddings de OpenAI y disminuir la dependencia de servicios externos, se recomienda realizar pruebas con los modelos de embeddings disponibles a través de Ollama (u otras librerías de embeddings locales).\n\nIndexado incremental:\n\nActualmente, la actualización de la base vectorial puede implicar procesar grandes lotes. Para un mantenimiento más eficiente, especialmente si solo cambian algunos productos o sus precios/promociones, se podría implementar una función de actualización a nivel de producto.\n\nMonitoreo avanzado de rendimiento:\n\nAunque ya se registran métricas de costo y duración, se puede profundizar en el monitoreo.\n\nOptimización del manejo de promociones complejas:\n\nLas promociones tipo “en compra X lleva Y” aún presenta desafíos. Podría ser necesario enriquecer el contexto del embedding para estos productos específicos, reglas más explícitas dentro del LLM o creación de plantillas de prompts más específicos para estas promociones."
  },
  {
    "objectID": "documentacion.html#diagrama-de-arquitectura",
    "href": "documentacion.html#diagrama-de-arquitectura",
    "title": "Documentación",
    "section": "4 Diagrama de arquitectura",
    "text": "4 Diagrama de arquitectura\nEl siguiente diagrama ilustra la arquitectura general del ssitema del chatbot, mostrando los componentes principales y el flujo de datos desde la interacción del usuario hasta la generación de respuestas y el almacenamiento del historial. Se ha actualizado para reflejar la implementación de MongoDB y los diferentes flujos.\n\n4.1 Componentes clave\n\nInterfaz de usuario (widget del chatbot): El componente frontal incrustado en la página web de CT Internacional, permitiendo la interacción directa del usuario.\nServicio intermediario PHP: Actúa como un puente seguro entre el frontend (HTTPS) y la API del chatbot (HTTPS pero con certificado autofirmado, o sea, no seguro) para resolver problemas de contenido mixto. Si el backend ya está en en HTTPS con un certificado SSL seguro, este componente puede ser obviado.\nAPI del chatbot (FastAPI): El servicio backend principal que procesa las consultas de los usuarios, orquesta la recuperación de información y se comunica con los modelos de lenguaje.\nBase de datos de productos y promociones (MySQL): Almacena la información transaccional y de precios de los productos y promociones de CT Internacional. Es la fuente original de los datos.\nServicio de fichas técnicas: Fuente de datos para la información detallada y semi-estructurada (XML) de los productos.\nMódulo ETL: Proceso automatizado que:\n\nExtrae datos de MySQL y el servicio de fichas técnicas.\nLimpia, unifica, y transforma los datos, persistiendo las fichas técnicas en MongoDB.\nCarga los datos limpios en colecciones de MongoDB (products, sales, specifications).\n\nBase de datos NoSQL (MongoDB): Almacena las fichas técnicas (specifications), así como el historial de las interacciones de los usuarios (sessions, message_backup).\n\nsessions: Mantiene los últimos n mensajes de cada usuario para recuperación rápida y una experiencia de usuario fluida.\nmessage_backup: Almacena un historial completo de todas las consultas y respuestas con métricas detalladas para fines de análisis y reportes.\n\nModelo de embeddings: Componente encargado de transformar tanto las consultas de los usuarios como la información de los productos en representaciones vectoriales numéricas (usando OpenAI Embeddings).\nBase de datos vectorial (FAISS): Almacena las representaciones vectoriales de la información de productos y ofertas, permitiendo búsquedas de similitud eficientes. Se actualiza con datos del ETL.\nClasificador de consultas (Ollama - gemma3:4b): Componente central que, para consultas relevantes, coordina la búsqueda de información en la base de datos vectorial y contextualiza esta información con la consulta del usuario.\nLLM: El motor principal del chatbot para consultas relevantes, responsable de generar respuestas coherentes y detalladas en base a la consulta contextualizada por el RAG.\nSistema de reportes automatizados: Procesa los datos del message_backup en MongoDB para generar insights sobre el uso del chatbot, intereses de los clientes, costos y rendimiento.\n\n\n\n4.2 Flujo de interacción principal\n\nEl usuario interactúa con el widget del chatbot en la página web.\nLa consulta se envía a la API del chatbot (potencialmente vía el servicio intermediario PHP)\nLa API crea o continúa una sesión de conversación, y la consulta es pasada al asistente conversacional.\nSi la consulta es relevante:\n\n\nsearch_information_tool: busca productos relevantes.\ninventory_tool: obtiene precios, existencias y moneda por clave de producto.\nsales_rules_tool: calcula el precio final considerando promociones o reglas de negocio.\nLa información recuperada se contextualiza y se envía al LLM para generar la respuesta al usuario.\n\n\nSi la consulta es irrelevante o inapropiada, el clasificador genera una respuesta adecuada (cortés o de advertencia) directamente al usuario.\nTodas las interacciones (consultas y respuestas) se registran en las colecciones sessions y message_backup en MongoDB.\nEl sistema de reportes automatizados accede a message_backup para generar análisis de datos.\n\n\n\n4.3 Flujo de datos\n\nEl módulo ETL extrae datos de MySQL y el servicio de fichas técnicas.\nLos datos se transforman y las fichas técnicas se cargan en MongoDB (colección specifications).\nLos datos transformados de products y sales (incluyendo las fichas técnicas obtenidas de MongoDB) son utilizados por el módulo ETL para construir y actualizar la base de datos vectorial.\n\n\n\n\nArquitectura del sistema"
  },
  {
    "objectID": "modelado.html",
    "href": "modelado.html",
    "title": "Modelado y Evaluación",
    "section": "",
    "text": "El objetivo de esta fase es desarrollar la arquitectura del sistema de recuperación aumentada con generación (RAG) y las herramientas que utilizará el chatbot para la información dinámica. Para ello, se utiliza como fuente de conocimiento la base de datos vectorizada construida en la etapa anterior.\n\n\nDado que el enfoque del proyecto se basa en herramientas de OpenAI, los modelos considerados para esta fase son los siguientes:\n\nGPT-4o-mini:\nUna versión ligera de GPT-4o, diseñada para ofrecer un buen balance entre costo, velocidad de respuesta y calidad en tareas de lenguaje natural. Es ideal para pruebas rápidas o implementaciones donde se requiere eficiencia.\nGPT-4o:\nModelo multimodal de última generación de OpenAI, capaz de procesar texto, imágenes y audio. En este proyecto se utiliza solo su capacidad textual, destacando por su mayor comprensión semántica y coherencia en las respuestas.\nModelos open-source integrados mediante Ollama:\nOllama permite correr modelos de lenguaje open-source de manera local o privada. En este proyecto se contemplan modelos como LLaMA o Mistral, que ofrecen alternativas de código abierto con buen rendimiento en tareas conversacionales.\n\n\n\n\nLa implementación del sistema se basa en una estructura modular orientada a clases. Esto permite una mayor reutilización de código, facilita su mantenimiento y mejora la legibilidad, aspectos clave para futuras modificaciones o revisiones.\nAdemás, esta estructura permite importar únicamente la clase necesaria para ejecutar todo el sistema, lo cual es ideal para su integración a través de una API. De esta forma, se evita depender de notebooks o archivos extensos y poco escalables.\nEl sistema se construyó utilizando principalmente la librería LangChain, la cual ofrece una base robusta para conectar modelos de lenguaje con herramientas externas y flujos personalizados.\n\n\nA partir de los datos mencionados en el apartado anterior, procederemos a crear la base de datos vectorial con esta información. Tomamos los datos limpios y transformados directamente de las funciones de limpieza (clean_products y clean_sales en ct/ETL/transform.py) y los convertimos en un tipo Document para poder pasarlo a FAISS (base vectorial) junto con los embeddings y guardarlo de forma local.\ncampos = [\n        \"nombre\",\n        \"producto\",\n        \"categoria\",\n        \"marca\",\n        \"tipo\",\n        \"modelo\",\n        \"detalles\",\n        \"fichaTecnica\",\n        \"resumen\"\n        ]\n\ndocs = [\n    Document(\n        page_content=construir_contenido(producto, campos), # recibe la información de cada producto y las columnas \n        metadata={\"collection\": 'promociones'} # o 'productos' dependiendo el caso\n    )\n    for producto in productos\n]\n\n# Usar embeddings de OpenAI \nembeddings = OpenAIEmbeddings(api_key=api_key)\n\n# Crear base de datos FAISS con los documentos\nvectorstore = FAISS.from_documents(docs, embeddings)\n\n# Guardar la base de datos para futuras consultas\nvectorstore.save_local()\n\n\n\nAdemás de la información estática precargada en la base vectorial, el sistema cuenta con herramientas dinámicas que permiten consultar datos actualizados en tiempo real, como existencias, precios y promociones. Estas herramientas se integran al agente a través del framework LangChain, lo que permite invocarlas solo cuando el modelo detecta que son necesarias para responder con precisión.\nLas herramientas dinámicas disponibles son:\n\nsearch_information_tool: realiza búsquedas básicas en los productos embebidos para encontrar coincidencias.\ninventory_tool: consulta las existencias, precio actual y moneda para un producto específico.\nsales_rules_tool: calcula promociones y reglas de venta que aplican a un producto según su lista de precios y sucursal.\n\n        self.tools = [\n            Tool(\n                name='search_information_tool',\n                func=search_information_tool,\n                description=\"Busca productos relacionados con lo que se pide.\"\n            ),\n            StructuredTool.from_function(\n                func=inventory_tool,\n                name='inventory_tool',\n                description=\"Esta herramienta sirve como referencia y devuelve precios, moneda y existencias de un producto por su clave y listaPrecio.\",\n                args_schema=ExistenciasInput # Explicitly link the Pydantic schema\n            ),\n            StructuredTool.from_function(\n            func=sales_rules_tool,\n            name='sales_rules_tool',\n            description=\"Aplica reglas de promoción, devuelve el precio final y mensaje para mostrar al usuario.\",\n            args_schema=SalesInput\n        )\n]\nEstas herramientas son invocadas automáticamente por el agente cuando la consulta del usuario requiere información que no está contenida en el contexto estático. Esto permite entregar respuestas más precisas y alineadas con la situación real del negocio (existencias, promociones activas, etc.).\n\n\n\nEl sistema se alimenta con información a través de un proceso ETL (Extracción, Transformación y Carga) que asegura que los datos estén limpios, estructurados y listos para ser utilizados por el modelo y las herramientas.\nPara una descripción detallada de cada etapa del proceso ETL, incluyendo la extracción de datos, la transformación (y el almacenamiento de fichas técnicas en MongoDB), y la carga directa a la base de datos vectorial, por favor, consultar el documento “Preparación de los datos”.\nEn resumen, este proceso garantiza que el modelo tenga acceso a una base de datos de conocimiento robusta y actualizada, tanto estática (productos y promociones embedidas) como dinámica (a través de las herramientas que consultan datos en tiempo real).\n\n\n\n\nDurante la ejecución del sistema, los modelos de lenguaje no operan en aislamiento. Se alimentan con diversos atributos y herramientas que enriquecen la interacción y permiten generar respuestas precisas y contextualizadas. A continuación, se describen los principales elementos que intervienen en este proceso y cómo la información preparada se integra en el modelo:\nAtributos del modelo en tiempo de ejecución\n\nquery: Pregunta o instrucción directa del usuario. Es el punto de entrada para iniciar el procesamiento.\nsession_id: Identificador de sesión que permite obtener el contexto del usuario (incluye la sucursal asociada para aplicar reglas de negocio como promociones).\nlistaPrecio: Parámetro numérico que indica la lista de precios relevante para consultas de productos y promociones.\n\nEstos atributos permiten personalizar las respuestas con base en el usuario que consulta, su sucursal, y las reglas comerciales que le aplican.\nAlimentación del modelo con información adicional\nEl LLM se alimenta con información contextualizada de dos maneras principales, ambas derivadas de los datos procesados en la fase de Preparación de los Datos:\n\nInformación estática (a través del RAG):\n\nProviene de la base de datos vectorial (FAISS) construida con los productos y promociones previamente embedidos mediante OpenAIEmbeddings.\nCuando el usuario realiza una query relevante, el sistema RAG busca los documentos más similares en el vector store. Estos documentos (page_content y metadata) se inyectan en el context window del LLM como información de referencia.\nEsto se activa principalmente para consultas generales de productos, descripciones, características, comparativas, etc., permitiendo al LLM generar respuestas basadas en un conocimiento específico y actualizado de tu catálogo.\n\nInformación dinámica (a través de herramientas LangChain):\n\nSe accede a datos en tiempo real mediante herramientas personalizadas integradas con LangChain, como inventory_tool y sales_rules_tool.\nEl LLM, basado en la query del usuario y su propio razonamiento, decide cuándo invocar estas herramientas. Por ejemplo, si el usuario pregunta por “el precio de la clave X”, el LLM activará inventory_tool con la clave proporcionada.\nEl resultado de la ejecución de estas herramientas (e.g., el precio actual, las existencias, el precio final con promoción) se devuelve al LLM y se inyecta también en su context window.\nEsto permite al LLM generar respuestas con datos actualizados y específicos, como la disponibilidad de un producto o el precio final con promociones activas para una listaPrecio y session_id dados.\n\n\nModeración y clasificación de la consulta\nAntes de ejecutar cualquier acción, la consulta pasa por una etapa de moderación:\n\nSe valida si el usuario está baneado (por comportamiento inapropiado).\nSe clasifica la consulta como relevante, irrelevante o inapropiada.\nDependiendo de esta clasificación, se permite o bloquea el paso al modelo principal y las herramientas.\n\nEste flujo asegura robustez, control y trazabilidad en la interacción con el modelo.\n\n\n\nA diferencia de los modelos clásicos de machine learning (ML), la evaluación de sistemas basados en modelos de lenguaje grande (LLMs) requiere enfoques distintos, centrados en la calidad de las respuestas generadas.\nEn este proyecto, la evaluación se realiza mediante un análisis cualitativo de las respuestas del chatbot, tomando en cuenta los siguientes criterios:\n\nLa información sobre productos, descripciones y características debe estar alineada con los datos disponibles en la base vectorial.\nLas respuestas deben ser claras, concisas y coherentes, evitando alucinaciones o información incorrecta.\nLos precios deben coincidir con los establecidos en la base de datos, y en el caso de promociones, estas deben estar correctamente aplicadas, evitando errores que impliquen pérdidas económicas.\n\nEstos criterios serán evaluados por los expertos y personas con conocimiento en la empresa."
  },
  {
    "objectID": "modelado.html#modelado",
    "href": "modelado.html#modelado",
    "title": "Modelado y Evaluación",
    "section": "",
    "text": "El objetivo de esta fase es desarrollar la arquitectura del sistema de recuperación aumentada con generación (RAG) y las herramientas que utilizará el chatbot para la información dinámica. Para ello, se utiliza como fuente de conocimiento la base de datos vectorizada construida en la etapa anterior.\n\n\nDado que el enfoque del proyecto se basa en herramientas de OpenAI, los modelos considerados para esta fase son los siguientes:\n\nGPT-4o-mini:\nUna versión ligera de GPT-4o, diseñada para ofrecer un buen balance entre costo, velocidad de respuesta y calidad en tareas de lenguaje natural. Es ideal para pruebas rápidas o implementaciones donde se requiere eficiencia.\nGPT-4o:\nModelo multimodal de última generación de OpenAI, capaz de procesar texto, imágenes y audio. En este proyecto se utiliza solo su capacidad textual, destacando por su mayor comprensión semántica y coherencia en las respuestas.\nModelos open-source integrados mediante Ollama:\nOllama permite correr modelos de lenguaje open-source de manera local o privada. En este proyecto se contemplan modelos como LLaMA o Mistral, que ofrecen alternativas de código abierto con buen rendimiento en tareas conversacionales.\n\n\n\n\nLa implementación del sistema se basa en una estructura modular orientada a clases. Esto permite una mayor reutilización de código, facilita su mantenimiento y mejora la legibilidad, aspectos clave para futuras modificaciones o revisiones.\nAdemás, esta estructura permite importar únicamente la clase necesaria para ejecutar todo el sistema, lo cual es ideal para su integración a través de una API. De esta forma, se evita depender de notebooks o archivos extensos y poco escalables.\nEl sistema se construyó utilizando principalmente la librería LangChain, la cual ofrece una base robusta para conectar modelos de lenguaje con herramientas externas y flujos personalizados.\n\n\nA partir de los datos mencionados en el apartado anterior, procederemos a crear la base de datos vectorial con esta información. Tomamos los datos limpios y transformados directamente de las funciones de limpieza (clean_products y clean_sales en ct/ETL/transform.py) y los convertimos en un tipo Document para poder pasarlo a FAISS (base vectorial) junto con los embeddings y guardarlo de forma local.\ncampos = [\n        \"nombre\",\n        \"producto\",\n        \"categoria\",\n        \"marca\",\n        \"tipo\",\n        \"modelo\",\n        \"detalles\",\n        \"fichaTecnica\",\n        \"resumen\"\n        ]\n\ndocs = [\n    Document(\n        page_content=construir_contenido(producto, campos), # recibe la información de cada producto y las columnas \n        metadata={\"collection\": 'promociones'} # o 'productos' dependiendo el caso\n    )\n    for producto in productos\n]\n\n# Usar embeddings de OpenAI \nembeddings = OpenAIEmbeddings(api_key=api_key)\n\n# Crear base de datos FAISS con los documentos\nvectorstore = FAISS.from_documents(docs, embeddings)\n\n# Guardar la base de datos para futuras consultas\nvectorstore.save_local()\n\n\n\nAdemás de la información estática precargada en la base vectorial, el sistema cuenta con herramientas dinámicas que permiten consultar datos actualizados en tiempo real, como existencias, precios y promociones. Estas herramientas se integran al agente a través del framework LangChain, lo que permite invocarlas solo cuando el modelo detecta que son necesarias para responder con precisión.\nLas herramientas dinámicas disponibles son:\n\nsearch_information_tool: realiza búsquedas básicas en los productos embebidos para encontrar coincidencias.\ninventory_tool: consulta las existencias, precio actual y moneda para un producto específico.\nsales_rules_tool: calcula promociones y reglas de venta que aplican a un producto según su lista de precios y sucursal.\n\n        self.tools = [\n            Tool(\n                name='search_information_tool',\n                func=search_information_tool,\n                description=\"Busca productos relacionados con lo que se pide.\"\n            ),\n            StructuredTool.from_function(\n                func=inventory_tool,\n                name='inventory_tool',\n                description=\"Esta herramienta sirve como referencia y devuelve precios, moneda y existencias de un producto por su clave y listaPrecio.\",\n                args_schema=ExistenciasInput # Explicitly link the Pydantic schema\n            ),\n            StructuredTool.from_function(\n            func=sales_rules_tool,\n            name='sales_rules_tool',\n            description=\"Aplica reglas de promoción, devuelve el precio final y mensaje para mostrar al usuario.\",\n            args_schema=SalesInput\n        )\n]\nEstas herramientas son invocadas automáticamente por el agente cuando la consulta del usuario requiere información que no está contenida en el contexto estático. Esto permite entregar respuestas más precisas y alineadas con la situación real del negocio (existencias, promociones activas, etc.).\n\n\n\nEl sistema se alimenta con información a través de un proceso ETL (Extracción, Transformación y Carga) que asegura que los datos estén limpios, estructurados y listos para ser utilizados por el modelo y las herramientas.\nPara una descripción detallada de cada etapa del proceso ETL, incluyendo la extracción de datos, la transformación (y el almacenamiento de fichas técnicas en MongoDB), y la carga directa a la base de datos vectorial, por favor, consultar el documento “Preparación de los datos”.\nEn resumen, este proceso garantiza que el modelo tenga acceso a una base de datos de conocimiento robusta y actualizada, tanto estática (productos y promociones embedidas) como dinámica (a través de las herramientas que consultan datos en tiempo real).\n\n\n\n\nDurante la ejecución del sistema, los modelos de lenguaje no operan en aislamiento. Se alimentan con diversos atributos y herramientas que enriquecen la interacción y permiten generar respuestas precisas y contextualizadas. A continuación, se describen los principales elementos que intervienen en este proceso y cómo la información preparada se integra en el modelo:\nAtributos del modelo en tiempo de ejecución\n\nquery: Pregunta o instrucción directa del usuario. Es el punto de entrada para iniciar el procesamiento.\nsession_id: Identificador de sesión que permite obtener el contexto del usuario (incluye la sucursal asociada para aplicar reglas de negocio como promociones).\nlistaPrecio: Parámetro numérico que indica la lista de precios relevante para consultas de productos y promociones.\n\nEstos atributos permiten personalizar las respuestas con base en el usuario que consulta, su sucursal, y las reglas comerciales que le aplican.\nAlimentación del modelo con información adicional\nEl LLM se alimenta con información contextualizada de dos maneras principales, ambas derivadas de los datos procesados en la fase de Preparación de los Datos:\n\nInformación estática (a través del RAG):\n\nProviene de la base de datos vectorial (FAISS) construida con los productos y promociones previamente embedidos mediante OpenAIEmbeddings.\nCuando el usuario realiza una query relevante, el sistema RAG busca los documentos más similares en el vector store. Estos documentos (page_content y metadata) se inyectan en el context window del LLM como información de referencia.\nEsto se activa principalmente para consultas generales de productos, descripciones, características, comparativas, etc., permitiendo al LLM generar respuestas basadas en un conocimiento específico y actualizado de tu catálogo.\n\nInformación dinámica (a través de herramientas LangChain):\n\nSe accede a datos en tiempo real mediante herramientas personalizadas integradas con LangChain, como inventory_tool y sales_rules_tool.\nEl LLM, basado en la query del usuario y su propio razonamiento, decide cuándo invocar estas herramientas. Por ejemplo, si el usuario pregunta por “el precio de la clave X”, el LLM activará inventory_tool con la clave proporcionada.\nEl resultado de la ejecución de estas herramientas (e.g., el precio actual, las existencias, el precio final con promoción) se devuelve al LLM y se inyecta también en su context window.\nEsto permite al LLM generar respuestas con datos actualizados y específicos, como la disponibilidad de un producto o el precio final con promociones activas para una listaPrecio y session_id dados.\n\n\nModeración y clasificación de la consulta\nAntes de ejecutar cualquier acción, la consulta pasa por una etapa de moderación:\n\nSe valida si el usuario está baneado (por comportamiento inapropiado).\nSe clasifica la consulta como relevante, irrelevante o inapropiada.\nDependiendo de esta clasificación, se permite o bloquea el paso al modelo principal y las herramientas.\n\nEste flujo asegura robustez, control y trazabilidad en la interacción con el modelo.\n\n\n\nA diferencia de los modelos clásicos de machine learning (ML), la evaluación de sistemas basados en modelos de lenguaje grande (LLMs) requiere enfoques distintos, centrados en la calidad de las respuestas generadas.\nEn este proyecto, la evaluación se realiza mediante un análisis cualitativo de las respuestas del chatbot, tomando en cuenta los siguientes criterios:\n\nLa información sobre productos, descripciones y características debe estar alineada con los datos disponibles en la base vectorial.\nLas respuestas deben ser claras, concisas y coherentes, evitando alucinaciones o información incorrecta.\nLos precios deben coincidir con los establecidos en la base de datos, y en el caso de promociones, estas deben estar correctamente aplicadas, evitando errores que impliquen pérdidas económicas.\n\nEstos criterios serán evaluados por los expertos y personas con conocimiento en la empresa."
  },
  {
    "objectID": "modelado.html#evaluación",
    "href": "modelado.html#evaluación",
    "title": "Modelado y Evaluación",
    "section": "2 Evaluación",
    "text": "2 Evaluación\nCon base en las respuestas generadas durante la etapa de modelado, se llevó a cabo una evaluación cualitativa para analizar la coherencia, relevancia y precisión de las recomendaciones de cada modelo. Este análisis nos permitió identificar oportunidades de mejora en el sistema, así como validar si el comportamiento del modelo es adecuado para continuar con su implementación o si requiere ajustes adicionales.\nA continuación, se presentan las respuestas generadas por el sistema para una serie de consultas simuladas por un usuario. Estas imágenes muestran el resultado del mejor modelo seleccionado (GPT 4o) ante cada solicitud:\n\nConsulta: “¡Hola! Me interesan computadoras de oficina”\n\nConsulta: “También me gustaría ver monitores de 27 pulgadas arriba de 75Hz”\n\nConsulta: “Y un no break gamer”\n\nConsulta: “Y una extensión doméstica”\n\n\nLos resultados obtenidos reflejan un desempeño sólido por parte del sistema. En todos los casos evaluados, las respuestas del chatbot fueron coherentes, alineadas con la base de datos y cumplieron con los criterios definidos:\n\nLas ofertas y promociones fueron correctamente identificadas y presentadas.\nLos precios y descripciones de los productos coincidieron con los datos reales.\nNo se observaron errores de alucinación ni pérdidas de coherencia en la conversación.\n\nEsto sugiere que el modelo es capaz de generar respuestas confiables y útiles para los usuarios, por lo que se considera viable continuar con las siguientes etapas del proyecto o bien escalar el sistema hacia una versión de prueba."
  },
  {
    "objectID": "comprension_datos.html",
    "href": "comprension_datos.html",
    "title": "Comprensión de los datos",
    "section": "",
    "text": "La fuente principal de información es la base de datos SQL local de la empresa, específicamente las tablas que contienen los datos relevantes de los productos, promociones y sus características más importantes. Estos datos se extraen mediante una conexión de Python con SQL. Además, complementamos esta información con datos obtenidos de un servicio local, el cual proporciona las fichas técnicas de los productos en formato XML.\n\n\n\nPara la exploración hemos usado un subconjunto de los datos. La información extraída de SQL incluye las siguientes columnas:\n\n\n['nombre',\n 'clave',\n 'categoria',\n 'categoria',\n 'marca',\n 'tipo',\n 'modelo',\n 'descripcion',\n 'descripcion_corta',\n 'palabrasClave']\n\n\nEstas columnas fueron seleccionadas en la consulta enviada al servidor SQL, priorizando aquellas más relevantes para el proyecto y ricas en información textual.\nEn cuanto a la información de las fichas técnicas, una vez llamado el servicio con la lista de claves de los productos, los datos obtenidos tienen el siguiente formato:\n{\n    \"ACCCDM1010\": {\n        \"respuesta\": {\n            \"tag\": \"CT-Respuesta\",\n            \"status\": \"success\",\n            \"mensaje\": \"Consulta realizada\",\n            \"data\": {\n                \"Product\": {\n                    \"@attributes\": {}}}}}}\nEn este ejemplo, se muestra la ficha técnica de un solo producto. De este archivo, los atributos de interés que utilizaremos son: Clave, Feature, Presentation_Value y SummaryDescription, los cuales se encuentran en el atributo de Product.\n\n\n\nPara completar el sistema de ofertas de productos, añadiremos aquellos productos que estén en promoción. De este modo, los clientes podrán encontrar y aprovechar fácilmente las ofertas que les interesen."
  },
  {
    "objectID": "comprension_datos.html#recolección-de-los-datos",
    "href": "comprension_datos.html#recolección-de-los-datos",
    "title": "Comprensión de los datos",
    "section": "",
    "text": "La fuente principal de información es la base de datos SQL local de la empresa, específicamente las tablas que contienen los datos relevantes de los productos, promociones y sus características más importantes. Estos datos se extraen mediante una conexión de Python con SQL. Además, complementamos esta información con datos obtenidos de un servicio local, el cual proporciona las fichas técnicas de los productos en formato XML.\n\n\n\nPara la exploración hemos usado un subconjunto de los datos. La información extraída de SQL incluye las siguientes columnas:\n\n\n['nombre',\n 'clave',\n 'categoria',\n 'categoria',\n 'marca',\n 'tipo',\n 'modelo',\n 'descripcion',\n 'descripcion_corta',\n 'palabrasClave']\n\n\nEstas columnas fueron seleccionadas en la consulta enviada al servidor SQL, priorizando aquellas más relevantes para el proyecto y ricas en información textual.\nEn cuanto a la información de las fichas técnicas, una vez llamado el servicio con la lista de claves de los productos, los datos obtenidos tienen el siguiente formato:\n{\n    \"ACCCDM1010\": {\n        \"respuesta\": {\n            \"tag\": \"CT-Respuesta\",\n            \"status\": \"success\",\n            \"mensaje\": \"Consulta realizada\",\n            \"data\": {\n                \"Product\": {\n                    \"@attributes\": {}}}}}}\nEn este ejemplo, se muestra la ficha técnica de un solo producto. De este archivo, los atributos de interés que utilizaremos son: Clave, Feature, Presentation_Value y SummaryDescription, los cuales se encuentran en el atributo de Product.\n\n\n\nPara completar el sistema de ofertas de productos, añadiremos aquellos productos que estén en promoción. De este modo, los clientes podrán encontrar y aprovechar fácilmente las ofertas que les interesen."
  },
  {
    "objectID": "comprension_datos.html#descripción-de-los-datos",
    "href": "comprension_datos.html#descripción-de-los-datos",
    "title": "Comprensión de los datos",
    "section": "0.2 Descripción de los datos",
    "text": "0.2 Descripción de los datos\nAl combinar la información extraída de SQL con las fichas técnicas en formato XML, obtenemos las siguientes variables:\n\n\n\n\n\n\n\n\nVariable\nDescripción\nTipo de dato\n\n\n\n\nNombre\nNombre del producto tal como aparece en la página web o catálogo.\nTexto\n\n\nClave\nCódigo único que distingue al producto de otros en el sistema.\nTexto\n\n\nCategoría\nClasificación o tipo de producto al que pertenece.\nTexto\n\n\nMarca\nNombre de la empresa que fabrica o distribuye el producto.\nTexto\n\n\nTipo\nEspecificación del tipo de producto (por ejemplo, cable, bateria, etc.).\nTexto\n\n\nModelo\nIdentificación del modelo específico del producto.\nTexto\n\n\nDetalles\nDescripción completa y detallada del producto.\nTexto\n\n\nFicha técnica\nInformación técnica detallada sobre el producto.\nTexto\n\n\nResumen\nResumen general del producto y sus características principales.\nTexto"
  },
  {
    "objectID": "comprension_datos.html#exploración-de-los-datos",
    "href": "comprension_datos.html#exploración-de-los-datos",
    "title": "Comprensión de los datos",
    "section": "0.3 Exploración de los datos",
    "text": "0.3 Exploración de los datos\n\n0.3.1 Variedad de productos\nPara comenzar, examinaremos la distribución de categorías dentro del catálogo, identificando cuáles son las más representativas entre un total de 247 categorías distintas.\nEl siguiente análisis nos permitirá visualizar las 10 categorías más frecuentes, lo que nos dará una mejor comprensión de la composición del inventario.\n\n\n\n\n\n\n\n\n\nVemos que los tóners destacan como la categoría predominante, seguidos por las aplicaciones de seguridad, aunque en este último caso, la diferencia con el resto de las categorías es menos marcada en comparación con la primera.\nDel mismo modo, exploraremos la distribución de marcas en los productos y visualizaremos las 10 más comunes dentro de un total de 195 marcas registradas.\n\n\n\n\n\n\n\n\n\nEn este análisis, observamos que la marca BROBOTIX sobresale como la más frecuente en el catálogo. Le sigue MANHATTAN, con una diferencia más reducida respecto a las siguientes marcas, en un patrón similar al que se observó en las categorías.\n\n\n0.3.2 Distribución de las palabras asociadas a los productos\nEn esta sección, analizaremos la cantidad de palabras utilizadas en diferentes descripciones de los productos. Esto nos permitirá entender cómo se estructuran los nombres, descripciones y palabras clave dentro del catálogo.\nPara ello, compararemos la distribución de palabras en los siguientes atributos:\n\nNombre del producto\nDescripción completa\nDescripción corta\nPalabras clave asociadas Este análisis nos ayudará a identificar patrones en la longitud de las descripciones y su posible impacto en la categorización y búsqueda de los productos.\n\n\n\n\n\n\n\n\n\n\nAl analizar las distribuciones, observamos que muchas de las instancias de la descripción corta comienzan con 0, pero luego la distribución se aproxima a una distribución quasi-normal, con un promedio de 5 palabras por instancia. En el caso de las descripciones, aunque no todas las instancias comienzan con 0, la distribución de palabras muestra un promedio de 1 palabra. Finalmente, las palabras en los nombres siguen una distribución aparentemente normal, con un promedio de 8 palabras por instancia."
  },
  {
    "objectID": "comprension_datos.html#verificación-de-la-calidad-de-los-datos",
    "href": "comprension_datos.html#verificación-de-la-calidad-de-los-datos",
    "title": "Comprensión de los datos",
    "section": "0.4 Verificación de la calidad de los datos",
    "text": "0.4 Verificación de la calidad de los datos\n\n0.4.1 Datos faltantes\nObservando el comportamiento de las distribuciones de la gráfica pasada, observamos que debe haber presencia de varios datos nulos, además de una distribución poco común para la variable de descripción ya que el promedio indica que es 1, algo que no se esperaría en una variable de este estilo.\nSi observamos la información de los datos:\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 6526 entries, 0 to 6525\nData columns (total 11 columns):\n #   Column             Non-Null Count  Dtype \n---  ------             --------------  ----- \n 0   idProductos        6526 non-null   int64 \n 1   nombre             6526 non-null   object\n 2   clave              6526 non-null   object\n 3   categoria          6526 non-null   object\n 4   marca              6526 non-null   object\n 5   tipo               6526 non-null   object\n 6   modelo             6526 non-null   object\n 7   descripcion        6526 non-null   object\n 8   descripcion_corta  763 non-null    object\n 9   palabrasClave      6268 non-null   object\n 10  detalles_precio    6526 non-null   object\ndtypes: int64(1), object(10)\nmemory usage: 561.0+ KB\n\n\nAquí vemos que la razón por la cual la distribución de descripción corta empezaba en 0, era porque alrededor del 89% son datos nulos. Sin embargo vemos que la descripción no tiene datos nulos, pero aún así sigue siendo curioso que la cantidad de palabras en promedio sea 1. Para esto analizaremos esta columna, contando sus valores únicos.\n\n\ndescripcion\n0                                                                                                                   4983\n                                                                                                                    1460\nTipo: Limpiador& Función: Para computadoras& Características: Limpieza profunda y protección antiestática              1\nColor: Negro& Compatible: L200                                                                                         1\nTipo: Vertical sencillo&#38; Compatible: Para rack de 42U&#38; Ducto: 4x4 pulgadas&#38; Color: Negro texturizado       1\nName: count, dtype: int64\n\n\nSi observamos en los datos, la gran mayoría de los datos tienen escrito el valor 0 (en tipo string). Y el segundo valor más frecuente son un espacio en blanco."
  },
  {
    "objectID": "home.html",
    "href": "home.html",
    "title": "Chatbot para sugerencias de productos: un enfoque personalizado",
    "section": "",
    "text": "Este proyecto se enfoca en el desarrollo de un chatbot para la recomendación de productos, basado en un sistema de recuperación de información desde una base de datos vectorial. A través de este chatbot, se busca optimizar la experiencia del cliente, sugiriendo productos relevantes según las especificaciones y presupuesto proporcionados.\nEl proyecto se divide en seis fases:\n\nComprensión del Negocio: Definición de los objetivos y el contexto del proyecto, enfocados en la mejora de la experiencia del cliente y la eficiencia del proceso de recomendación de productos.\nComprensión de los Datos: Recolección y análisis preliminar de los datos disponibles, tales como características de los productos y preferencias de los usuarios.\nPreparación de los Datos: Limpieza, transformación y estructuración de los datos para que puedan ser utilizados en el sistema de recomendación y la base de datos vectorial.\nDesarrollo del Sistema de Recomendación: Implementación del chatbot que utilizará un modelo de lenguaje grande (LLM) para interpretar las consultas y hacer recomendaciones basadas en la base de datos vectorial.\nEvaluación: Validación de la precisión y relevancia de las recomendaciones ofrecidas por el chatbot, utilizando métricas de evaluación del sistema.\nImplementación: Integración del chatbot en la plataforma de la empresa, presentación de los resultados y recomendaciones para mejorar el sistema de recomendación y la interacción con los clientes.\n\nA través de estas fases, se busca proporcionar una solución innovadora que mejore la interacción con los clientes, optimizando la selección de productos y mejorando la eficiencia del proceso de recomendación."
  },
  {
    "objectID": "home.html#introducción",
    "href": "home.html#introducción",
    "title": "Chatbot para sugerencias de productos: un enfoque personalizado",
    "section": "",
    "text": "Este proyecto se enfoca en el desarrollo de un chatbot para la recomendación de productos, basado en un sistema de recuperación de información desde una base de datos vectorial. A través de este chatbot, se busca optimizar la experiencia del cliente, sugiriendo productos relevantes según las especificaciones y presupuesto proporcionados.\nEl proyecto se divide en seis fases:\n\nComprensión del Negocio: Definición de los objetivos y el contexto del proyecto, enfocados en la mejora de la experiencia del cliente y la eficiencia del proceso de recomendación de productos.\nComprensión de los Datos: Recolección y análisis preliminar de los datos disponibles, tales como características de los productos y preferencias de los usuarios.\nPreparación de los Datos: Limpieza, transformación y estructuración de los datos para que puedan ser utilizados en el sistema de recomendación y la base de datos vectorial.\nDesarrollo del Sistema de Recomendación: Implementación del chatbot que utilizará un modelo de lenguaje grande (LLM) para interpretar las consultas y hacer recomendaciones basadas en la base de datos vectorial.\nEvaluación: Validación de la precisión y relevancia de las recomendaciones ofrecidas por el chatbot, utilizando métricas de evaluación del sistema.\nImplementación: Integración del chatbot en la plataforma de la empresa, presentación de los resultados y recomendaciones para mejorar el sistema de recomendación y la interacción con los clientes.\n\nA través de estas fases, se busca proporcionar una solución innovadora que mejore la interacción con los clientes, optimizando la selección de productos y mejorando la eficiencia del proceso de recomendación."
  },
  {
    "objectID": "comprension.html",
    "href": "comprension.html",
    "title": "Comprensión del negocio",
    "section": "",
    "text": "CT Internacional es una empresa mexicana encargada en distribuir soluciones de Tecnologías de la Información (TI), preferida para hacer negocios de los distribuidores e integradores del mundo de la tecnología. Fundada en 1992 en la ciudad de Hermosillo, esta empresa surgió como una respuesta a la oportunidad de llevar soluciones de TI, principalmente en el noroeste del país (Saúl Rojo). La cual poco a poco se fue expandiendo hasta lograr el alcance que tiene hoy en día, convirtiéndose en una empresa mayorista de alto impacto en el canal de distribución. Actualmente es una de las mejores empresas mexicanas y tiene presencia con 52 sucursales en todos los estados del país. Además cuenta con un canal de distribución integrado por más de 31 mil clientes y aliados de negocio a quiénes proporciona un extenso catálogo de productos y servicios de más de 202 marcas agrupadas en 12 unidades de negocio.\nTeniendo más de 25 años en el mercado, el crecimiento constante del negocio y del mundo de la tecnología, además de la gran cantidad de productos que ofrecen para su distribución; la empresa tiene la oportunidad de aprovechar los avances tecnológicos que se han ido generado hoy en día a su favor. Esto con la intención de ofrecer un mejor servicio a sus clientes y darles un mejor acercamiento de sus productos para que sean capaces de ver un catálogo especial o personalizado en sus necesidades específicas.\nObjetivo del proyecto:\nOptimizar el proceso de recomendación de productos dentro de la empresa mediante el uso de inteligencia artificial, mejorando la precisión y eficiencia en la búsqueda de opciones alineadas con las necesidades de los clientes.\nImpacto:\nLa empresa se estaría adaptando a la nueva era tecnológica, consolidando su liderazgo en el mercado al ofrecer productos que antes podían pasar desapercibidos para el cliente. Con este sistema, los consumidores pueden tomar decisiones ajustadas a sus necesidades, generando valor tanto para ellos como para la empresa."
  },
  {
    "objectID": "comprension.html#objetivos-de-la-línea-de-investigación",
    "href": "comprension.html#objetivos-de-la-línea-de-investigación",
    "title": "Comprensión del negocio",
    "section": "0.1 Objetivos de la línea de investigación",
    "text": "0.1 Objetivos de la línea de investigación\nEsta línea de investigación se enfoca en aprovechar la información disponible sobre los productos de la empresa, incluyendo características técnicas, precios y disponibilidad, con el objetivo de optimizar su accesibilidad y uso en procesos comerciales.\nEl primer paso es extraer, analizar y evaluar la calidad de los datos almacenados en la base de datos de la empresa para determinar si son suficientes para el desarrollo del sistema o si es necesario complementarlos con información adicional.\nUna vez validada la información, se procederá a su transformación mediante modelos de embeddings, convirtiéndola en representaciones vectoriales para su almacenamiento en una base de datos vectorial.\nEl objetivo final es desarrollar un sistema que permita a los usuarios consultar productos con base en especificaciones detalladas mediante un chatbot, el cual recuperará información relevante desde la base de datos vectorial, mejorando la precisión y relevancia de las recomendaciones.\nActualmente:\nLos productos se ofrecen a través de la plataforma, donde los clientes pueden realizar búsquedas según sus necesidades. Sin embargo, el sistema de búsqueda presenta limitaciones: si los clientes no utilizan ciertas palabras clave que están relacionadas al producto, encontrar lo que buscan puede volverse complicado. Además, la empresa cuenta con un sistema de asistencia por llamada o correo, donde un operador ayuda a los clientes a resolver dudas o encontrar productos específicos.\nObjetivo principal:\nDesarrollar un chatbot inteligente para la recomendación de productos, conectado a una base de datos vectorial para ofrecer respuestas precisas y relevantes a los usuarios, y conectado a SQL para consultar información dinámica. Con la intención de optimizar el proceso de búsqueda de productos que la empresa ofrece.\nObjetivos específicos:\n\nAnalizar y organizar la información disponible sobre un conjunto de los productos para evaluar su calidad y definir los atributos clave que se utilizarán en el sistema de recomendación. Además, identificar si es necesario crear nuevas características o ajustar las existentes para mejorar la precisión de las recomendaciones.\nDesarrollar un modelo de representación vectorial que convierta la información de los productos en representaciones vectoriales para su almacenamiento y recuperación eficiente.\nImplementar un sistema RAG que integre un modelo de lenguaje grande (LLM) con la base de datos vectorial para generar recomendaciones de productos precisas y relevantes.\nValidar el desempeño del sistema mediante pruebas de precisión, relevancia de las recomendaciones y eficiencia en la recuperación de información.\n\nCriterios de éxito:\n\nPrecisión en la recomendación de productos:\n\nAl menos el 90% de las 100 recomendaciones evaluadas deben coincidir con las necesidades descritas por el usuario, dando una respuesta directa, precisa y concisa en la información que proporcionada. La calidad de estas recomendaciones será validada por expertos con conocimiento en los productos.\n\nReproducibilidad y consistencia:\n\nConsultas similares deben producir respuestas coherentes en al menos el 90% de los 100 casos, evitando variaciones innecesaria, además de sugerencia de productos no disponibles o fuera de stock, ni productos que no existan. Este criterio también será evaluado por los mismos expertos quiénes determinarán la calidad de la respuesta.\n\nEscalabilidad:\n\nLa estructura del sistema mantiene su rendimiento al aumentar la cantidad de productos de la base de datos.\nLa calidad de las respuestas y el rendimiento del chatbot se mantiene constante al aumentar la cantidad de usuarios."
  },
  {
    "objectID": "comprension.html#evaluación-de-la-situación-actual",
    "href": "comprension.html#evaluación-de-la-situación-actual",
    "title": "Comprensión del negocio",
    "section": "0.2 Evaluación de la situación actual",
    "text": "0.2 Evaluación de la situación actual\nLos recursos que se tienen actualmente para este proyecto son:\n\nDatos: Registro de todos los productos de la empresa disponibles para distribución. La información incluye nombre, marca, tipo, modelo, descripción, palabras clave, stock disponible, ficha técnica con especificaciones, características principales, fecha del registro y precio. Toda esta información se tiene en una base de datos SQL, la cual se actualiza periódicamente.\nHerramientas: Python, servicios de OpenAI y bases de datos SQL.\nEquipo humano: Especialistas en el sistema de distribución de la empresa, expertos en la estructura y manejo de la base de datos para facilitar la extracción de datos, y científicos de datos encargados del desarrollo y evaluación del sistema junto con los especialistas.\n\n\n0.2.1 Requisitos, supuestos y restricciones\nRequisitos:\n\nAcceso a los registros de los productos de la empresa y equipo de compúto con características específicas para el desarrollo del proyecto.\nComunicación constante con los expertos de la empresa para constante evaluación y retroalimentación.\nCredenciales para el uso y acceso de herramientas de la empresa.\n\nSupuestos:\n\nLos permisos y el acceso a la base de datos se matendrá a la disposición del equipo siempre que se necesite sin problemas.\nAPI KEY con créditos suficientes disponibles, para evitar problemas con las llamadas de los modelos de OpenAI.\n\nRestricciones:\n\nLimitaciones en el uso del enlace de la conexión de la base de datos ya que se puede saturar.\nConexiones del sistema con la red de la empresa presentan problemas de sseguridad y permisos que se deben verificar y aprobar por las personas encargadas de infraestructura."
  },
  {
    "objectID": "comprension.html#terminologia",
    "href": "comprension.html#terminologia",
    "title": "Comprensión del negocio",
    "section": "0.3 Terminología",
    "text": "0.3 Terminología\nAlgunas de las terminologías clave para este proyecto son:\n\nPython: Lenguaje de programación de alto nivel, ampliamente utilizado en el desarrollo de software, análisis de datos, inteligencia artificial y automatización.\n\nFastAPI: Framework de desarrollo web en Python que permite construir APIs de forma rápida, sencilla y eficiente.\n\nLangChain: Herramienta para construir aplicaciones que combinan modelos de lenguaje con fuentes de datos externas y lógica personalizada.\n\nInteligencia artificial (IA): Campo de la informática que desarrolla sistemas capaces de realizar tareas que requieren inteligencia humana, como el aprendizaje, la toma de decisiones y el procesamiento del lenguaje natural.\n\nModelos de lenguaje grande (LLM): Modelos de inteligencia artificial entrenados con grandes volúmenes de texto para comprender y generar lenguaje natural.\n\nSistema de recuperación mejorada (RAG): Técnica que combina modelos de lenguaje con bases de datos externas para recuperar información relevante y generar respuestas más precisas.\n\nRepresentación vectorial: Proceso de convertir datos textuales en representaciones numéricas (vectores) para facilitar su análisis y búsqueda.\n\nModelo de embeddings: Algoritmo que transforma palabras o frases en vectores de manera que su similitud semántica pueda medirse matemáticamente.\n\nBase de datos vectorial: Sistema de almacenamiento optimizado para buscar y recuperar información midiendo la similitud entre vectores.\n\nBase de datos SQL (Structured Query Language): Sistema de almacenamiento relacional que organiza los datos en tablas con filas y columnas, usando SQL para consultarlos.\n\nAPI (Interfaz de Programación de Aplicaciones): Conjunto de reglas que permite que diferentes sistemas de software se comuniquen entre sí.\n\nAPI KEY: Clave de autenticación utilizada para acceder a servicios protegidos por una API.\n\nEndpoint (API): Dirección específica dentro de una API donde se accede a una funcionalidad concreta.\n\nPayload (HTTP): Contenido de los datos enviados en una solicitud HTTP, como un formulario o un JSON con información.\n\nChatbot: Programa que interactúa con los usuarios mediante lenguaje natural para responder preguntas o realizar tareas automatizadas.\n\nFrontend: Parte visual o interfaz con la que interactúa el usuario en una aplicación web.\n\nBackend: Parte lógica o del servidor donde se procesa la información y se ejecutan las funciones principales de una aplicación.\n\nHTTP (Hypertext Transfer Protocol): Protocolo para la transferencia de datos en la web.\n\nHTTPS (HTTP Secure): Versión segura del protocolo HTTP que cifra los datos para proteger la comunicación.\n\nContenido mixto (Mixed Content): Problema de seguridad que ocurre cuando una página HTTPS carga recursos desde una fuente HTTP, lo cual puede ser bloqueado por los navegadores.\n\nCertificado SSL/TLS: Archivo digital que autentica la identidad de un sitio web y cifra la información entre el servidor y el navegador.\n\nAutoridad Certificadora (CA): Entidad confiable que emite certificados digitales para garantizar la seguridad en la web.\n\nProxy: Servidor que actúa como intermediario entre un cliente y otro servidor, utilizado para redirigir solicitudes.\n\nPuerto (de red): Punto lógico de conexión en un servidor que permite recibir solicitudes de red.\n\nWidget: Componente visual incrustado en una página web, como un chatbot o formulario."
  },
  {
    "objectID": "comprension.html#beneficios",
    "href": "comprension.html#beneficios",
    "title": "Comprensión del negocio",
    "section": "0.4 Beneficios",
    "text": "0.4 Beneficios\n\nInnovación en la oferta de productos: Introducir un nuevo enfoque en la recomendación de productos, brindando una experiencia más personalizada y efectiva tanto para la empresa como para los clientes.\nOptimización de la estrategia del mercado: La implementación del sistema abre oportunidades para nuevas estrategias de negocio, mejorando la eficiencia y el alcance en la comercialización de productos."
  },
  {
    "objectID": "comprension.html#costos",
    "href": "comprension.html#costos",
    "title": "Comprensión del negocio",
    "section": "0.5 Costos",
    "text": "0.5 Costos\n\nTiempo: El proyecto tiene un plazo estimado de 3 meses para desarrollar una versión funcional y tangible que sirva como punto de partida para futuras mejoras e implementación en el flujo de trabajo.\nFinancieros: Se consideran costos asociados a suscripciones de herramientas de pago necesarias para el desarrollo e implementación del sistema."
  },
  {
    "objectID": "preparacion.html",
    "href": "preparacion.html",
    "title": "Preparación de los datos",
    "section": "",
    "text": "Una vez establecidas las conexiones con los servicios de datos, transformamos y consolidamos la información en una base de datos completa y estructurada. Esta base servirá como fuente robusta para el chatbot, permitiéndole responder consultas de manera eficiente y precisa. El objetivo de esta fase es asegurar que los datos estén limpios, consistentes y optimizados para la generación de embeddings y la recuperación de información.\nEl proceso de preparación de datos se divide en tres etapas fundamentales: Extracción, Transformación y Carga (ETL).\n\n\nLa etapa de extracción se enfoca en recolectar la información bruta de sus fuentes originales, asegurando que todos los datos necesarios para el chatbot sean accesibles.\n\n\nLos datos principales provienen de tres fuentes:\n\nBase de datos MySQL: Contiene los productos disponibles en la plataforma y los productos en promoción. De aquí se extraen atributos como nombre, clave, categoria, marca, tipo, modelo, descripcion, descripcion_corta, y palabrasClave.\nServicio local de fichas técnicas (XML): Proporciona información detallada y semi-estructurada de las fichas técnicas de los productos en formato XML.\n\nTodos los productos se relacionan a través de la claves idProducto y Clave del producto.\n\n\n\n\nExtracción de MySQL: La conexión se realiza mediante mysql.connector-python. Se ejecutan consultas SQL específicas (ids_query, product_query, current_sales_query en ct/ETL/extraction.py) para obtener los IDs de productos válidos, los detalles de los productos y las promociones vigentes.\nExtracción de fichas técnicas: Se interactúa con el servicio de fichas técnicas a través de solicitudes HTTP POST utilizando la librería cloudscraper. Esta librería es crucial para manejar posibles protecciones como Cloudflare, que podrían bloquear las solicitudes directas. Los headers incluyen tokens de API y cookies para autenticación.\nFrecuencia: Este proceso de extracción está diseñado para ejecutarse periódicamente (e.g., diariamente o semanalmente) para asegurar que la base de conocimientos del chatbot esté siempre actualizada con la información más reciente de productos y promociones.\n\n\n\n\nSe implementan mecanismos de manejo de errores para garantizar la robustez del proceso:\n\nReintentos con Backoff exponencial: Para las llamadas al servicio de fichas técnicas, se utilizan reintentos con un backoff exponencial controlado (max_retries, sleep_seconds en get_specifications_cloudscraper de ct/ETL/extraction.py). Esto ayuda a superar problemas temporales de red o sobrecarga del servicio.\nManejo de errores HTTP: Se capturan errores HTTP específicos, como el 403 Forbidden, que puede indicar un bloqueo de IP. En estos casos, se registra el error y se maneja la situación para evitar interrupciones completas del proceso.\nRegistro de fallos: Las claves de productos para las cuales no se pudo obtener la ficha técnica se registran, permitiendo una revisión manual o una re-extracción posterior. En caso de fallo persistente, se añadirá una ficha técnica vacía para mantener la integridad de la estructura de datos.\n\n\n\n\n\nLa etapa de transformación se encarga de limpiar, unificar y normalizar los datos extraídos, preparándolos para su uso en el sistema de recomendación.\n\n\nLas fichas técnicas, originalmente en formato XML, son procesadas para extraer los atributos más relevantes. Del formato original, se extraen los datos de @attributes, Feature, Presentation_Value y SummaryDescription, transformando la estructura a un formato más optimizado y fácil de consumir:\nFormato original de la ficha técnica (ejemplo):\n{\n    \"ACCCDM1010\": {\n        \"respuesta\": {\n            \"tag\": \"CT-Respuesta\",\n            \"status\": \"success\",\n            \"mensaje\": \"Consulta realizada\",\n            \"data\": {\n                \"Product\": {\n                    \"@attributes\": {},\n                    \"ProductFeature\": [...],\n                    \"SummaryDescription\": {...}\n                }\n            }\n        }\n    }\n}\nFormato optimizado:\n{'ACCCDM1010': {\n    'fichaTecnica': {\n        'NombreCaracteristica1': 'Valor1',\n        'NombreCaracteristica2': 'Valor2'\n    },\n    'resumen': {\n        'ShortSummary': 'Resumen corto del producto.',\n        'LongSummary': 'Resumen largo y detallado del producto.'\n    }},\n   }\nEsta optimización permite un acceso directo a las características y resúmenes, y evita hacer llamadas al servicio de XML cada vez que se requiere la ficha técnica de un producto ya conocido. Las fichas técnicas transformadas se guardan en MongoDB para su reutilización.\n\n\n\nPara las columnas textuales en los datos de productos y promociones (descripcion, descripcion_corta, palabrasClave), se aplican los siguientes pasos de limpieza para asegurar la calidad del texto y evitar ruido en los embeddings:\n\nReemplazo de valores nulos: Los valores nulos (NaN) se reemplazan por un espacio vácio ('').\nSustitución de ‘0’ en descripciones: Los caracteres ‘0’ (que a menudo representan valores nulos o ausentes en la fuente original) en la columna descripcion se sustituyen por un espacio vacío ('').\nConversión a tipo string: Todas las columnas relevantes se convierten explícitamente a tipo string para asegurar consistencia en el manejo del texto.\nEliminación de espacios extra: Se eliminan los espacios en blanco al inicio y al final de las cadenas (.str.strip()).\n\nEsta limpieza es fundamental para asegurar la calidad del texto que será utilizado en los embeddings, evitando ruido y mejorando la relevancia de las búsquedas.\n\n\n\n\nConcatenación de detalles: Las columnas descripcion, descripcion_corta y palabrasClave se concatenan en una nueva columna llamada detalles. Esto se hace con el fin de crear un campo textual más completo y denso para la generación de embeddings, capturando la mayor cantidad de información descriptiva relevante para cada producto en un solo lugar.\nIntegración de fichas técnicas: Una vez transformadas, la fichaTecnica y el resumen se incorporan como campos anidados a los diccionarios de productos y promociones, utilizando la clave del producto como identificador común. Esto enriquece cada registro con información técnica detallada y resúmenes generados.\n\n\n\n\nDurante la transformación, se aplican filtros para incluir solo productos con existencias superiores a un umbral definido (actualmente, más de 3 unidades) y aquellos con precios válidos. Esto asegura que solo los productos comercializables y relevantes para el negocio sean procesados y cargados en el sistema.\n\n\n\n\nLa etapa de carga es donde los datos limpios y transformados se persisten en los destinos finales y se preparan para el consumo del chatbot.\n\n\nLos datos limpios y transformados se persisten en MongoDB, específicamente en una colección principal:\n\nspecifications: Respalda las fichas técnicas transformadas de los productos, evitando llamadas repetidas al servicio XML.\n\nLos datos de productos y promociones, una vez transformados, se utilizan directamente para la construcción de la base de datos vectorial sin una carga intermedia en colecciones dedicadas de MongoDB.\n\n\n\nLa carga en MongoDB se realiza mediante operaciones de upsert (insertar si no existe, actualizar si existe) utilizando UpdateOne dentro de operaciones bulk_write. Esta estrategia optimiza el rendimiento al enviar múltiples operaciones de escritura en un solo lote y asegura que las fichas técnicas existentes se actualicen de manera eficiente, mientras que las nuevas se insertan.\n\n\n\nLa información de productos y promociones se utilizan para construir y mantener la base de datos vectorial FAISS, que es el corazón del sistema RAG.\n\nConversión a documentos LangChain: Los datos limpios de productos y promociones, obtenidos directamente de la etapa de transformación (clean_products, clean_sales en ct/ETL/load.py), se convierten en objetos langchain.schema.Document. Estos documentos incluyen el contenido textual (page_content construido por build_content) y metadatos relevantes como la clave y la colección de origen (productos o promociones)\nGeneración de embeddings: Se utilizan OpenAIEmbeddings para transformar el contenido textual de cada documento en representaciones vectoriales numéricas de alta dimensionalidad.\nCreación y actualización de FAISS:\n\nPara productos (productos_vs): Se crea un índice FAISS inicial a partir de los documentos de productos. Este proceso se realiza en lotes (e.g., lotes de 150 documentos) para gestionar eficientemente el consumo de memoria y cumplir con los límites de tasa de la API de OpenAI. El índice resultante se guarda localmente en PRODUCTS_VECTOR_PATH.\nPara promociones (sales_products_vs): Las ofertas se añaden incrementalmente al vector store de productos ya existente. Se carga el índice de productos desde PRODUCTS_VECTOR_PATH y luego se utilizan add_documents para incorporar los documentos de ofertas al mismo índice. Esta estrategia evita la necesidad de re-vectorizar todo el catálogo de productos cada vez que se actualizan las ofertas, optimizando el tiempo y los recursos. El índice combinado (productos + ofertas) se guarda en SALES_PRODUCTS_VECTOR_PATH."
  },
  {
    "objectID": "preparacion.html#preparación-de-los-datos",
    "href": "preparacion.html#preparación-de-los-datos",
    "title": "Preparación de los datos",
    "section": "",
    "text": "Una vez establecidas las conexiones con los servicios de datos, transformamos y consolidamos la información en una base de datos completa y estructurada. Esta base servirá como fuente robusta para el chatbot, permitiéndole responder consultas de manera eficiente y precisa. El objetivo de esta fase es asegurar que los datos estén limpios, consistentes y optimizados para la generación de embeddings y la recuperación de información.\nEl proceso de preparación de datos se divide en tres etapas fundamentales: Extracción, Transformación y Carga (ETL).\n\n\nLa etapa de extracción se enfoca en recolectar la información bruta de sus fuentes originales, asegurando que todos los datos necesarios para el chatbot sean accesibles.\n\n\nLos datos principales provienen de tres fuentes:\n\nBase de datos MySQL: Contiene los productos disponibles en la plataforma y los productos en promoción. De aquí se extraen atributos como nombre, clave, categoria, marca, tipo, modelo, descripcion, descripcion_corta, y palabrasClave.\nServicio local de fichas técnicas (XML): Proporciona información detallada y semi-estructurada de las fichas técnicas de los productos en formato XML.\n\nTodos los productos se relacionan a través de la claves idProducto y Clave del producto.\n\n\n\n\nExtracción de MySQL: La conexión se realiza mediante mysql.connector-python. Se ejecutan consultas SQL específicas (ids_query, product_query, current_sales_query en ct/ETL/extraction.py) para obtener los IDs de productos válidos, los detalles de los productos y las promociones vigentes.\nExtracción de fichas técnicas: Se interactúa con el servicio de fichas técnicas a través de solicitudes HTTP POST utilizando la librería cloudscraper. Esta librería es crucial para manejar posibles protecciones como Cloudflare, que podrían bloquear las solicitudes directas. Los headers incluyen tokens de API y cookies para autenticación.\nFrecuencia: Este proceso de extracción está diseñado para ejecutarse periódicamente (e.g., diariamente o semanalmente) para asegurar que la base de conocimientos del chatbot esté siempre actualizada con la información más reciente de productos y promociones.\n\n\n\n\nSe implementan mecanismos de manejo de errores para garantizar la robustez del proceso:\n\nReintentos con Backoff exponencial: Para las llamadas al servicio de fichas técnicas, se utilizan reintentos con un backoff exponencial controlado (max_retries, sleep_seconds en get_specifications_cloudscraper de ct/ETL/extraction.py). Esto ayuda a superar problemas temporales de red o sobrecarga del servicio.\nManejo de errores HTTP: Se capturan errores HTTP específicos, como el 403 Forbidden, que puede indicar un bloqueo de IP. En estos casos, se registra el error y se maneja la situación para evitar interrupciones completas del proceso.\nRegistro de fallos: Las claves de productos para las cuales no se pudo obtener la ficha técnica se registran, permitiendo una revisión manual o una re-extracción posterior. En caso de fallo persistente, se añadirá una ficha técnica vacía para mantener la integridad de la estructura de datos.\n\n\n\n\n\nLa etapa de transformación se encarga de limpiar, unificar y normalizar los datos extraídos, preparándolos para su uso en el sistema de recomendación.\n\n\nLas fichas técnicas, originalmente en formato XML, son procesadas para extraer los atributos más relevantes. Del formato original, se extraen los datos de @attributes, Feature, Presentation_Value y SummaryDescription, transformando la estructura a un formato más optimizado y fácil de consumir:\nFormato original de la ficha técnica (ejemplo):\n{\n    \"ACCCDM1010\": {\n        \"respuesta\": {\n            \"tag\": \"CT-Respuesta\",\n            \"status\": \"success\",\n            \"mensaje\": \"Consulta realizada\",\n            \"data\": {\n                \"Product\": {\n                    \"@attributes\": {},\n                    \"ProductFeature\": [...],\n                    \"SummaryDescription\": {...}\n                }\n            }\n        }\n    }\n}\nFormato optimizado:\n{'ACCCDM1010': {\n    'fichaTecnica': {\n        'NombreCaracteristica1': 'Valor1',\n        'NombreCaracteristica2': 'Valor2'\n    },\n    'resumen': {\n        'ShortSummary': 'Resumen corto del producto.',\n        'LongSummary': 'Resumen largo y detallado del producto.'\n    }},\n   }\nEsta optimización permite un acceso directo a las características y resúmenes, y evita hacer llamadas al servicio de XML cada vez que se requiere la ficha técnica de un producto ya conocido. Las fichas técnicas transformadas se guardan en MongoDB para su reutilización.\n\n\n\nPara las columnas textuales en los datos de productos y promociones (descripcion, descripcion_corta, palabrasClave), se aplican los siguientes pasos de limpieza para asegurar la calidad del texto y evitar ruido en los embeddings:\n\nReemplazo de valores nulos: Los valores nulos (NaN) se reemplazan por un espacio vácio ('').\nSustitución de ‘0’ en descripciones: Los caracteres ‘0’ (que a menudo representan valores nulos o ausentes en la fuente original) en la columna descripcion se sustituyen por un espacio vacío ('').\nConversión a tipo string: Todas las columnas relevantes se convierten explícitamente a tipo string para asegurar consistencia en el manejo del texto.\nEliminación de espacios extra: Se eliminan los espacios en blanco al inicio y al final de las cadenas (.str.strip()).\n\nEsta limpieza es fundamental para asegurar la calidad del texto que será utilizado en los embeddings, evitando ruido y mejorando la relevancia de las búsquedas.\n\n\n\n\nConcatenación de detalles: Las columnas descripcion, descripcion_corta y palabrasClave se concatenan en una nueva columna llamada detalles. Esto se hace con el fin de crear un campo textual más completo y denso para la generación de embeddings, capturando la mayor cantidad de información descriptiva relevante para cada producto en un solo lugar.\nIntegración de fichas técnicas: Una vez transformadas, la fichaTecnica y el resumen se incorporan como campos anidados a los diccionarios de productos y promociones, utilizando la clave del producto como identificador común. Esto enriquece cada registro con información técnica detallada y resúmenes generados.\n\n\n\n\nDurante la transformación, se aplican filtros para incluir solo productos con existencias superiores a un umbral definido (actualmente, más de 3 unidades) y aquellos con precios válidos. Esto asegura que solo los productos comercializables y relevantes para el negocio sean procesados y cargados en el sistema.\n\n\n\n\nLa etapa de carga es donde los datos limpios y transformados se persisten en los destinos finales y se preparan para el consumo del chatbot.\n\n\nLos datos limpios y transformados se persisten en MongoDB, específicamente en una colección principal:\n\nspecifications: Respalda las fichas técnicas transformadas de los productos, evitando llamadas repetidas al servicio XML.\n\nLos datos de productos y promociones, una vez transformados, se utilizan directamente para la construcción de la base de datos vectorial sin una carga intermedia en colecciones dedicadas de MongoDB.\n\n\n\nLa carga en MongoDB se realiza mediante operaciones de upsert (insertar si no existe, actualizar si existe) utilizando UpdateOne dentro de operaciones bulk_write. Esta estrategia optimiza el rendimiento al enviar múltiples operaciones de escritura en un solo lote y asegura que las fichas técnicas existentes se actualicen de manera eficiente, mientras que las nuevas se insertan.\n\n\n\nLa información de productos y promociones se utilizan para construir y mantener la base de datos vectorial FAISS, que es el corazón del sistema RAG.\n\nConversión a documentos LangChain: Los datos limpios de productos y promociones, obtenidos directamente de la etapa de transformación (clean_products, clean_sales en ct/ETL/load.py), se convierten en objetos langchain.schema.Document. Estos documentos incluyen el contenido textual (page_content construido por build_content) y metadatos relevantes como la clave y la colección de origen (productos o promociones)\nGeneración de embeddings: Se utilizan OpenAIEmbeddings para transformar el contenido textual de cada documento en representaciones vectoriales numéricas de alta dimensionalidad.\nCreación y actualización de FAISS:\n\nPara productos (productos_vs): Se crea un índice FAISS inicial a partir de los documentos de productos. Este proceso se realiza en lotes (e.g., lotes de 150 documentos) para gestionar eficientemente el consumo de memoria y cumplir con los límites de tasa de la API de OpenAI. El índice resultante se guarda localmente en PRODUCTS_VECTOR_PATH.\nPara promociones (sales_products_vs): Las ofertas se añaden incrementalmente al vector store de productos ya existente. Se carga el índice de productos desde PRODUCTS_VECTOR_PATH y luego se utilizan add_documents para incorporar los documentos de ofertas al mismo índice. Esta estrategia evita la necesidad de re-vectorizar todo el catálogo de productos cada vez que se actualizan las ofertas, optimizando el tiempo y los recursos. El índice combinado (productos + ofertas) se guarda en SALES_PRODUCTS_VECTOR_PATH."
  },
  {
    "objectID": "preparacion.html#estructura-final-de-los-datos",
    "href": "preparacion.html#estructura-final-de-los-datos",
    "title": "Preparación de los datos",
    "section": "2. Estructura final de los datos",
    "text": "2. Estructura final de los datos\nUna vez completado el proceso ETL, la información de productos y promociones queda estructurada de la siguiente manera, lista para ser consumida por el chatbot y el sistema de recomendación:\nEn el caso de los productos, la información final queda estructurada de la siguiente manera:\n {\n        \"nombre\": \"Nombre del Producto\",\n        \"clave\": \"Clave\",\n        \"categoria\": \"Categoría del Producto\",\n        \"marca\": \"Marca del Producto\",\n        \"tipo\": \"Tipo de Producto\",\n        \"modelo\": \"Modelo del Producto\",\n        \"detalles\": \"Descripción completa, corta y palabras clave concatenadas.\",\n        \"fichaTecnica\": {\n            \"Caracteristica1\": \"Valor1\",\n            \"Caracteristica2\": \"Valor2\"\n        },\n        \"resumen\": {\n            \"ShortSummary\": \"Resumen corto.\",\n            \"LongSummary\": \"Resumen largo.\"\n        }\n    },\n    {...}\nReiteramos que el proceso de limpieza y enriquecimiento aplicado a las promociones fue similar al de los productos. A continuación, se presenta el resultado final de la estructura deseada para las promociones:\n{\n        \"nombre\": \"Nombre de la Promoción\",\n        \"producto\": \"Clave\", # Clave del producto en promoción\n        \"categoria\": \"Categoría del Producto\",\n        \"marca\": \"Marca del Producto\",\n        \"tipo\": \"Tipo de Producto\",\n        \"modelo\": \"Modelo del Producto\",\n        \"detalles\": \"Descripción completa, corta y palabras clave de la promoción.\",\n        \"fichaTecnica\": {\n            \"CaracteristicaA\": \"ValorA\",\n            \"CaracteristicaB\": \"ValorB\"\n        },\n        \"resumen\": {\n            \"ShortSummary\": \"Resumen corto de la promoción.\",\n            \"LongSummary\": \"Resumen largo de la promoción.\"\n        }\n    },\n    {...}"
  },
  {
    "objectID": "despliegue.html",
    "href": "despliegue.html",
    "title": "Despliegue",
    "section": "",
    "text": "El desarrollo del proyecto de Chatbot siguió un enfoque iterativo, basado en los principios del ciclo CRISP-DM. Esta metodología estructurada permitió abordar las distintas fases del proyecto de manera organizada, con un énfasis continuo en la mejora del diseño, la calidad del código y, fundamentalmente, el rendimiento del sistema en sus componentes clave.\nLos principales retos se concentraron en la fase de extracción, manipulación y estructuración de los datos, con el objetivo de mantenerlos lo más tidy posible y así garantizar una mayor precisión y coherencia en las respuestas generadas por el sistema. Además de las condiciones bajo las promociones y el dinamismo de los precios tanto para productos normales como ofertas.\nA lo largo de los ciclos de desarrollo, se han cumplido los hitos establecidos, manteniendo un ritmo de trabajo adecuado; aunque hubieron varios cambios, o ajustes, con respecto a la propuesta inicial mencionada en la comprensión del negocio, el objetivo sigue siendo el mismo:\nOptimizar el proceso de recomendación de productos dentro de la empresa mediante el uso de inteligencia artificial, mejorando la precisión y eficiencia en la búsqueda de opciones alineadas con las necesidades de los clientes.\nLa fase de evaluación, aunque continua, ha mostrado resultados prometedores que sugieren que el sistema, en su estado actual, posee la robustez necesaria para avanzar a una etapa de prueba en un entorno controlado y, aunque sea de pruebas, real.\n\n\nConsiderando los avances y los aprendizajes obtenidos, se evaluaron dos opciones principales para la continuación del proyecto:\n\nContinuar en fases de desarrollo/modelado: Dedicar más tiempo a la refinación interna de los datos, explorar técnicas avanzadas de preprocesamiento, o actualizar versiones de modelos y librerías principales.\nPasar a la fase de implementación en un entorno de prueba: Desplegar el sistema en un entorno controlado que simule las condiciones de uso real, permitiendo obtener feedback directo y validar el comportamiento del chatbot en interacción con usuarios y la infraestructura existente.\n\n\n\n\nSe ha decidido priorizar la implementación en la página de pruebas de la empresa. Esta decisión se fundamenta en la necesidad de validar el sistema en un entorno lo más cercano posible a producción, identificar rápidamente fallos en la integración, la experiencia del usuario, y orientar los ciclos de mejora futuros con base en datos de uso real. La implementación en pruebas servirá como una plataforma funcional sobre la cual se podrá continuar iterando y perfeccionando la solución de manera incremental."
  },
  {
    "objectID": "despliegue.html#revisión-del-proceso",
    "href": "despliegue.html#revisión-del-proceso",
    "title": "Despliegue",
    "section": "",
    "text": "El desarrollo del proyecto de Chatbot siguió un enfoque iterativo, basado en los principios del ciclo CRISP-DM. Esta metodología estructurada permitió abordar las distintas fases del proyecto de manera organizada, con un énfasis continuo en la mejora del diseño, la calidad del código y, fundamentalmente, el rendimiento del sistema en sus componentes clave.\nLos principales retos se concentraron en la fase de extracción, manipulación y estructuración de los datos, con el objetivo de mantenerlos lo más tidy posible y así garantizar una mayor precisión y coherencia en las respuestas generadas por el sistema. Además de las condiciones bajo las promociones y el dinamismo de los precios tanto para productos normales como ofertas.\nA lo largo de los ciclos de desarrollo, se han cumplido los hitos establecidos, manteniendo un ritmo de trabajo adecuado; aunque hubieron varios cambios, o ajustes, con respecto a la propuesta inicial mencionada en la comprensión del negocio, el objetivo sigue siendo el mismo:\nOptimizar el proceso de recomendación de productos dentro de la empresa mediante el uso de inteligencia artificial, mejorando la precisión y eficiencia en la búsqueda de opciones alineadas con las necesidades de los clientes.\nLa fase de evaluación, aunque continua, ha mostrado resultados prometedores que sugieren que el sistema, en su estado actual, posee la robustez necesaria para avanzar a una etapa de prueba en un entorno controlado y, aunque sea de pruebas, real.\n\n\nConsiderando los avances y los aprendizajes obtenidos, se evaluaron dos opciones principales para la continuación del proyecto:\n\nContinuar en fases de desarrollo/modelado: Dedicar más tiempo a la refinación interna de los datos, explorar técnicas avanzadas de preprocesamiento, o actualizar versiones de modelos y librerías principales.\nPasar a la fase de implementación en un entorno de prueba: Desplegar el sistema en un entorno controlado que simule las condiciones de uso real, permitiendo obtener feedback directo y validar el comportamiento del chatbot en interacción con usuarios y la infraestructura existente.\n\n\n\n\nSe ha decidido priorizar la implementación en la página de pruebas de la empresa. Esta decisión se fundamenta en la necesidad de validar el sistema en un entorno lo más cercano posible a producción, identificar rápidamente fallos en la integración, la experiencia del usuario, y orientar los ciclos de mejora futuros con base en datos de uso real. La implementación en pruebas servirá como una plataforma funcional sobre la cual se podrá continuar iterando y perfeccionando la solución de manera incremental."
  },
  {
    "objectID": "despliegue.html#plan-de-implementación",
    "href": "despliegue.html#plan-de-implementación",
    "title": "Despliegue",
    "section": "2 Plan de implementación",
    "text": "2 Plan de implementación\nLa fase de implementación implica el despliegue de los componentes desarrollados y su integración en el entorno web de pruebas de CT Online. El objetivo es habilitar el widget de chatbot para un grupo controlado de usuarios.\n\n2.1. Arquitectura de Despliegue y Conexión\nLa fase de implementación en el entorno de pruebas requiere el despliegue de la API del chatbot (desarrollada en Python con FastAPI) y la integración del widget (desarrollado en JavaScript, HTML y CSS) en la página web de pruebas de CT Online.\nUn desafío técnico crucial identificado durante la planificación del despliegue fue la política de seguridad de “contenido mixto” (mixed content) impuesta por los navegadores modernos. Dado que la página de pruebas de la empresa se sirve a través de HTTPS para garantizar una conexión segura, el navegador bloquea las peticiones que el código JavaScript del widget intenta realizar a recursos o servicios que no son seguros, como la API FastAPI que opera con el protocolo HTTP. Realizar llamadas directas desde HTTPS a HTTP resulta en un error de “mixed content”, impidiendo la comunicación.\nTambién se evaluó la posibilidad de servir la API directamente a través de HTTPS. Sin embargo, esta alternativa implicaba mayores retos técnicos y operativos, como la gestión de certificados SSL válidos que autentiquen la comunicación segura entre el servidor y los navegadores. Aunque se intentó utilizar un certificado autofirmado, los navegadores modernos no lo reconocen como confiable, lo que resultaba en el bloqueo automático de las conexiones. Además del reto técnico, esta opción implicaba complicaciones administrativas relacionadas con la obtención, configuración y renovación de certificados válidos, lo que aumentaba la complejidad del despliegue inicial.\nPara superar este obstáculo y permitir la comunicación segura entre el frontend en HTTPS y la API, se ha adoptado la siguiente arquitectura de despliegue utilizando un backend proxy:\n\nLa aplicación principal de la empresa, que opera en un entorno seguro con HTTPS, actuará como intermediaria. Dado que esta aplicación utiliza PHP, el proxy se implementará en este lenguaje.\nEl widget, integrado en la página de pruebas (servida en HTTPS), no realizará llamadas directas a la API FastAPI. En su lugar, el JavaScript del widget será configurado para enviar todas sus peticiones a un nuevo endpoint específico en el backend PHP.\nEl backend PHP recibirá estas peticiones entrantes del frontend (en HTTPS).\nEl código PHP, realizará entonces la solicitud real a la API. La comunicación entre el backend PHP y la API no está sujeta a las restricciones de contenido mixto del navegador, o sea, sin importa si la API se sirve en HTTP o HTTPS, el PHP siempre se puede comunicar con el servicio sin problema.\nEl backend PHP recibirá la respuesta de la API y la reenviará de vuelta al frontend del widget (en HTTPS), en otras palabras, se apunta así mismo.\n\nEste enfoque de proxy en el backend PHP resuelve el problema del contenido mixto al asegurar que la comunicación entre el navegador (frontend) y la infraestructura del backend de la empresa siempre se realice a través de HTTPS. El desafío de integrar una fuente HTTP en un entorno HTTPS queda encapsulado en la comunicación de servidor a servidor.\n\n\n2.2 Gestión de Persistencia de Datos con MongoDB\nUna mejora significativa en la arquitectura del chatbot ha sido la migración de la gestión del historial de conversaciones desde archivos JSON locales a una base de datos NoSQL, específicamente MongoDB. Esta decisión se tomó para optimizar el almacenamiento, mejorar el rendimiento y facilitar el análisis de datos, especialmente considerando el volumen y la frecuencia de las interacciones de los usuarios.\nLa implementación en MongoDB se estructura en dos colecciones principales:\n\nsessions: Esta colección está diseñada para mantener los últimos n mensajes de cada sesión de usuario. Su objetivo es asegurar una recuperación de mensajes mínima y rápida, optimizando la experiencia del usuario final al evitar la carga de historiales extensos en cada interacción. Cada vez que se añade un nuevo mensaje, el más antiguo se desplaza si se supera el límite de mensajes configurado, manteniendo la colección ligera y eficiente para las operaciones del chatbot.\nmessage_backup: A diferencia de sessions, esta colección actúa como un histórico completo de todos los mensajes generados. Su propósito principal es el análisis de datos y la alimentación de sistemas de reportes automatizados. El esquema de esta colección está pensado para simular una tabla SQL, lo que facilita la búsqueda y recuperación de información para análisis posteriores. Cada documento en esta colección incluye tanto la consulta del usuario como la respuesta del chatbot, junto con metadatos relevantes como timestamps y detalles de tokens utilizados.\n\nEsta estrategia de persistencia de datos en MongoDB ha permitido superar las limitaciones de los archivos JSON, que no eran escalables para una gran cantidad de usuarios, ofreciendo un camino más robusto y óptimo para almacenar la información de las conversaciones.\n\n\n\nSchema MongoDB\n\n\n\n\n2.3 Desarrollo del widget frontend\nSe desarrolló un widget de chat personalizado e integrable mediante un simple script (sdk.js). Este widget se encarga de inyectar la interfaz de usuario (html) y cargar la lógica de la aplicación (app.js) y los estilos (styles.css) de forma dinámica en cualquier página web.\n\n\n2.4 Plan de monitoreo\nDurante la fase de pruebas, se implementará un plan de monitoreo para evaluar el rendimiento y comportamiento del sistema. Las métricas clave a seguir incluirán:\n\nTiempo de respuesta de la API: Latencia entre el envío de una consulta y la recepción de la primera parte o la respuesta completa.\nTasa de éxito/Error de las peticiones a la API: Proporción de peticiones que resultan en códigos de estado.\nCalidad de las respuestas: Evaluación manual o semi-automatizada de la coherencia, relevancia y precisión de las respuestas del chatbot, especialmente en casos donde no se encuentran recomendaciones.\nFrecuencia de uso del widget: Número de aperturas del chat y cantidad de interacciones por usuario.\nErrores en la consola del navegador: Monitoreo de errores de JavaScript o CSS reportados por los usuarios durante el uso del widget.\n\n\n\n2.5 Plan de mantenimiento\nSe establecerá un plan de mantenimiento periódico para asegurar la estabilidad y el buen funcionamiento del sistema desplegado:\n\nActualización de dependencias: Programar revisiones y actualizaciones de las librerías y paquetes utilizados en la API (Python, Langchain, FastAPI, etc.) y potencialmente en el frontend si se usan librerías externas.\nRevisión de logs: Monitorear activamente los logs del servidor donde corre la API y de los servicios web para identificar y solucionar errores.\nAuditoría de calidad de datos y respuestas: Realizar evaluaciones regulares de la calidad de los datos de origen y verificar la calidad de las respuestas generadas por el modelo con el tiempo.\nRefactorización y optimización: A medida que se identifiquen áreas de mejora o cambien los requisitos, planificar tareas de refactorización de código para mejorar la modularidad, el rendimiento o la mantenibilidad.\n\n\n\n2.6 Experiencia de desarrollo\nEl proyecto ha permitido consolidar la experiencia en el ciclo completo de desarrollo de una aplicación basada en modelos de lenguaje, desde la comprensión y preparación de datos complejos, pasando por el prototipado con herramientas como Langchain, hasta el desarrollo de una API robusta con FastAPI y la implementación de una interfaz de usuario dinámica y reusable (widget frontend desarrollado en JS, HTML y CSS). La resolución de desafíos específicos como el manejo de diferentes estructuras de datos para las vector stores y la integración segura de una API a un entorno web real (HTTPS/contenido mixto, CORS, permisos de red) han sido aprendizajes clave con complicaciones y problemas que se pudieron corregir y solucionar. Se han seguido buenas prácticas de desarrollo, enfocándose en la modularidad para facilitar futuras expansiones (ej: integración de LangGraph) y el mantenimiento del código.\n\n\n2.7 Despliegue del chatbot en el sistema de desarrollo\nEl chatbot fue desplegado exitosamente en el entorno de pruebas de CT Online, habilitado específicamente para fines de desarrollo e integración continua. Este entorno permite validar en condiciones casi reales el comportamiento tanto del frontend (widget) como de la API conversacional.\nEl proceso de despliegue consistió en los siguientes pasos:\n\nMontaje del entorno de la API: La API desarrollada con FastAPI se desplegó en un servidor, o ambiente virtual de linux, utilizando Gunicorn como servidor de aplicaciones y conectándose al backend de la página de CT Online.\nIntegración del widget en la página de pruebas: Se inyectó el script del widget directamente en la página, asegurando que se pudieran cargar dinámicamente los recursos necesarios (JS, HTML y CSS) desde un servidor de archivos estáticos. La integración se validó en distintos navegadores modernos para asegurar la compatibilidad y el correcto funcionamiento.\nGestión de versiones y control de cambios: Se utilizó Git para gestionar versiones del código tanto del sistema del Chatbot, la API y del widget. Esto permitió llevar un registro detallado de los cambios realizados y facilitó el proceso de despliegue incremental, en caso de futuras modificaciones o ajustes.\nVerificación funcional: Tras el despliegue inicial, se realizaron pruebas manuales y automatizadas para verificar el correcto funcionamiento del flujo de conversación, el tiempo de respuesta de la API y el comportamiento del widget en diferentes escenarios (errores, entradas no reconocidas, ausencia de resultados, etc.).\nConsideraciones de seguridad: Aunque se trata de un entorno de pruebas, se aseguraron medidas básicas como la validación de origen en CORS, limitación de rutas expuestas en la API, y uso de HTTPS para todas las comunicaciones entre cliente y servidor.\n\nLa imagen a continuación muestra el chatbot funcionando en su entorno de desarrollo, con el widget incrustado en la página de pruebas de CT Online:\n\n\n\nChatbot desplegado en el ambiente de desarrollo\n\n\nEste hito marca un avance significativo hacia la validación en entorno real del sistema conversacional, permitiendo recopilar feedback de usuarios internos antes de considerar un despliegue completo en producción.\n\n\n2.8 Despliegue del chatbot en el sistema de producción\nA partir del despliegue en el ambiente de pruebas, donde recopilamos retroalimentación e hicimos iteraciones sobre el modelo, finalmente pudimos desplegarlo al ambiente de producción. Este despliegue es escalonado, empezando por la sucursal de Hermosillo y con los vendedores de la empresa, recopilando nuevamente retroalimentación pero más pegada a casos de estudio concretos y reales.\n\n\n\nChatbot desplegado en el ambiente de producción"
  }
]