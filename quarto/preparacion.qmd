---
title: "Preparación de los datos"
format: 
    html:
         page-layout: article
toc-title: "Tabla de Contenidos"
toc: true
toc-depth: 5
---

::: {style="text-align: justify"}
## 1. Preparación de los datos  

Una vez establecidas las conexiones con los servicios de datos, transformamos y consolidamos la información en una base de datos completa y estructurada. Esta base servirá como fuente robusta para el chatbot, permitiéndole responder consultas de manera eficiente y precisa. El objetivo de esta fase es asegurar que los datos estén limpios, consistentes y optimizados para la generación de embeddings y la recuperación de información.

El proceso de preparación de datos se divide en tres etapas fundamentales: Extracción, Transformación y Carga (ETL).

### 1.1. Extracción de los datos

La etapa de extracción se enfoca en recolectar la información bruta de sus fuentes originales, asegurando que todos los datos necesarios para el chatbot sean accesibles.

#### 1.1.1. Fuentes de datos

Los datos principales provienen de tres fuentes:

1. **Base de datos MySQL**: Contiene los productos disponibles en la plataforma y los productos en promoción. De aquí se extraen atributos como `nombre`, `clave`, `categoria`, `marca`, `tipo`, `modelo`, `descripcion`, `descripcion_corta`, y `palabrasClave`.
2. **Servicio local de fichas técnicas (XML)**: Proporciona información detallada y semi-estructurada de las fichas técnicas de los productos en formato XML.

Todos los productos se relacionan a través de la claves `idProducto` y `Clave del producto`.

#### 1.1.2. Métodos y frecuencia de extracción

* **Extracción de MySQL**: La conexión se realiza mediante `mysql.connector-python`. Se ejecutan consultas SQL específicas (`ids_query`, `product_query`, `current_sales_query` en `ct/ETL/extraction.py`) para obtener los IDs de productos válidos, los detalles de los productos y las promociones vigentes.
* **Extracción de fichas técnicas**: Se interactúa con el servicio de fichas técnicas a través de solicitudes HTTP POST utilizando la librería `cloudscraper`. Esta librería es crucial para manejar posibles protecciones como Cloudflare, que podrían bloquear las solicitudes directas. Los headers incluyen tokens de API y cookies para autenticación.
* **Frecuencia**: Este proceso de extracción está diseñado para ejecutarse periódicamente (e.g., diariamente o semanalmente) para asegurar que la base de conocimientos del chatbot esté siempre actualizada con la información más reciente de productos y promociones.

#### 1.1.3. Manejo de error en la extracción

Se implementan mecanismos de manejo de errores para garantizar la robustez del proceso:

* **Reintentos con Backoff exponencial**: Para las llamadas al servicio de fichas técnicas, se utilizan reintentos con un _backoff_ exponencial controlado (`max_retries`, `sleep_seconds` en `get_specifications_cloudscraper` de `ct/ETL/extraction.py`). Esto ayuda a superar problemas temporales de red o sobrecarga del servicio.
* **Manejo de errores HTTP**: Se capturan errores HTTP específicos, como el `403 Forbidden`, que puede indicar un bloqueo de IP. En estos casos, se registra el error y se maneja la situación para evitar interrupciones completas del proceso.
* **Registro de fallos**: Las claves de productos para las cuales no se pudo obtener la ficha técnica se registran, permitiendo una revisión manual o una re-extracción posterior. En caso de fallo persistente, se añadirá una ficha técnica vacía para mantener la integridad de la estructura de datos.

### 1.2. Transformación de los datos

La etapa de transformación se encarga de limpiar, unificar y normalizar los datos extraídos, preparándolos para su uso en el sistema de recomendación.

#### 1.2.1. Estructuración de las fichas técnicas

Las fichas técnicas, originalmente en formato XML, son procesadas para extraer los atributos más relevantes. Del formato original, se extraen los datos de `@attributes`, `Feature`, `Presentation_Value` y `SummaryDescription`, transformando la estructura a un formato más optimizado y fácil de consumir:

**Formato original de la ficha técnica (ejemplo):**  

```python
{
    "ACCCDM1010": {
        "respuesta": {
            "tag": "CT-Respuesta",
            "status": "success",
            "mensaje": "Consulta realizada",
            "data": {
                "Product": {
                    "@attributes": {},
                    "ProductFeature": [...],
                    "SummaryDescription": {...}
                }
            }
        }
    }
}
```

**Formato optimizado:**

```python
{'ACCCDM1010': {
    'fichaTecnica': {
        'NombreCaracteristica1': 'Valor1',
        'NombreCaracteristica2': 'Valor2'
    },
    'resumen': {
        'ShortSummary': 'Resumen corto del producto.',
        'LongSummary': 'Resumen largo y detallado del producto.'
    }},
   }
```

Esta optimización permite un acceso directo a las características y resúmenes, y evita hacer llamadas al servicio de XML cada vez que se requiere la ficha técnica de un producto ya conocido. Las fichas técnicas transformadas se guardan en MongoDB para su reutilización.

#### 1.2.2. Limpieza y normalización

Para las columnas textuales en los datos de `productos` y `promociones` (`descripcion`, `descripcion_corta`, `palabrasClave`), se aplican los siguientes pasos de limpieza para asegurar la calidad del texto y evitar ruido en los embeddings:

1. **Reemplazo de valores nulos**: Los valores nulos (`NaN`) se reemplazan por un espacio vácio (`''`).
2. **Sustitución de '0' en descripciones**: Los caracteres '0' (que a menudo representan valores nulos o ausentes en la fuente original) en la columna `descripcion` se sustituyen por un espacio vacío (`''`).
3. **Conversión a tipo `string`**: Todas las columnas relevantes se convierten explícitamente a tipo `string` para asegurar consistencia en el manejo del texto.
4. **Eliminación de espacios extra**: Se eliminan los espacios en blanco al inicio y al final de las cadenas (`.str.strip()`).

Esta limpieza es fundamental para asegurar la calidad del texto que será utilizado en los embeddings, evitando ruido y mejorando la relevancia de las búsquedas.

#### 1.2.3. Enriquecimiento y consolidación

* **Concatenación de `detalles`**: Las columnas `descripcion`, `descripcion_corta` y `palabrasClave` se concatenan en una nueva columna llamada `detalles`. Esto se hace con el fin de crear un campo textual más completo y denso para la generación de embeddings, capturando la mayor cantidad de información descriptiva relevante para cada producto en un solo lugar. 
* **Integración de fichas técnicas**: Una vez transformadas, la `fichaTecnica` y el `resumen` se incorporan como campos anidados a los diccionarios de productos y promociones, utilizando la `clave` del producto como identificador común. Esto enriquece cada registro con información técnica detallada y resúmenes generados.

#### 1.2.4. Lógica de negocio aplicada

Durante la transformación, se aplican filtros para incluir solo productos con existencias superiores a un umbral definido (actualmente, más de 3 unidades) y aquellos con precios válidos. Esto asegura que solo los productos comercializables y relevantes para el negocio sean procesados y cargados en el sistema.

### 1.3. Carga de los datos

La etapa de carga es donde los datos limpios y transformados se persisten en los destinos finales y se preparan para el consumo del chatbot.

#### 1.3.1 Destinos de carga

Los datos limpios y transformados se cargan en **MongoDB**, distribuidos en tres colecciones principales:

* `products`: Almacena la información principal de los productos.
* `sales`: Contiene los detalles de las promociones.
* `specifications`: Respalda las fichas técnicas transformadas de los productos, evitando llamadas repetidas al servicio XML.

#### 1.3.2. Estrategia de carga en MongoDB

La carga en MongoDB se realiza mediante operaciones de `upsert` (insertar si no existe, actualizar si existe) utilizando `UpdateOne` dentro de operaciones `bulk_write` (`mongo_products`, `mongo_sales` en `ct/ETL/load.py`). Esta estrategia optimiza el rendimiento al enviar múltiples operaciones de escritura en un solo lote y asegura que los datos existentes se actualicen de manera eficiente, mientras que los nuevos se insertan.

#### 1.3.3. Construcción y actualización del vector store

La información de productos y promociones se utilizan para construir y mantener la base de datos vectorial FAISS, que es el corazón del sistema RAG.

* **Conversión a documentos LangChain**: Los datos cargados en MongoDB se recuperan y se convierten en objetos `langchain.schema.Document` (`load_producs`, `load_sales` en `ct/ETL/load.py`). Estos documentos incluyen el contenido textual (`page_content` construido por `build_content`) y metadatos relevantes como la `clave` y la colección de origen (`productos` o `promociones`).
* **Generación de embeddings**: Se utilizan `OpenAIEmbeddings` para transformar el contenido textual de cada documento en representaciones vectoriales numéricas de alta dimensionalidad.
* **Creación y actualización de FAISS**:
    * **Para productos (`productos_vs`)**: Se crea un índice FAISS inicial a partir de los documentos de productos. Este proceso se realiza en lotes (e.g., lotes de 150 documentos) para gestionar eficientemente el consumo de memoria y cumplir con los límites de tasa de la API de OpenAI. El índice resultante se guarda localmente en `PRODUCTS_VECTOR_PATH`.
    * **Para promociones (`sales_products_vs`)**: Las ofertas se añaden incrementalmente al vector store de productos ya existente. Se carga el índice de productos desde `PRODUCTS_VECTOR_PATH` y luego se utilizan `add_documents` para incorporar los documentos de ofertas al mismo índice. Esta estrategia evita la necesidad de re-vectorizar todo el catálogo de productos cada vez que se actualizan las ofertas, optimizando el tiempo y los recursos. El índice combinado (productos + ofertas) se guarda en `SALES_PRODUCTS_VECTOR_PATH`.
:::

::: {style="text-align: justify"}
## 2. Estructura final de los datos

Una vez completado el proceso ETL, la información de productos y promociones queda estructurada de la siguiente manera, lista para ser consumida por el chatbot y el sistema de recomendación:

En el caso de los `productos`, la información final queda estructurada de la siguiente manera:

```python
 {
        "nombre": "Nombre del Producto",
        "clave": "Clave",
        "categoria": "Categoría del Producto",
        "marca": "Marca del Producto",
        "tipo": "Tipo de Producto",
        "modelo": "Modelo del Producto",
        "detalles": "Descripción completa, corta y palabras clave concatenadas.",
        "fichaTecnica": {
            "Caracteristica1": "Valor1",
            "Caracteristica2": "Valor2"
        },
        "resumen": {
            "ShortSummary": "Resumen corto.",
            "LongSummary": "Resumen largo."
        }
    },
    {...}
```

Reiteramos que el proceso de limpieza y enriquecimiento aplicado a las `promociones` fue similar al de los `productos`. A continuación, se presenta el resultado final de la estructura deseada para las promociones:

```python
{
        "nombre": "Nombre de la Promoción",
        "producto": "Clave", # Clave del producto en promoción
        "categoria": "Categoría del Producto",
        "marca": "Marca del Producto",
        "tipo": "Tipo de Producto",
        "modelo": "Modelo del Producto",
        "detalles": "Descripción completa, corta y palabras clave de la promoción.",
        "fichaTecnica": {
            "CaracteristicaA": "ValorA",
            "CaracteristicaB": "ValorB"
        },
        "resumen": {
            "ShortSummary": "Resumen corto de la promoción.",
            "LongSummary": "Resumen largo de la promoción."
        }
    },
    {...}
```
:::