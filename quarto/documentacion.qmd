---
title: "Documentación"
format: 
  html:
    page-layout: article
toc-title: "Tabla de Contenidos"
toc: true
toc-depth: 3
---

::: {style="text-align: justify"}
## 1 Manual de instalación y despliegue.

### 1.1 Configuraciones importantes

* El proyecto está diseñado para ser desplegado en entornos **Linux o Windows** con **Python 3.12.9**. Requiere acceso a **Ollama** (para la ejecución de modelos *open-source* como `gemma3:12b`), así como conectividad a una instancia de **MongoDB** y bases de datos **MySQL**.
* La aplicación *backend* se expone a través de **FastAPI** en el puerto `8000`. Es crucial asegurar que este puerto esté abierto y accesible en el entorno de despliegue.
* Archivo `.env` con variables cargadas
* Todas las credenciales y configuraciones sensibles se gestionan mediante un archivo de variables de entorno (`.env`), garantizando la seguridad y facilidad de configuración.

### 1.2 Requisitios del sistema

* **Python**: Versión 3.12.9
* **Pip**: Última versión
* **UV**: Última versión (gestor de paquetes y entornos)
* **Ollama**: Instalado y en ejecución en el servidor para el *hosting* de modelos *open-source*.
* **MongoDB**: Acceso remoto configurado para las colecciones de historial, productos, ofertas y fichas técnicas.
* **MySQL**: Acceso remoto configurado para la extracción de datos de productos y precios. 

### 1.3 Dependencias principales del sistema

* `langchain`: Framework principal para la construcción de cadenas RAG y la orquestación del flujo del chatbot.
* `tiktoken`: Utilizado para el conteo preciso de tokens en las consutlas y respuestas, fundamental para la estimación de costos.
* `ollama`: Herramienta para servir modelos de lenguaje *open-source* localmente, como `gemma3:12b`, permitiendo flexibilidad en la elección del LLM.
* `pymongo`: Driver Python para la interacción con MongoDB, utilizado para el almacenamiento y recuperación de sesiones de usuario, historial de mensajes, fichas técnicas, y datos de productos/ofertas.
* `mysql-connector-python`: Conector para MySQL, empleado para la extracción de datos de producto, sus detalles y precios desde la base de datos relacional.
* `faiss-cpu`: Biblioteca para la búsqueda eficiente de similitudes, crucial para la creación y consulta de la base de datos vectorial donde se almacenan los embeddings de productos.
* `gunicorn`: Servidor WSGI utilizado para desplegar la aplicación FastAPI en producción, gestionando la concurrencia y el rendimiento.
* **Otras dependencias**: Todas las demás librerías requeridas se detallan en el archivo `pyproject.toml`. La instalación de este archivo se detalla más adelante.

### 1.4 Instalación del backend (API)

1. Clonar el repositorio:
```bash
git clone https://github.com/anmerino-pnd/proyectoCT
cd proyectoCT
```

2. Crear un entorno virtual e instalar dependencias:

   Se recomienda usar `uv` por su eficiencia.
```bash
pip install uv # En caso de no estar instalado
uv venv
source .venv/bin/activate  # Para Linux/macOS
# o `.venv\Scripts\activate` para Windows
uv pip install -e .
```

3. Asegurarse de estar corriendo Ollama en el ambiente:

   Verifica que el servicio de Ollama esté instalado y activo, y que el modelo `gemma3:12b` esté disponible.
```bash
curl -fsSL https://ollama.com/install.sh | sh # Para instalar Ollama
ollama serve
ollama list # Para verificar que el modelo gemma3:12b esté descargado y listo
ollama pull gemma3:12b # Correr esta línea en caso que el modelo no aparezca
```
4. Configurar variables de entorno:

   Antes de levantar el *backend*, asegurarse de que el archivo `.env` en la raíz del proyecto contenga las siguientes variables con sus valores correctos. 
   
```python
# Conexión a la base de datos SQL
ip=
port=
user=
pwd=
db=

# Clave de la API de OpenAI para correr sus modelos
OPENAI_API_KEY=

# Configuración para el servicio de fichas técnicas
url= '' 
Token-api=''
Token-ct=''
Content-Type=''
Cookie=''

dominio=""
boundary=''

# Conexión a MongoDB
MONGO_URI = "mongodb://" # En la URI debe estar incrustrado el nombre de la DB
MONGO_COLLECTION_SESSIONS = ""
MONGO_COLLECTION_MESSAGE_BACKUP = ""
MONGO_COLLECTION_PRODUCTS = ""
MONGO_COLLECTION_SALES = ""         
MONGO_COLLECTION_SPECIFICATIONS = ""
```

5. Levantar el backend con Gunicorn:

  Este comando inicia la API, especificando el número de *workers*, el *binding* de IP y puerto, y la configuración de SSL/TLS para HTTPS.
```bash
nohup gunicorn ct.main:app   --workers 4   --bind 0.0.0.0:8000   --certfile=static/ssl/cert.pem   --keyfile=static/ssl/key.pem   -k uvicorn.workers.UvicornWorker   --access-logfile -   --error-logfile - &
```
  El uso de `nogup` y `&` asegura que el proceso continúe ejecutándose en segundo plano incluso si la sesión SSH se cierra.

6. Regenerar el certificado SSL (si expira o es necesario):

  Si el certificado SSL autofirmado ha expirado o necesitas uno nuevo:
```bash
openssl req -x509 -newkey rsa:2048 -nodes -keyout ssl/key.pem -out ssl/cert.pem -days 365
```
  Asegurarse de que los archivos `cert.pem` y `key.pem` estén en la ruta `ssl` dentro de tu proyecto.

### 1.5 Instalación del frontend (Widget)

1. **Cargar archivos del widget**: Los archivos del *frontend* (principalmente `sdk.js` y cualquier recurso gráfico como `chat.png`) deben ser cargados en el servidor donde reside el *frontend* de la página.

2. **Incrustar el widget en el HTML**: Ejemplo de cómo se puede añadir el widget en la página web donde se desea que aparezca el chatbot.

```html
<!DOCTYPE html>
<html lang="es">
<head>
  <meta charset="UTF-8" />
  <title>Prueba del Widget</title>
</head>
<body>
  <script 
    src="sdk.js" 
    data-user-id="test" 
    data-user-key="2" 
    data-api-base="https://ctdev.ctonline.mx/chatbot" 
    data-chat-icon-url="chat.png" 
    type="text/javascript">
  </script>
</body>
</html>
```
**Notas importantes para el `data-api-base`:**

* Si la API corre en HTTP y el *frontend* en HTTPS, se enfrentarán problemas de "contenido mixto". La solución propuesta fue usar un archivo PHP (*backend* del sitio web) como intermediario, el cual es crucial aquí. La API debe apuntar a este PHP y el PHP a su *frontend* donde se encuentra el widget.
* La `data-api-base` es el dominio donde es accesible la API mediante PHP.


### 1.6 Notas adicionales

* **Problemas de caché**: Es común que los navegadores almacenen versiones antiguas de archivos JS/CSS. Si la interfaz del *widget* no funciona correctamente después de una actualización, instruye a los usuarios a limpiar la caché de su navegador o a realizar un "hard refresh" (Ctrl+F5). Implementar una estrategia de *versioning* para los archivos del *widget* (ej.js?v=1.2.3) puede mitigar esto a futuro.
* **Rotación de IP para fichas técnicas**: El sistema está diseñado para manejar el bloqueo de IP del servicio de fichas técnicas. Se recomienda monitorear los logs de la extracción (`extraction.py`) para identificar errores 403, lo que indicaría la necesidad de actualizar la IP en el servicio externo.

:::

::: {style="text-align: justify"}
## 2 Documentación técnica del código

### 2.1 Estructura de carpetas y módulos

El proyecto sigue una estructura modular para facilitar la gestión y el mantenimiento. A continuación, se detalla el propósito de los módulos principales y algunas de sus funciones clave:

**ct/langchain/vectorstore.py**

* Este módulo implementa la lógica para la creación, carga y consulta de la base de datos vectorial FAISS. Gestiona el retriever que busca los documentos más relevantes.
* **Clases y funciones clave**:
  * `class LangchainVectorStore`:
    * `__init__(embedder, index_path: str = None)`: Inicializa el *vector store*.
      * **Parámetros**: 
        * `embedder`: Una instancia de la clase *embedder* utilizada para generar los *embeddings* (ej., `OpenAIEmbeddings(openai_api_key=openai_api_key)`).
        * `index_path` (`str`): Ruta al directorio donde se guardará o cargará en el índice FAISS.
      * **Comportarmiento**: Si `index_path` existe, carga el índice preexistente. Configura un `retriever` con `k=10` (top 10 documentos) y  `score_threshold=0.8` para la búsqueda.
    * `_load_index()`: Carga un índice FAISS existente desde disco.
      * **Parámetros**: Ninguno (usa `self.index_path`).
      * **Comportamiento**: Carga el índice y configura `self.retriever` búsquedas. Es una función interna.

**ct/langchain/assistant.py**

* Contiene la lógica central del chatbot, incluyendo la gestión del historial de sesiones (a través de MongoDB), la creación de las cadenas RAG con memoria conversacional y la interacción con el LLM de OpenAI.
* **Clases y funciones clave**:
  * `class LangchainAssistant`:
    * `__init__(retriever)`: Inicializa el asistente del chatbot.
      * **Parámetros**:
        * `retriever`: Una instancia de un **retriever** (ej., `vectorstore.retriever`) que proporciona documentos relevantes.
      * **Comportamiento**: Configura el LLM (OpenAI), las conexiones a MongoDB para `sessions` y `message_backup`, define el tamaño de la ventana de memoria (`memory_window_size`), y construye la cadena RAG principal (`rag_chain`).
    * `get_session_history(session_id: str) -> List[BaseMessage]`: Recupera el historial de chat de una sesión específica desde MongoDB.
      * **Parámetros**:
        * `session_id (str)`: El identificador único de la sesión del usuario.
      * **Retorna**: `List[BaseMessage]`: Una lista de objetos `HumanMessage` y `AIMessage` que representan la conversación previa.
    * `clear_session_history(session_id: str)`: Elimina el historial de mensajes de una sesión específica en MongoDB.
      * **Parámetros**:
        * `session_id (str)`: El identificador único de la sesión a borrar.
      * **Retorna**: `bool: True` si la operación fue exitosa, `False` en caso contrario.
    * `ensure_session(session: str)`: Asegura que exista una entrada para la `session_id` en la colección `sessions` de MongoDB, o la crea si no existe.
      * **Parámetros**:
        * `session_id (str)`: El identificador de la sesión.
      * **Comportamiento**: Actualiza la última actividad de la sesión o inserta una nueva sesión.
    * `build_chain()`: Construye la cadena RAG que incorpora el historial de chat.
      * **Parámetros**: Ninguno.
      * **Retorna**: Una instancia de `langchain.chains.create_retrieval_chain` configurada para manejar el historial y la recuperación de documentos.
    * `answer`:

     ```python
    answer(
      session_id: str, 
      question: str, 
      listaPrecio: str = None
      ) -> AsyncGenerator[str, None]: 
    ```

    Responde a una consulta del usuario, transmitiendo la respuesta en tiempo real.
      * **Parámetros**:
        * `session_id (str)`: ID de la sesión del usuario.
        * `question (str)`: La pregunta del usuario.
        * `listaPrecio (str)`: El nivel de lista de precios asociado al usuario, usado en el *prompt* del LLM.
      * **Retorna**: `AsyncGenerator[str, None]`: Un generador asíncrono que cede fragmentos (`chunks`) de la respuesta a medida que se generan.
      * **Comportamiento**: Recupera el historial, lo trunca si es muy largo, ejecuta la cadena RAG, recopila métricas de tokens/costo/duración y persiste el mensaje en `sessions` y `message_backup`.
    * `add_message`
    
     ```python
    add_message(
      session_id: str, 
      message_type: str, 
      content: str, 
      metadata: dict = None): 
    ```

    Agrega un mensaje a la colección `sessions` para mantener un historial de ventana.
      * **Parámetros**:
        * `session_id (str)`: ID de la sesión.
        * `message_type (str)`: *human* o *assistant*.
        * `content (str)`: Contenido del mensaje.
        * `metadata (dict)`: Metadatos adicionales (usado para mensajes del asistente).
      * **Comportamiento**: Inserta el mensaje y trunca la colección `last_messages` a un tamaño fijo (actualmente 50 mensajes) para optimizar el rendimiento.
    * `add_message_backup`
     
     ```python
    add_message_backup(
      session_id: str, 
      question: str, 
      full_answer: str, 
      metadata: dict = None): 
    ```

    Guarda un respaldo completo de cada interacción (pregunta y respuesta) junto con métricas detalladas en la colección `message_backup`.
      * **Parámetros**:
        * `session_id (str)`: ID de la sesión.
        * `message_type (str)`: *human* o *assistant*.
        * `content (str)`: Contenido del mensaje.
        * `metadata (dict)`: Metadatos adicionales (usado para mensajes del asistente).
      * **Comportamiento**: Inserta un nuevo documento en `message_backup` con toda la información relevante para análisis posterior.
      
**ct/langchain/rag.py**

* Orquesta el flujo completo de una consulta de usuario, incluyendo la clasificación de la intención y la delegación a los modelos adecuados.
* **Clases y funciones claves**:
  * `class LangchainRAG`:
    * `__init__(index_path: str = None)`: Inicializa el orquestador RAG.
      * **Parámetros**:
        * `index_path (str)`: Ruta al índice FAISS.
      * **Comportamiento**: Inicializa el *embedder*, el *vector store* (cargando el índice si existe), y el asistente de LangChain. Configura el modelo Ollama (`gemma3:1b`) para la clasificación.
    * `classify_query(query: str) -> str`: Clasifica la consulta del usuario.
      * **Parámetros**:
        * `query (str)`: La consulta del usuario.
      * **Retorna**: Un `str` con una de las clasificaciones *relevante*, *irrelevante*, o *inapropiado*.
      * **Comportamiento**: Utiliza un modelo Ollama (`gemma3:1b`) con un *system prompt* específico para realizar la clasificación.
    * `polite_answer(query: str) -> AsyncGenerator[str, None]`: Genera una respuesta cortés para consultas irrelevantes.
      * **Parámetros**:
        * `query (str)`: La consulta del usuario.
        * **Retorna**: `AsyncGenerator[str, None]`: Cede fragmentos de una respuesta amigable indicando que la consulta está fuera de tema.
        * **Comportamiento**: Usa el modelo Ollama (`gemma3:1b`) con un *system prompt* que lo instruye a ser cortés y redirigir al usuario a temas tecnológicos.
    * `ban_answer(query: str) -> AsyncGenerator[str, None]`: Genera una respuesta firme para consultas inapropiadas.
      * **Parámetros**:
        * `query (str)`: La consulta original del usuario.
      * **Retorna**: `AsyncGenerator[str, None]`: Cede fragmentos de una respuesta que prohíbe el lenguaje inapropiado y advierte de un posible bloqueo.
      * **Comportamiento**: Usa el modelo Ollama (`gemma3:1b`) con un *system prompt* que exige una respuesta directa y sin concensiones.
    * `run`
     
     ```python
    run(
      query: str, 
      session_id: str = None, 
      listaPrecio: str = None
      ) -> AsyncGenerator[str, None]: 
     ```

    Ejecuta el flujo completo de la consulta del usuario.
      * **Parámetros**:
        * `query (str)`: La consulta del usuario.
        * `session_id (str)`: ID de la sesión.
        * `listaPrecio (str)`: Nivel de lista de precios.
      * **Retorna**: `AsyncGenerator[str, None]`: Cede fragmentos de la respuesta final del chatbot.
      * **Comportamiento**: Primero clasifica la consulta. Dependiendo de la clasificación, llama a `assistant.answer` (para relevante), `polite_answer` (para irrelevante) o `ban_answer` (para inapropiado).

**ct/chat.py**

* Define los *endpoints* de la API para la interacción del chat y la gestión del historial de sesiones. 
* **Funciones clave**:
  * `get_chat_history(user_id: str)`: Recupera el historial de chat de un usuario específico.
    * **Parámetros**:
      * `user_id (str)`: El ID del usuario cuyo historial se desea recuperar.
    * **Retorna**: `List[Dict[str, str]]`: Una lista de diccionarios, donde cada diccionario representa un mensaje con *role* (*user* o *bot*) y *content*.
    * **Comportamiento**: Llama a `assistant.get_session_history()` y formatea la respuesta.
  * `async_chat_generator(request: QueryRequest) -> AsyncGenerator[str, None]`: Un generador asíncrono que envuelve la función `run` de la clase `LangchainRAG` para permitir el *streaming* de respuestas.
    * **Parámetros**:
      * `request (QueryRequest)`: Un objeto que contiene `user_query`, `user_id`, y `listaPrecio`.
    * **Retorna**: `AsyncGenerator[str, None]`: Cede los fragmentos de respuesta directamente desde el `rag.run(query: str, session_id: str, listaPrecio: str)`.
  * `async_chat_endpoint(request: QueryRequest)`: El *endpoint* HTTP POST para recibir nuevas consultas de chat y devolver respuestas en *streaming*.
    * **Parámetros**
      * `request (QueryRequest)`: Objeto de solicitud Pydantic con la consulta del usuario, ID de usuario y lista de precios.
    * **Retorna**: `StreamingResponse`: Una respuesta HTTP que permite al cliente recibir los fragmentos de la respuesta en tiempo real.
  * `delete_chat_history_endpoint(user_id: str)`: El *endpoint* HTTP DELETE para eliminar el historial de chat de un usuario.
    * **Parámetros**:
      * `user_id (str)`: El ID del usuario cuyo historial se va a eliminar.
    * **Retorna**: Un `str` que indica "success" si la eliminación fue exitosa.
    * **Comportamiento**: Llama a `assistant.clear_session_history()` y maneja excepciones.

**ct/main.py**

* Es el archivo principal de la aplicación FastAPI. Configura la aplicación, habilita CORS y registra los *endpoints* definidos en `ct.chat`.
* **Funciones clave**:
  * `app = FastAPI`: Inicializa la aplicación FastAPI.
  * `app.add_middleware(CORSMiddleware, ...)`: Configura el middleware de CORS para permitir solicitudes desde cualquier origen, métodos y cabeceras, lo cual es crucial para la integración del *widget* en diferentes dominios.
  * `@app.get("/history/{user_id}")`: Decorador que mapea la ruta GET `/history/{user_id}` a la función `handle_history`.
  * `handle_history(user_id: str)`: Llama a `get_chat_history` de `ct.chat` para obtener y retornar el historial.
  * `@app.post("/chat")`: Decorador que mapea la ruta POST `/chat` a la función `handle_chat`.
  * `handle_chat(request: QueryRequest)`: Llama a `async_chat_endpoint` de `ct.chat` para manejar la solicitud de chat y el *streaming* de la respuesta.
  * `@app.delete("/history/{user_id}")`: Decorador que mapea la ruta DELETE `/history/{user_id}` a la función `handle_delete_history`.
  * `handle_delete_history(user_id: str)`: Llama a `delete_chat_history_endpoint` de `ct.chat` para borrar 
  el historial de un usuario.
  * `if __name__ == "__main__":`: Bloque de ejecución principal para correr la aplicación con Uvicorn en desarrollo. En producción con Gunicorn.

**ct/ETL/extraction.py**:

* Módulo de la capa de Extracción. Responsable de conectarse a la base de datos MySQL para extraer información de productos y promociones, y de interactuar con el servicio externo para obtener fichas técnicas.
* **Clases y funciones clave**:
  * `class Extraction`:
    * `__init__()`: Inicializa la clase con los parámetros de conexión a MySQL y configura un `cloudscraper` para la extracción de fichas técnicas.
      * **Parámetros**: Ninguno explícito, lee de `ct.clients`.
      * **Comportamiento**: Establece `scraper` con *headers* personalizados para los tokens de la API y *cookies*.
    * `ids_query() -> str`: Retorna la consulta SQL para obtener IDs de productos válidos (con existencias y precios).
    * `get_valid_ids() -> list`: Ejecuta la consulta `ids_query` y retorna una lista de IDs de productos válidos.
      * **Retorna**: `list`: Lista de IDs de productos.
      * **Comportamiento**: Se conecta a MySQL, ejecuta la consulta y maneja errores de conexión.
    * `product_query(id) -> str`: Retorna la consulta SQL para obtener detalles de un producto específico por ID. Incluye detalles de precios por lista, categoría, marca, etc.
    * `get_products() -> pd.DataFrame`: Extrae la información de todos los productos válidos desde MySQL.
      * **Retorna**: `pd.DataFrame`: Un DataFrame de Pandas con la información de los productos.
      * **Comportamientos**: Itera sobre los IDs válidos, ejecuta `product_query` para cada uno y consolida los resultados en un DataFrame.
    * `current_sales_query() -> str`: Retorna la consulta SQL para obtener las promociones vigentes.
    * `get_current_sales() -> pd.DataFrame`: Extrae las promociones vigentes desde MySQL.
      * **Retorna**: `pd.DataFrame`: Un DataFrame de Pandas con la información de las promociones.
    * `get_specifications_cloudscraper`:
    
    ```python
    get_specifications_cloudscraper(
        claves: List[str],
        max_retries: int = 3,
        sleep_seconds: float = 0.15
    ) -> Dict[str, dict]
    ```

     Intenta obtener las fichas técnicas para una lista de claves de productos desde un servicio externo, utilizando `cloudscraper` para manejar posibles protecciones como Cloudflare.
      * **Parámetros**:
        * `claves (List[str])`: Lista de claves de productos.
        * `max_retries (int)`: Número máximo de reintentos por cada clave.
        * `sleep_seconds (float)`: Tiempo inicial de espera entre reintentos (con *backoff* exponencial).
      * **Retorna**: `Dict[str, dict]`: Un diccionario donde la clave es la `claveProducto` y el valor es la ficha técnica en formato JSON.
      * **Comportamiento**: Realiza solicitudes POST al `url` del servicio de fichas técnicas. Implementa lógica de reintentos con *backoff* controlado y maneja diversos errores HTTP (ej., 403 Forbidden) y errores de JSON/red.

**ct/ETL/transform.py** 

* Módulo de la capa de Transformación. Se encarga de limpiar, unificar y normalizar los datos extraídos, y de persistir las fichas técnicas en MongoDB.
* **Clases y funciones claves**:
  * `class Transform`:
    * `__init__()`: Inicializa la clase `Transform`, creando una instancia de `Extraction` y configurando la conexión a la colección de especificaciones de producto.
      * **Parámetros**:
        * `specifications (dict)`: El diccionario que contiene los datos de la ficha técnica de un producto.
      * **Retorna**: Un diccionario estructurado con `fichaTecnica` (pares nombre-valor de características) y `resumen` (descripciones cortas y largas).
      * **Comportamiento**: Navega a través de la estructura anidada de `ProductFeature` y `SummaryDescription` para extraer la información.
    * `transform_specifications(specs: dict) -> dict`: Transforma múltiples especificaciones brutas (obtenidas del servicio externo) en un formato limpio.
      * **Parámetros**:
        * `specs (dict)`: Diccionario de fichas técnicas brutas, donde la clave es la `claveProducto`.
      * **Retorna**: Diccionario con fichas técnicas transformadas y limpias, listas para ser usadas o guardadas.
    * `transform_products() -> pd.DataFrame`: Transforma los datos brutos de productos obtenidos de MySQL en un DataFrame limpio y estandarizado.
      * **Retorna**: Un DataFrame con columnas relevantes, `detalles` concatenados y `detalles_precio` parseado de JSON.
    * `clean_products() -> dict`: Limpia los datos de productos y los enriquece con las fichas técnicas. Primero busca en MongoDB y si no encuentra, las extrae y las guarda.
      * **Retorna**: Un diccionario de productos limpios y enriquecidos, listos para la carga final.
      * **Comportamiento**: Identifica las claves de productos para las que faltan fichas técnicas en MongoDB, las extrae usando `self.data.get_specifications`, las transforma y las guarda en la colección `specifications`. Finalmente, combina las fichas técnicas existentes y nuevas con los datos de productos.
    * `transform_sales() -> pd.DataFrame`: Transforma los datos brutos de promociones (ventas) en un DataFrame limpio.
      * **Retorna**: Un DataFrame con información de promociones, fechas formateadas y descuentos con símbolo de porcentaje.
    * `clean_sales() -> dict`: Limpia los datos de promociones y los enriquece con fichas técnicas de manera similar a `clean_products()`.
      * **Retorna**: Un diccionario de promociones limpias y enriquecidas con fichas técnicas.

**ct/ETL/load.py**

* Módulo de la capa de Carga. Maneja la inserción de los datos transformados en las colecciones de MongoDB y la construcción de la base de datos vectorial.
* **Clases y funciones clave**:
  * `class Load`:
    * `__init__()`: Inicializa la clase `Load`, creando una instancia de `Transform` y configurando las conexiones a las colecciones de MongoDB (`products`, `sales`, `specifications`) y el *embedder* de OpenAI.
    * `build_content(product: dict, product_features: list) -> str`: Contruye el contenido textual de un documento a partir de un diccionario de producto y una lista de características.
      * **Parámetros**:
        * `product (dict)`: Un diccionario con los datos de un producto.
        * `product_features (list)`: Lista de claves de características a incluir en el contenido.
      * **Retorna**: Una cadena de texto concatenada que resume el producto, adecuada para la vectorización.
    * `mongo_products()`: Carga las promociones limpias (procesadas por `Transform`) en la colección `products` de MongoDB, realizando *upserts*.
    * `mongo_sales()`: Carga las promociones limpias (procesadas por `Transform`) en la colección `sales` de MongoDB, realizando *upserts*.
    * `load_products() -> List[Document]`: Carga los productos desde la colección `products` de MongoDB y los convierte en objetos `langchain.schema.Document`.
      * **Retorna**: Una instancia del índice FAISS.
    * `products_vs()`: Crea o actualiza el *vector store* para productos.
      * **Comportamiento**: Carga los productos desde MongoDB, los vectoriza en lotes (`batch_size = 250`) para optimizar el uso de memoria, y guarda el índice FAISS localmente en `PRODUCTS_VECTOR_PATH`.
    * `sales_products_vs()`: Crea o actualiza el *vector store* para ventas/ofertas.
      * **Comportamiento**: Carga las ofertas desde MongoDB. Luego, carga el *vector store* de productos existente desde `PRODUCTS_VECTOR_PATH` y añade las ofertas a este índice (también en lotes), guardando el índice combinado en `SALES_PRODUCTS_VECTOR_PATH`.


### 2.2 Modelos LLM utilizados

El sistema utiliza una combinación estratégica de modelos de lenguaje para optimizar la funcionalidad y los costos:

* **Clasificación de consultas y respuestas iniciales (Ollama - `gemma3:4b`)**:
  * **Función**: Este modelo *open-source*, cargado localmente a través de Ollama, es el primer punto de contacto. Su función principal es clasificar las consultas de los usuarios como *relevantes* (productos que se ofrecen en la empresa), *irrelevantes* (cualquier producto o tema fuera del ámbito de negocio), o *inapropiado* (lenguaje ofensivo).
  * **Ventaja**: Permite una gestión eficiente de consultas no relacionadas con el negocio sin incurrir en costos de API's comerciales, y es ideal para respuestas de "baneo" o corteses.

* **Generación de respuestas relevantes (OpenAI - `gpt-4o`)**:
  * **Función**: Este modelo avanzado de OpenAI es el encargado de generar las respuestas detalladas y contextualizadas para las consultas clasificadas como *relevantes*. Trabaja en conjunto con la cadena RAG para integrar la información recuperada de la base de datos vectorial.
  * **Ventaja**: Ofrece alta calidad y precisión en las respuestas, especialmente en el manejo de precios, promociones complejas y detalles de productos, lo que fue validad en pruebas comparativas.
  * **Nota**: Se ha estado testeando el nuevo modelo `gpt-4.1` y ha dado buenos resultados. Posiblemente haya una actualización del modelo utilizado por OpenAI.

### 2.3 Puntos de entrada y funciones clave

Estos son los principales puntos de inicio para interactuar con las funcionalidades del chatbot:

* `LangchainRAG.run`:

```python
LangchainRAG.run(
  query: str, 
  session_id: str = None, 
  listaPrecio: str = None
  ) -> AsyncGenerator[str, None]:
```

  * Es la función principal que orquesta el glujocompleto de una consulta de usuario.
  * **Parámetros**:
    * `query (str)`: La pregunta o entrada del usuario.
    * `session_id (str)`: Un identificador único para la sesión del usuario, utilizado para mantener el historial correspondiente. 
    * `listaPrecio (str)`: El nivel o clave de precio específico del cliente, que se pasa al LLM para asegurar la precisión de los precios.
  * **Comportamiento**:
    1. Clasifica la `query` (usando `rag.classify_query`).
    2. Basado en la clasificación (*relevante*, *irrelevante*, *inapropiado*), delega la generación de la respuesta al método apropiado (`assistant.answer`, `rag.polite_answer`, `rag.ban_answer`).
    3. Cede fragmentos de la respuesta en tiempo real.
  
* `assistant.answer`:

 ```python
assistant.answer(
  session_id: str, 
  question: str, 
  listaPrecio: str
  ) -> AsyncGenerator[str, None]: 
```

  * Responde a consultas relevantes de productos, utilizando la cadena RAG con memoria.
  * **Parámetros**:
    * `session_id (str)`: ID de la sesión para recuperar/actualizar el historial.
    * `question (str)`: La pregunta original del usuario.
    * `listaPrecio (str)`: El nivel de precios del cliente.
  * **Comportamiento**:
    1. Asegura la existencia de la sesión en MongoDB (`ensure_session`).
    2. Recupera el historial de la sesión (`get_session_history`) y lo trunca a la ventana de memoria definida (`memory_window_size`).
    3. Ejecuta la cadena RAG (`rag_chain`), que involucra la recuperación de documentos (`retriever`) y la generación de respuesta por el LLM.
    4. Transmite la respuesta en fragmentos (`yield_chunk_content`).
    5. Registra la interacción completa (pregunta, respuesta, métricas) en MongoDB (`add_message` y  `add_message_backup`).
  
* `load.py::products_vs()` y `load.py::sales_products_vs()`:
  * Funciones clave para la creación y actualización incremental de las bases de datos vectoriales de productos y ofertas, respectivamente. Orquestan el proceso de carga de documentos desde MongoDB y su vectorización en FAISS.
  * **Parámetros**: Ninguno.
  * **Comportamiento**: 
    1. `products_vs()`: Carga todos los productos de la colección `products` de MongoDB, los vectoriza en lotes para optimizar la memoria y guarda el índice FAISS resultante en `PRODUCTS_VECTOR_PATH`.
    2. `sales_products_vs()`: Carga las ofertas de la colección `sales` de MongoDB. Luego, carga el índice de productos existente (desde `PRODUCTS_VECTOR_PATH`) y añade las ofertas a este mismo índice, también en lotes. Finalmente, guarda el índice combinado (productos + ofertas) en `SALES_PRODUCTS_VECTOR_PATH`. Esta estrategia asegura que las ofertas se integren sin necesidad de re-vectorizar todo el catálogo de productos.

:::

::: {style="text-align: justify"}
## 3 Guía de entrenamiento y mejora 

La mejora continua del chatbot se basa en un ciclo ETL robusto y monitoreo constante del rendimiento.

**Flujo de Datos (ETL)**

La información que alimenta la base de conocimientos del chatbot sigue un proceso ETL (Extracción, Transformación, Carga) bien definido.

* Extracción:
  * Los datos de productos base (ID, nombre, categoría, marca, tipo, modelo, descripciones, palabras clave, precios por lista de precio) se obtienen directamente de una **base de datos MySQL**.
  * La información detallada de las fichas técnicas se extrae de un **servicio externo** a través de solicitudes HTTP/HTTPS, que devuelven los datos en formato XML. Se ha implementado un mecanismo de reintentos y manejo de errores (ej. 403 Forbidden por cambio de IP) para extracción.

* Transformación:
  * Esta etapa es crucial para preparar los datos para la vectorización y el consumo por el LLM. Incluye:
    * Limpieza y normalización: Eliminación de valores nulos o inconsistentes, estandarización de formatos.
    * Unificación de contenido: Concatenación de campos textuales como `descripcion`. `descripcion_corta` y `palabrasClave` en un campo `detalles` para enriquecer el contexto del *embedding*.
    * Integración de fichas técnicas: Los datos de las fichas técnicas XML son parseados y estructurados, combinándose con la información de productos. Las fichas técnicas transformadas se persisten en una colección dedicada en MongoDB (`specifications`) para optimizar futuras extracciones y reusabilidad.
    * Manejo de promociones: Procesamiento de datos de promociones (`precio_oferta`, `descuento`, `EnCompraDE`, `Unidades`, `fecha_fin`, `limitadoA`) para asegurar que el chatbot pueda aplicar la lógica de promociones correctamente.

* Carga:
  * Los datos limpios y transformados se cargan en **tres colecciones principales en MongoDB**:
    * `products`: Almacena la información principal de los productos.
    * `sales`: Contiene los detalles de las promociones y ofertas vigentes.
    * `specifications`: Respalda las fichas técnicas transformadas de los productos.
  * Posteriormente, estos datos se recuperan de MongoDB y se convierten en objetos `Document` de LangChain, que son el formato estándar para la ingestión en bases de datos vectoriales. Se añaden metadatos relevantes como `clave` y `_id` de MongoDB a cada `Document`.


### 3.1 Generación de la base de datos vectorial

La base de datos vectorial es el corazón del sistema RAG:

* Proceso de creación:
  * El archivo `load.py` contiene la lógica principal. La función `load_products()` obtiene los documentos de productos desde MongoDB.
  * La función `vector_store()` crea un índice FAISS local utilizando `OpenAIEmbeddings` para generar las representaciones vectoriales de los documentos.
  * las colecciones de productos y ofertas se procesan y vectorizan en **lotes (`batch_size = 250` para productos, `200` para ofertas)**. Esta estrategia es crucial para manejar grandes volúmenes de datos sin agotar la memoria del sistema ni exceder los límites de tasa (rate limits) de la API de OpenAI.
* Inclusión de ofertas:
  * Las ofertas (`sales`) se añaden *incrementalmente* sobre el *vector store* de productos ya existente utilizando `vector_store.add_documents(sales)`. Esto evita la necesidad de re-vectorizar todo el catálogo.
  * Una copia del índice final, que combina productos y ofertas, se guarda en `SALES_PRODUCTS_VECTOR_PATH`.

### 3.2 Recomendaciones para futura mejora

1. **Embeddings locales (Ollama)**: 

  Para reducir los costos asociados con los *embeddings* de OpenAI y disminuir la dependencia de servicios externos, se recomienda realizar pruebas con los modelos de *embeddings* disponibles a través de Ollama (u otras librerías de *embeddings* locales).

2. **Indexado incremental**: 

  Actualmente, la actualización de la base vectorial puede implicar procesar grandes lotes. Para un mantenimiento más eficiente, especialmente si solo cambian algunos productos o sus precios/promociones, se podría implementar una función de actualización a nivel de producto.

3. **Monitoreo avanzado de rendimiento**:

  Aunque ya se registran métricas de costo y duración, se puede profundizar en el monitoreo.
  
4. **Optimización del manejo de promociones complejas**:

  Las promociones tipo "en compra X lleva Y" aún presenta desafíos. Podría ser necesario enriquecer el contexto del *embedding* para estos productos específicos, reglas más explícitas dentro del LLM o creación de plantillas de *prompts* más específicos para estas promociones.

:::

::: {style="text-align: justify"}
## 4 Diagrama de arquitectura

El siguiente diagrama ilustra la arquitectura general del ssitema del chatbot, mostrando los componentes principales y el flujo de datos desde la interacción del usuario hasta la generación de respuestas y el almacenamiento del historial. Se ha actualizado para reflejar la implementación de MongoDB y los diferentes flujos.

### 4.1 Componentes clave:
* **Interfaz de usuario (widget del chatbot)**: El componente frontal incrustado en la página web de CT Internacional, permitiendo la interacción directa del usuario.
* **Servicio intermediario PHP**: Actúa como un puente seguro entre el *frontend* (HTTPS) y la API del chatbot (HTTPS pero con certificado autofirmado, o sea, no seguro) para resolver problemas de *contenido mixto*. Si el *backend* ya está en en HTTPS con un certificado SSL seguro, este componente puede ser obviado.
* **API del chatbot (FastAPI)**: El servicio *backend* principal que procesa las consultas de los usuarios, orquesta la recuperación de información y se comunica con los modelos de lenguaje.
* **Base de datos de productos y promociones (MySQL)**: Almacena la información transaccional y de precios de los productos y promociones de CT Internacional. Es la fuente original de los datos.
* **Servicio de fichas técnicas**: Fuente de datos para la información detallada y semi-estructurada (XML) de los productos.
* **Módulo ETL**: Proceso automatizado que:
  * Extrae datos de MySQL y el servicio de fichas técnicas.
  * Limpia, unifica, y transforma los datos, persistiendo las fichas técnicas en MongoDB.
  * Carga los datos limpios en colecciones de MongoDB (`products`, `sales`, `specifications`).
* **Base de datos NoSQL (MongoDB)**: Almacena los datos limpios de productos, ofetas y fichas técnicas, así como el historial de las interacciones de los usuarios (`sessions`, `message_backup`).
  * `sessions`: Mantiene los últimos *n* mensajes de cada usuario para recuperación rápida y una experiencia de usuario fluida.
  * `message_backup`: Almacena un historial completo de todas las consultas y respuestas con métricas detalladas para fines de análisis y reportes.
* **Modelo de embeddings**: Componente encargado de transformar tanto las consultas de los usuarios como la información de los productos en representaciones vectoriales numéricas (usando OpenAI Embeddings).
* **Base de datos vectorial (FAISS)**: Almacena las representaciones vectoriales de la información de productos y ofertas, permitiendo búsquedas de similitud eficientes. Se actualiza con datos del ETL.
* **Clasificador de consultas (Ollama - `gemma3:12b`)**: Componente central que, para consultas relevantes, coordina la búsqueda de información en la base de datos vectorial y contextualiza esta información con la consulta del usuario.
* **LLM**: El motor principal del chatbot para consultas relevantes, responsable de generar respuestas coherentes y detalladas en base a la consulta contextualizada por el RAG.
* **Sistema de reportes automatizados**: Procesa los datos del `message_backup` en MongoDB para generar *insights* sobre el uso del chatbot, intereses de los clientes, costos y rendimiento.

### 4.2 Flujo de interacción principal:
1. El **usuario** interactúa con el **widget del chatbot** en la página web.
2. La consulta se envía a la **API del chatbot** (**potencialmente vía el servicio intermediario PHP**)
3. La **API** primero pasa consulta al **clasificador de consultas**.
4. Si la consulta es *relevante*:
  * Pasa por el **modelo de embeddings** y luego al **módulo RAG** para buscar en la **base de datos vectorial**.
  * La información recuperada se contextualiza y se envía al **LLM** para generar la **respuesta al usuario**.
5. Si la consulta es *irrelevante* o *inapropiada*, el **clasificador** genera una respuesta adecuada (cortés o de advertencia) directamente al **usuario**.
6. Todas las interacciones (consultas y respuestas) se registran en las colecciones `sessions` y `message_backup` en **MongoDB**.
7. El **sistema de reportes automatizados** accede a `message_backup` para generar análisis de datos.

### 4.3 Flujo de datos
1. El **módulo ETL** extrae datos de **MySQL** y el **servicio de fichas técnicas**.
2. Los datos se transforman y se cargan en **MongoDB** (colecciones `products`, `sales`, `specifications`).
3. Los datos de `products` y `sales` en **MongoDB** son utilizados por el **módulo ETL** para construir y actualizar la **base de datos vectorial**

![Arquitectura del sistema](./arquitectura.jpg){width=90%}

:::