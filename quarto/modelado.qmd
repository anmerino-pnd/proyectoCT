---
title: "Modelado y Evaluación"
format: 
  html:
    page-layout: article
toc-title: "Tabla de Contenidos"
toc: true
toc-depth: 3
---

::: {style="text-align: justify"}
## 1 Modelado

El objetivo de esta fase es desarrollar la arquitectura del sistema de recuperación aumentada con generación (RAG). Para ello, se utiliza como fuente de conocimiento la base de datos vectorizada construida en la etapa anterior.

### 1.1 Modelos

Dado que el enfoque del proyecto se basa en herramientas de OpenAI, los modelos considerados para esta fase son los siguientes:

- **GPT-4o-mini:**  
  Una versión ligera de GPT-4o, diseñada para ofrecer un buen balance entre costo, velocidad de respuesta y calidad en tareas de lenguaje natural. Es ideal para pruebas rápidas o implementaciones donde se requiere eficiencia.

- **GPT-4o:**  
  Modelo multimodal de última generación de OpenAI, capaz de procesar texto, imágenes y audio. En este proyecto se utiliza solo su capacidad textual, destacando por su mayor comprensión semántica y coherencia en las respuestas.

- **Modelos open-source integrados mediante Ollama:**  
  Ollama permite correr modelos de lenguaje open-source de manera local o privada. En este proyecto se contemplan modelos como LLaMA o Mistral, que ofrecen alternativas de código abierto con buen rendimiento en tareas conversacionales.


### 1.2 Arquitectura del sistema RAG

La implementación del sistema se basa en una estructura modular orientada a clases. Esto permite una mayor reutilización de código, facilita su mantenimiento y mejora la legibilidad, aspectos clave para futuras modificaciones o revisiones.

Además, esta estructura permite importar únicamente la clase necesaria para ejecutar todo el sistema, lo cual es ideal para su integración a través de una API. De esta forma, se evita depender de notebooks o archivos extensos y poco escalables.

El sistema se construyó utilizando principalmente la librería **LangChain**, la cual ofrece una base robusta para conectar modelos de lenguaje con herramientas externas y flujos personalizados.

### 1.3 Métricas de Evaluación

A diferencia de los modelos clásicos de *machine learning* (ML), la evaluación de sistemas basados en modelos de lenguaje grande (LLMs) requiere enfoques distintos, centrados en la calidad de las respuestas generadas.

En este proyecto, la evaluación se realiza mediante un análisis cualitativo de las respuestas del chatbot, tomando en cuenta los siguientes criterios:

- La información sobre productos, descripciones y características debe estar alineada con los datos disponibles en la base vectorial.
- Las respuestas deben ser claras, concisas y coherentes, evitando alucinaciones o información incorrecta.
- Los precios deben coincidir con los establecidos en la base de datos, y en el caso de promociones, estas deben estar correctamente aplicadas, evitando errores que impliquen pérdidas económicas.

Estos criterios serán evaluados por los expertos y personas con conocimiento en la empresa.
:::

::: {style="text-align: justify"}
## 2 Evaluación

Con base en las respuestas generadas durante la etapa de modelado, se llevó a cabo una evaluación cualitativa para analizar la coherencia, relevancia y precisión de las recomendaciones de cada modelo. Este análisis nos permitió identificar oportunidades de mejora en el sistema, así como validar si el comportamiento del modelo es adecuado para continuar con su implementación o si requiere ajustes adicionales.

A continuación, se presentan las respuestas generadas por el sistema para una serie de consultas simuladas por un usuario. Estas imágenes muestran el resultado del mejor modelo seleccionado (GPT 4o) ante cada solicitud:

1. **Consulta:** _"¡Hola! Me interesan computadoras de oficina"_  
   ![Respuesta a consulta 1](./evaluacion/image1.png){width=40%}

2. **Consulta:** _"También me gustaría ver monitores de 27 pulgadas arriba de 75Hz"_  
   ![Respuesta a consulta 2](./evaluacion/image2.png){width=40%}

3. **Consulta:** _"Y un no break gamer"_  
   ![Respuesta a consulta 3](./evaluacion/image3.png){width=40%}

4. **Consulta:** _"Y una extensión doméstica"_  
   ![Respuesta a consulta 4](./evaluacion/image4.png){width=40%}

Los resultados obtenidos reflejan un desempeño sólido por parte del sistema. En todos los casos evaluados, las respuestas del chatbot fueron coherentes, alineadas con la base de datos y cumplieron con los criterios definidos:

- Las ofertas y promociones fueron correctamente identificadas y presentadas.
- Los precios y descripciones de los productos coincidieron con los datos reales.
- No se observaron errores de alucinación ni pérdidas de coherencia en la conversación.

Esto sugiere que el modelo es capaz de generar respuestas confiables y útiles para los usuarios, por lo que se considera viable continuar con las siguientes etapas del proyecto o bien escalar el sistema hacia una versión de prueba.
:::